{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f451fb8-048c-455b-a5a6-df7d5b426123",
   "metadata": {},
   "source": [
    "# Hello LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712498c-b9a3-4917-84f1-d000869ffb91",
   "metadata": {},
   "source": [
    "# LangChain  관련 주요 링크\n",
    "\n",
    "-  Python Langchain 공식 홈:  https://python.langchain.com/\n",
    "-  API 레퍼런스 홈: https://python.langchain.com/api_reference/reference.html\n",
    "\n",
    "\n",
    "## Langchain 의 패키지 구성\n",
    "\n",
    "\n",
    "### Base Packages\n",
    "- [Core: langchain-core](https://python.langchain.com/api_reference/core)\n",
    "- [Langchain: langchain](https://python.langchain.com/api_reference/langchain)\n",
    "- [Test Splitters: langchain-text-splitters](https://python.langchain.com/api_reference/text_splitters)\n",
    "- [Community: langchain-community](https://python.langchain.com/api_reference/community)\n",
    "- [Experimental: langchain-experimental](https://python.langchain.com/api_reference/experimental)\n",
    "\n",
    "### Integrations\n",
    "- 랭체인은 수많은 LLM 모델들과 커뮤니티, 벡터스토어, 데이터베이스, 툴 들과 함께 사용할수 있도록 제공되는 패키지들이 많다 (앞으로 더 많아 질거다)\n",
    "- [OpanAI: langchain-openai](https://python.langchain.com/api_reference/openai)\n",
    "- [Huggingface: langchain-huggingface](https://python.langchain.com/api_reference/huggingface)\n",
    "- [MistalAI: langchain-mistralai](https://python.langchain.com/api_reference/mistralai)\n",
    "- 그밖에도 많이 있다 ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756d471-8be8-46f0-8e42-d4ebbd9dd6d7",
   "metadata": {},
   "source": [
    "# 환경변수 설정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7052afee-5248-40bf-a37e-db43ec58a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3345ce0-c96c-49a1-a8a6-704f2dd371cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcdda514-0b65-4836-a31a-d88b1696a2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-proj-iKU13YeoxNgF...'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'][:20] + '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dbc823e-6c98-4913-82bb-66f150880242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수동으로 읽어오기\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94b5270c-06a1-41c3-992d-fd9461e96cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89c72428-ef63-4745-ad04-ed8bf45a5fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-iKU13YeoxNgF...\n",
      "sk-proj-iKU13YeoxNgF...\n"
     ]
    }
   ],
   "source": [
    "print(f'{os.environ['OPENAI_API_KEY'][:20]}...')\n",
    "print(f'{os.getenv('OPENAI_API_KEY')[:20]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a58299a-3800-4831-b66d-0bc354e3f2b5",
   "metadata": {},
   "source": [
    "# ■ LLM vs. Chat model\n",
    "\n",
    "LangChain 은 LLM 과 Chat model 두가지를 지원합니다\n",
    "\n",
    "`LLM`(Large Language Model)과 `Chat Model`은 비슷한 역할을 하지만, 약간의 차이점이 있습니다.\n",
    "\n",
    "이는 주로 **모델의 입력 및 상호작용 방식**에서 나타난다.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. LLM (Large Language Model)\n",
    "- **특징**:\n",
    "  - 일반적으로 **텍스트 입력**을 받고, 이에 대한 텍스트 출력을 생성합니다.\n",
    "  - 단순한 프롬프트 기반 입력/출력을 처리하기 위한 모델입니다.\n",
    "  - 사용자가 제공한 입력 텍스트를 분석하고, 그에 대한 결과를 한 번에 출력합니다.\n",
    "- **입력 형식**:\n",
    "  ```plaintext\n",
    "  \"Tell me a summary of the benefits of LangChain.\"\n",
    "  ```\n",
    "- **출력 형식**:\n",
    "  ```plaintext\n",
    "  \"LangChain is a framework designed to simplify the development of applications powered by large language models, making it easier to manage prompts, chains, and integrations.\"\n",
    "  ```\n",
    "- **주요 사용 사례**:\n",
    "  - 단일 질문-답변\n",
    "  - 텍스트 생성\n",
    "  - 간단한 프롬프트 처리를 위한 작업\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Chat Model\n",
    "- **특징**:\n",
    "  - **대화 형식**으로 설계된 모델로, 다중 턴 대화를 처리할 수 있다.\n",
    "  - 입력 형식이 **메시지 Message**로 구성되며, 각 메시지는 사용자의 메시지 (User Message)와 시스템의 메시지(System Message)로 나뉜다.\n",
    "  - '문맥'을 이해하고 '대화의 흐름'을 유지하는 데 최적화되어 있다.\n",
    "- **입력 형식**:\n",
    "  메시지 객체를 전달해야 하며, 보통 아래와 같은 구조입니다.\n",
    "  ```python\n",
    "  [\n",
    "      {\"role\": \"system\", \"content\": \"You are an assistant who helps with Python programming.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Can you explain the difference between LLM and chat models in LangChain?\"}\n",
    "  ]\n",
    "  ```\n",
    "- **출력 형식**:\n",
    "  ```python\n",
    "  {\"role\": \"assistant\", \"content\": \"Sure! LLM and Chat Models differ in their input and interaction styles...\"}\n",
    "  ```\n",
    "- **주요 사용 사례**:\n",
    "  - 다중 턴 대화\n",
    "  - 문맥 추적 및 유지 (대화 히스토리 반영)\n",
    "  - 대화 기반 챗봇, FAQ 시스템 등\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 주요 차이점 요약\n",
    "| **특징**        | **LLM**                                                | **Chat Model**                                        |\n",
    "|-----------------|------------------------------------------------------|----------------------------------------------------|\n",
    "| **입력 형식**   | 단일 텍스트 입력                                         | 역할 기반의 대화 메시지 객체 (role: system, user, assistant) |\n",
    "| **대화 히스토리**| 문맥 추적 불가능 (단일 요청 처리)                          | 대화 히스토리를 통해 문맥을 유지하고 반영                 |\n",
    "| **사용 목적**   | 텍스트 생성, 요약, 단순 질의응답                             | 대화형 인터페이스, 챗봇, 다중 턴 질의응답               |\n",
    "| **응용 사례**   | 단일 질문-답변, 텍스트 생성                                | 고객 지원 챗봇, 인터랙티브 Q&A, 멀티턴 대화 시스템          |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 5. 언제 어떤 것을 선택해야 할까요?\n",
    "- **단일 작업이나 간단한 텍스트 생성**:\n",
    "  - `LLM`을 사용하는 것이 적합합니다.\n",
    "- **대화 기반 애플리케이션이나 문맥을 유지해야 하는 작업**:\n",
    "  - `Chat Model`을 사용하는 것이 더 적합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ccee37-7d81-4b7a-acc2-1aae8ed333ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.23'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3551daf4-a602-4ddd-9a8c-5fc8f2a7c0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms.base import OpenAI  # LLM   (gpt-3.5-turbo 사용  2024.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28c409e5-10c0-4c0d-b84a-99b642736de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models.base import ChatOpenAI  # Chat model  (gpt-3.5-turbo 사용  2024.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09a2b142-c3dc-4fb9-bb81-6c416e353245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랭체인을 사용하면!\n",
    "# 각 model 들의 API 을 따로따로 공부하고 알아야 할 필요없다.\n",
    "# 또한 각 model api 제공자가 제공하는 Python package 를 다운로드 할 필요도 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bc4964e-f686-4625-b10b-26e38366af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ebc1b90-affc-4258-a500-491890c12b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo-instruct'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46396760-901c-49e2-b2da-7c8ba354780f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI()\n",
    "\n",
    "chat.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f37149-925a-48ba-b182-7ff7fa6b668f",
   "metadata": {},
   "source": [
    "# LLM 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ffbb999-4e0e-4f25-9cd9-7b90406f8de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "답변 \n",
      "\n",
      "There are eight planets in our solar system:\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"How many planets are there?\")\n",
    "print(type(result))\n",
    "print('답변', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ce83bc0-9a66-411c-bb48-6e31290cf5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "답변 \n",
      "\n",
      "태양계에는 8개의 행성이 있습니다. 이들은 수성, 금성, 지구, 화성, 목성, 토성, 천왕성, 해왕성입니다.\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"태양계에는 얼마나 많은 행성들이 있나요? 그 행성들의 이름도 알려줘.\")\n",
    "print(type(result))\n",
    "print('답변', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13cf0a-997e-4feb-abde-e476273087b7",
   "metadata": {},
   "source": [
    "# ChatModel 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08679f59-0a61-47d8-b573-ce8bca5b5424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "💚 content='In our solar system, there are 8 planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 13, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ025LB5WoIyavTWn9M3bVpxgPS', 'finish_reason': 'stop', 'logprobs': None} id='run--fd10c442-8bde-478c-8301-c4642f620ff4-0' usage_metadata={'input_tokens': 13, 'output_tokens': 29, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "🧡답변 In our solar system, there are 8 planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\n"
     ]
    }
   ],
   "source": [
    "result = chat.invoke(\"How many planets are there?\")\n",
    "print(type(result))  # Message객체\n",
    "print('💚', result)\n",
    "print('🧡답변', result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f000845-966e-447e-9760-2756fceeca5f",
   "metadata": {},
   "source": [
    "## 한글 or 영어 ?\n",
    "\n",
    "챗 GPT의 언어 처리 능력은 인공지능 기술의 훌륭한 발전을 보여줍니다. 하지만 사용자가 받는 답변의 품질은 제출하는 언어에 따라 약간의 차이가 있을 수 있습니다. 이런 차이는 챗 GPT가 학습하는 과정에서 다양한 언어의 데이터 양과 품질, 그리고 언어별 특성을 얼마나 잘 처리하는지에 따라 결정됩니다.\n",
    "\n",
    "OpenAI의 언어 모델, 특히 GPT 시리즈는 다양한 데이터 소스에서 얻은 대량의 데이터로 학습됩니다. 이 데이터는 주로 영어를 비롯한 여러 언어에서 수집되며, 학습 데이터의 구성은 모델의 성능과 일반화 능력에 큰 영향을 미칩니다.\n",
    "\n",
    "영어는 전세계적으로 많이 사용되며*, 인터넷 상의 데이터도 영어가 많아서 챗 GPT는 영어 질문에 대해 더 정확하고 자연스러운 답변을 제공할 확률이 높습니다. 그러나 한국어와 같은 다른 언어는 상대적으로 데이터가 부족하거나, 언어의 복잡성 때문에 처리가 더 어려울 수 있어, 이로 인해 답변의 품질에 차이가 나타날 수 있습니다.\n",
    "\n",
    "참고\n",
    "- https://fastcampus.co.kr/gov_review_insightGPTlang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99e53fb-c03e-4632-a76c-f34f02a39c39",
   "metadata": {},
   "source": [
    "## 사용량 확인 해보기\n",
    "이쯤에서 openai 페이지\n",
    "DASHBOARD > Usage > Activity 를 확인해보자.\n",
    "사용한 양이 표시될거다.\n",
    "\n",
    "https://platform.openai.com/usage/activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ea5cba1-7480-47db-af08-c5e2eaff696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic.chat_models import ChatAnthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb945fe9-6037-41dc-92ed-8a1a65eb1812",
   "metadata": {},
   "source": [
    "# Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05876845-72ab-4858-b8d3-0d0a842e2db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Model 은 '질문'만 받는게 아니라 '대화' 도 할수 있다 (Message 를 보낼수도 있다)\n",
    "# '대화(conversation)' 은\n",
    "#    : 여러 메세지 묶음\n",
    "#    : 상대의 대화의 맥락에 맞게 대답할수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49555589-c391-4737-9f35-46ed47a3f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/chat/openai/#instantiation\n",
    "# 레퍼런스 : https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html\n",
    "\"\"\"\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "        # 모델의 응답 다양성을 제어하는 역할을 합니다.\n",
    "        # 이는 OpenAI의 GPT 모델에서 사용하는 매개변수로,\n",
    "        #  생성되는 텍스트의 창의성과 확률적 다양성(랜덤성을 조정합니다)ㄴ\n",
    "        #\n",
    "    max_tokens=None,  # model 리 리턴하는 결과의 최대 token 개수지정.\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    # base_url=\"...\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f59b93e-5aee-47ff-bf3a-1bd38ab55505",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dc11095-bcc7-4688-a87e-c69f2c951e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "# https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html\n",
    "\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "# https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html\n",
    "\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "# https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html\n",
    "\n",
    "# HumanMessage : 사람이 AI 에 보내는 Message\n",
    "# SystemMessage : LLM 에 설정들을 제공하기 위한 Message\n",
    "# AIMessage: AI 에 의해 리턴되는 Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "946ebe90-51db-4c82-a036-5d1039a54c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a geography expert. And your only reply in Korean\",\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content=\"안녕, 내 이름은 둘리 야\",\n",
    "    ),\n",
    "    HumanMessage(\n",
    "      content=\"\"\"\n",
    "          What is the distance between Mexico and Thailand.\n",
    "          Also, what is yoru name?\n",
    "      \"\"\",  \n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d02ad89-27fc-4ffc-b69a-1cbb49637e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='멕시코와 태국 사이의 거리는 대략 16,000km입니다. 제 이름은 둘리입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 62, 'total_tokens': 98, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ17vzhd1CfuizhOEtTCVXV3pFf', 'finish_reason': 'stop', 'logprobs': None}, id='run--b42370ba-56da-4c8e-aee4-8449d1624ea3-0', usage_metadata={'input_tokens': 62, 'output_tokens': 36, 'total_tokens': 98, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chat.invoke(messages)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737a10e-0168-44dc-815c-33527824867f",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453ab8c-ff24-4652-b328-5ba020937977",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "↑ messages 를 prompt 라고도 함 (?)\n",
    "- 모델에 입력으로 제공되는 텍스트나 데이터\n",
    "- 모델에게 작업을 수행하도록 지시하거나, 모델이 생성할 텍스트의 컨텍스트를 제공\n",
    "- LLM 과 의사소통하기 위한 방법\n",
    "\n",
    "---\n",
    "\n",
    "LLM(대형 언어 모델)에서 **프롬프트 prompt**란 모델에 **입력**으로 제공되는 텍스트나 데이터입니다. 이는 모델에게 작업을 수행하도록 지시하거나, 모델이 생성할 텍스트의 컨텍스트를 제공합니다. 프롬프트는 모델의 출력을 결정하는 중요한 역할을 합니다.\n",
    "\n",
    "### 프롬프트의 역할:\n",
    "1. **모델에 대한 지시**: 프롬프트는 모델에게 무엇을 해야 할지 알려주는 역할을 합니다. 예를 들어, 사용자가 모델에게 질문을 하거나, 특정 스타일의 텍스트를 생성하도록 요청할 때 프롬프트가 필요합니다.\n",
    "\n",
    "2. **컨텍스트 제공**: 모델이 적절한 응답을 생성할 수 있도록 필요한 배경 정보나 문맥을 제공합니다. 예를 들어, 어떤 주제에 대한 질문을 할 때, 관련 배경 정보를 제공하여 모델이 더 정확한 답을 할 수 있게 합니다.\n",
    "\n",
    "3. **모델의 출력 유도**: 프롬프트가 모델의 출력을 유도하고, 생성되는 텍스트의 스타일, 내용, 형식 등을 결정하는 데 중요한 영향을 미칩니다.\n",
    "\n",
    "### 예시:\n",
    "1. **질문 응답**:\n",
    "   - **프롬프트**: \"What is the capital of France?\"\n",
    "   - **출력**: \"The capital of France is Paris.\"\n",
    "\n",
    "2. **창의적 글쓰기**:\n",
    "   - **프롬프트**: \"Write a short story about a dragon and a knight.\"\n",
    "   - **출력**: 모델이 창의적으로 드래곤과 기사에 관한 이야기를 생성합니다.\n",
    "\n",
    "3. **번역**:\n",
    "   - **프롬프트**: \"Translate the following sentence to Spanish: 'Hello, how are you?'\"\n",
    "   - **출력**: \"Hola, ¿cómo estás?\"\n",
    "\n",
    "### 프롬프트의 종류:\n",
    "- **단순한 질문**: 사용자가 단순히 궁금한 점을 묻는 형태.\n",
    "- **지시문**: 특정 작업을 수행하도록 지시하는 형태.\n",
    "- **형식화된 입력**: 특정 형식이나 구조를 갖춘 입력(예: 텍스트 요약, 번역, 코드 작성 등).\n",
    "\n",
    "### 프롬프트 설계의 중요성:\n",
    "- **정확한 결과**를 얻기 위해서는 **프롬프트의 설계**가 매우 중요합니다. 프롬프트가 모호하거나 불완전하면 모델이 원하는 출력을 생성하기 어렵습니다.\n",
    "- 다양한 프롬프트를 실험하면서 모델의 반응을 관찰하고, 가장 적합한 프롬프트를 찾는 것이 중요합니다.\n",
    "\n",
    "### 프롬프트 설계 팁:\n",
    "1. **명확하고 구체적인 지시**: 무엇을 원하는지 정확하게 전달하세요. 예를 들어, \"Explain quantum mechanics\"보다는 \"Explain quantum mechanics in simple terms for a high school student\"와 같이 구체적인 요구를 하는 것이 좋습니다.\n",
    "   \n",
    "2. **적절한 컨텍스트 제공**: 필요한 배경 정보나 문맥을 제공하면 모델이 더 정확한 답변을 생성할 수 있습니다.\n",
    "\n",
    "3. **다양한 실험**: 프롬프트를 조금씩 바꿔가며 테스트해 보면서 최적의 응답을 유도할 수 있습니다.\n",
    "\n",
    "### 결론:\n",
    "프롬프트는 대형 언어 모델에게 작업을 지시하는 중요한 입력으로, 모델이 수행할 작업의 방향을 결정짓는 요소입니다. 프롬프트를 잘 설계하는 것이 LLM을 효과적으로 활용하는 데 큰 도움이 됩니다.## Prompt\n",
    "↑ messages 를 prompt 라고도 함 (?)\n",
    "- 모델에 입력으로 제공되는 텍스트나 데이터\n",
    "- 모델에게 작업을 수행하도록 지시하거나, 모델이 생성할 텍스트의 컨텍스트를 제공\n",
    "- LLM 과 의사소통하기 위한 방법\n",
    "\n",
    "---\n",
    "\n",
    "LLM(대형 언어 모델)에서 **프롬프트 prompt**란 모델에 **입력**으로 제공되는 텍스트나 데이터입니다. 이는 모델에게 작업을 수행하도록 지시하거나, 모델이 생성할 텍스트의 컨텍스트를 제공합니다. 프롬프트는 모델의 출력을 결정하는 중요한 역할을 합니다.\n",
    "\n",
    "### 프롬프트의 역할:\n",
    "1. **모델에 대한 지시**: 프롬프트는 모델에게 무엇을 해야 할지 알려주는 역할을 합니다. 예를 들어, 사용자가 모델에게 질문을 하거나, 특정 스타일의 텍스트를 생성하도록 요청할 때 프롬프트가 필요합니다.\n",
    "\n",
    "2. **컨텍스트 제공**: 모델이 적절한 응답을 생성할 수 있도록 필요한 배경 정보나 문맥을 제공합니다. 예를 들어, 어떤 주제에 대한 질문을 할 때, 관련 배경 정보를 제공하여 모델이 더 정확한 답을 할 수 있게 합니다.\n",
    "\n",
    "3. **모델의 출력 유도**: 프롬프트가 모델의 출력을 유도하고, 생성되는 텍스트의 스타일, 내용, 형식 등을 결정하는 데 중요한 영향을 미칩니다.\n",
    "\n",
    "### 예시:\n",
    "1. **질문 응답**:\n",
    "   - **프롬프트**: \"What is the capital of France?\"\n",
    "   - **출력**: \"The capital of France is Paris.\"\n",
    "\n",
    "2. **창의적 글쓰기**:\n",
    "   - **프롬프트**: \"Write a short story about a dragon and a knight.\"\n",
    "   - **출력**: 모델이 창의적으로 드래곤과 기사에 관한 이야기를 생성합니다.\n",
    "\n",
    "3. **번역**:\n",
    "   - **프롬프트**: \"Translate the following sentence to Spanish: 'Hello, how are you?'\"\n",
    "   - **출력**: \"Hola, ¿cómo estás?\"\n",
    "\n",
    "### 프롬프트의 종류:\n",
    "- **단순한 질문**: 사용자가 단순히 궁금한 점을 묻는 형태.\n",
    "- **지시문**: 특정 작업을 수행하도록 지시하는 형태.\n",
    "- **형식화된 입력**: 특정 형식이나 구조를 갖춘 입력(예: 텍스트 요약, 번역, 코드 작성 등).\n",
    "\n",
    "### 프롬프트 설계의 중요성:\n",
    "- **정확한 결과**를 얻기 위해서는 **프롬프트의 설계**가 매우 중요합니다. 프롬프트가 모호하거나 불완전하면 모델이 원하는 출력을 생성하기 어렵습니다.\n",
    "- 다양한 프롬프트를 실험하면서 모델의 반응을 관찰하고, 가장 적합한 프롬프트를 찾는 것이 중요합니다.\n",
    "\n",
    "### 프롬프트 설계 팁:\n",
    "1. **명확하고 구체적인 지시**: 무엇을 원하는지 정확하게 전달하세요. 예를 들어, \"Explain quantum mechanics\"보다는 \"Explain quantum mechanics in simple terms for a high school student\"와 같이 구체적인 요구를 하는 것이 좋습니다.\n",
    "   \n",
    "2. **적절한 컨텍스트 제공**: 필요한 배경 정보나 문맥을 제공하면 모델이 더 정확한 답변을 생성할 수 있습니다.\n",
    "\n",
    "3. **다양한 실험**: 프롬프트를 조금씩 바꿔가며 테스트해 보면서 최적의 응답을 유도할 수 있습니다.\n",
    "\n",
    "### 결론:\n",
    "프롬프트는 대형 언어 모델에게 작업을 지시하는 중요한 입력으로, 모델이 수행할 작업의 방향을 결정짓는 요소입니다. 프롬프트를 잘 설계하는 것이 LLM을 효과적으로 활용하는 데 큰 도움이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3b928-05b2-4f05-9334-f19f51e0fe35",
   "metadata": {},
   "source": [
    "Prompt 성능이 좋다면 LLM 답변의 성능도 좋을 것입니다.\n",
    "\n",
    "모든 웹 사이트들은 상황에 맞는 뛰어는 성능의 prompt 를 제작하는데 전념함.\n",
    "\n",
    "LangChain 은 prompt 를 공유하기 위한 커뮤니티도 형성되고 있다.\n",
    "산업 전체 전반적으로 각 분야별 prompt 를 만들어 내고 있다.\n",
    "\n",
    "예를 들면\n",
    "| 플랫폼               | 기능              | URL                                                             |\n",
    "| ----------------- | --------------- | --------------------------------------------------------------- |\n",
    "| **LangChain Hub** | 프롬프트 및 체인 공유    | [smith.langchain.com/hub](https://smith.langchain.com/hub)      |\n",
    "| **Discord**       | 커뮤니티, 프롬프트 논의   | [discord.gg/langchain](https://discord.gg/langchain)            |\n",
    "| **GitHub**        | 코드 예제, 프롬프트 활용법 | [LangChain Examples](https://github.com/langchain-ai/langchain) |\n",
    "\n",
    "\n",
    "그래서, LangChain 프레임워크의 많은 부분이 prompt 에 집중되어 있다.\n",
    "\n",
    "prompt 끼리 결함도 할수 있고, 저장하거나 불러올수도 있다.\n",
    "\n",
    "변수 설정 도중에 검증도 할수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee61e4-c1fa-4cd9-80f2-b4da917e30ba",
   "metadata": {},
   "source": [
    "## PromptTemplate\n",
    "메세지 커스터마이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "944e6615-8527-47fa-bebf-c86acce9c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html\n",
    "\n",
    "from langchain_core.prompts.chat import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatMessagePromptTemplate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05bb28c2-140f-4088-977e-85536b969802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country_a', 'country_b'], input_types={}, partial_variables={}, template='What is the distance between {country_a} and {country_b}')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = PromptTemplate.from_template(\n",
    "     # placeholder {...} 사용\n",
    "    \"What is the distance between {country_a} and {country_b}\"\n",
    ")\n",
    "\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c7ff551-685a-4f85-8eaf-066daa82bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template.format() # 에러 KeyError: 'country_a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07e59b79-d26e-4821-a9a0-9ca9eb942a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the distance between Mexico and Brazil'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = template.format(country_a = 'Mexico', country_b = 'Brazil')\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05bf36f8-4687-4790-812a-74fb960236b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The distance between Mexico and Brazil is approximately 6,000 kilometers (3,728 miles) when measured in a straight line. However, the actual distance may vary depending on the specific locations in each country.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 15, 'total_tokens': 57, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ3y1TuD3vf2wFzgx13ilf9sxus', 'finish_reason': 'stop', 'logprobs': None}, id='run--4369e298-be00-4e39-9c72-d92f023cf107-0', usage_metadata={'input_tokens': 15, 'output_tokens': 42, 'total_tokens': 57, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777d5a1-4a29-4981-ab2a-03f1ee2354c1",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8f1bce1-dd5c-4033-af42-30f5bffdc2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['country_a', 'country_b', 'language', 'name'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a geography expert. And your only reply in {language}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='안녕, 내 이름은 {name} 야'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country_a', 'country_b'], input_types={}, partial_variables={}, template='\\n        What is the distance between {country_a} and {country_b}.\\n        Also, what is your name?\\n        '), additional_kwargs={})])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    # SystemMessage 튜플\n",
    "    (\"system\", \"You are a geography expert. And your only reply in {language}\"),\n",
    "    # AIMessage 튜플\n",
    "    (\"ai\", \"안녕, 내 이름은 {name} 야\"),\n",
    "    # HumanMessage 튜플\n",
    "    (\"human\", \"\"\"\n",
    "        What is the distance between {country_a} and {country_b}.\n",
    "        Also, what is your name?\n",
    "        \"\"\"),    \n",
    "])\n",
    "\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e095f217-d479-44f5-b4e3-419b731e3470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a geography expert. And your only reply in Korean', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='안녕, 내 이름은 뽀로로 야', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='\\n        What is the distance between Canada and Japan.\\n        Also, what is your name?\\n        ', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = template.format_messages(\n",
    "    language=\"Korean\",\n",
    "    name=\"뽀로로\",\n",
    "    country_a=\"Canada\",\n",
    "    country_b=\"Japan\",\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1e111b5-b02d-448b-87a8-103c34ebb4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='캐나다와 일본 사이의 거리는 약 7,000km입니다. 제 이름은 뽀로로입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 62, 'total_tokens': 97, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ417m3mD6cC5R7cyUBmu7AFz47', 'finish_reason': 'stop', 'logprobs': None}, id='run--359698c5-013a-4469-a682-6af18ea14a8e-0', usage_metadata={'input_tokens': 62, 'output_tokens': 35, 'total_tokens': 97, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714540e8-89ee-4c3d-b540-2636dac098e7",
   "metadata": {},
   "source": [
    "# OutputParser and LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c82a91-f930-42dd-a4c8-1d8f9aa2c918",
   "metadata": {},
   "source": [
    "## Output Parser 란\n",
    "\n",
    "LLM(대형 언어 모델)에서 생성된 출력을 처리하고 '원하는 형식으로 변환'하는 데 사용되는 유틸리티입니다. 이를 통해 모델이 생성하는 텍스트를 '구조화된 데이터로 변환'하거나, '특정 규칙에 따라 데이터를 추출'할 수 있습니다\n",
    "\n",
    "1. 출력 구조화\n",
    "    - 모델의 텍스트 응답을 파싱하여 JSON, 딕셔너리, 목록 등과 같은 프로그래밍에서 사용 가능한 구조화된 데이터로 변환합니다\n",
    "    \n",
    "1. 출력 검증\n",
    "    - 모델이 예상치 못한 출력을 반환할 경우 적절한 에러 메시지를 제공하거나 기본값을 반환하도록 처리할 수 있습니다.\n",
    "    \n",
    "1. 출력 표준화\n",
    "    - 언어 모델의 출력이 항상 일관된 형식으로 제공되도록 보장합니다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da79679-449d-49c6-8c5e-0e0426ff7ba1",
   "metadata": {},
   "source": [
    "## BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25978921-f55d-4fb4-abf1-a89e6435edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제: LLM 의 출력을 -> list 로 변환시켜 보자.  (변환시키는 OutputParser 를 만들어 보자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a32250da-d4c3-4658-95cf-2d13dbb1260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.output_parsers.base import BaseOutputParser\n",
    "# https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html\n",
    "\n",
    "# ↓ 이를 상속 받아 OutputParser 를 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a00c83f-918c-48a3-99b7-a45b0bda2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommaOutputParser(BaseOutputParser):\n",
    "\n",
    "    # parse() 메소드를 반.드.시 구현.\n",
    "    #   text = 입력텍스트\n",
    "    def parse(self, text): \n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21b22dd8-4c0d-4bc8-89ad-9d4e72a47acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = CommaOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74cf95e9-153f-4a0a-8d4d-4ba1f4c7ee61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 동작 확인\n",
    "p.parse(\"   Hello,    how,   are,   you   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ca578a1-3843-4776-842c-8999f74ff63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='1. Mercury\\n2. Venus\\n3. Earth\\n4. Mars\\n5. Jupiter\\n6. Saturn\\n7. Uranus\\n8. Neptune\\n9. Pluto', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 47, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ5dciXT79Cr7nUoVWRq1QyhjVq', 'finish_reason': 'stop', 'logprobs': None}, id='run--f42ddc99-6493-4da6-9610-6bbecbc0511d-0', usage_metadata={'input_tokens': 47, 'output_tokens': 36, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "******************************\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune\n",
      "9. Pluto\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the planets?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.__repr__())\n",
    "print('*' * 30)\n",
    "print(result.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fbda688-4ae7-49a4-b9f3-32321185e27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 49, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ5la81j0Mq77up8Cs2bTByQ6ZI', 'finish_reason': 'stop', 'logprobs': None}, id='run--e49897c5-f5d3-48cc-a702-af7c30608117-0', usage_metadata={'input_tokens': 49, 'output_tokens': 17, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "******************************\n",
      "Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a comma separated list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the planets?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.__repr__())\n",
    "print('*' * 30)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ac815cb-21a2-43bf-a453-192801f7cb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='red, blue, green, yellow, orange, purple, pink, black, white, brown', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 49, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ7jy3Ph2e8gxndDseFLzMS3Yp3', 'finish_reason': 'stop', 'logprobs': None}, id='run--56ebfa92-b848-408e-b492-39dd1c7efc1e-0', usage_metadata={'input_tokens': 49, 'output_tokens': 19, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "******************************\n",
      "red, blue, green, yellow, orange, purple, pink, black, white, brown\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a comma separated list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the colors?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.__repr__())\n",
    "print('*' * 30)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7575fd7b-57df-4d04-ad37-f20b5367f3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='red, blue, green, yellow, orange, purple, pink, black, white, brown', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 53, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ7sNgVF0nF8mDK88c0R2O0vDRO', 'finish_reason': 'stop', 'logprobs': None}, id='run--6dc09180-1f61-4a15-8f82-e52febc9316c-0', usage_metadata={'input_tokens': 53, 'output_tokens': 19, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "******************************\n",
      "red, blue, green, yellow, orange, purple, pink, black, white, brown\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered \n",
    "        with a comma separated list of max {max_items} in lowercase.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the colors?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.__repr__())\n",
    "print('*' * 30)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa00ea5f-9722-4c48-a3b3-f91cd89750d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'orange',\n",
       " 'purple',\n",
       " 'pink',\n",
       " 'black',\n",
       " 'white',\n",
       " 'brown']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered \n",
    "        with a comma separated list of max {max_items} in lowercase.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the colors?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c356e142-1d4b-4de2-8438-eb86103c39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "template -> format -> invoke -> parser ...\n",
    "\n",
    "너무 단계가 많아 보인다..   이를 죄다 직접 하드 코딩하다니?\n",
    "\n",
    "↓ LCEL 을 사용하면 위 과정들이 많~이 생략된다!\n",
    "   => 바로 chain 의 등장!\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34575f-623b-4375-b49d-aec0a90fa2d7",
   "metadata": {},
   "source": [
    "## Chain, LCEL\n",
    "\n",
    "- LCEL (LangChain Expression Language: 랭체인 표현 언어)\n",
    "  - LCEL은 LangChain 내에서 복잡한 표현식을 처리하고,\n",
    "  - 모델과의 상호작용을 더 강력하고 유연하게 만드는 기능을 제공\n",
    "    - 코드양을 많이 줄여줌.\n",
    "    - 다양한 template 과 LLM 호출\n",
    "    - 서로 다른 응답(response) 를 함께 사용케 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6289d66-720a-4abd-be51-bbc2c01acde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['max_items', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['max_items'], input_types={}, partial_variables={}, template='You are a list generating machine.\\n        Everything you are asked will be answered \\n        with a comma separated list of max {max_items} in lowercase.\\n        Do NOT reply with anything else.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000226DABD66F0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000226DABD4500>, root_client=<openai.OpenAI object at 0x00000226DA597BC0>, root_async_client=<openai.AsyncOpenAI object at 0x00000226DABD6720>, temperature=0.1, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| CommaOutputParser()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain 생성!\n",
    "#  '|' 연산자 사용\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "print(type(chain))\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46b9ccb6-1f90-4b04-8266-4a43eaa20d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pikachu', 'charmander', 'bulbasaur', 'squirtle', 'jigglypuff']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain 호출!  invoke({...})\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\": \"What are the pokemons?\",  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68c1a689-4157-4a6c-892d-d9b841aac879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "↑ Chain 을 사용해 꽤나 간결한 코드로 작동시켰는지 보라!\n",
    "\n",
    "사실 랭체인은 내부에서\n",
    "  .format_message() 호출\n",
    "  chat.invoke() 호출\n",
    "  parse() 호출한다\n",
    "\n",
    "이러한 일련의 작업을 chain.invoke() 호출 단한번으로 끝낸다.\n",
    "\n",
    "이러한 chain 구문으로 정말 다양한 작업의 흐름들을 수행할수 있다.\n",
    "\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1b6610e-8669-404b-86dd-acfa5d2bee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "chain 끼리도 결합할수 도 있다.\n",
    "\n",
    "[예시]\n",
    "chain_one = template | chat | CommaOutputParser()\n",
    "chain_two = template_2 | chat | OutputParser2()\n",
    "\n",
    "all = chain_one | chain_two | OutputParser3()\n",
    "  ↑ chain_one 의 출력을 chain_two 의 입력값으로 사용 가능.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800327f4-5036-4497-acdc-975097b62a82",
   "metadata": {},
   "source": [
    "# Chaining Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a35ce4e-c94a-4636-80fa-3fb515df1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  공식]\n",
    "#  https://python.langchain.com/docs/concepts/lcel/\n",
    "#  https://python.langchain.com/docs/how_to/#langchain-expression-language-lcel\n",
    "#  https://python.langchain.com/docs/how_to/lcel_cheatsheet/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afecef-ccaa-4808-9b3e-ce9b29171afc",
   "metadata": {},
   "source": [
    "## LCEL 의 input / output\n",
    "\n",
    "LangChain Expression Language (LCEL)은 LangChain에서 다양한 입력 유형을 활용하여 LLM과 도구를 결합하고 데이터 흐름을 제어하는 언어입니다. LCEL은 LLM의 입력과 처리에 사용되는 **입력 타입**(Input Types)을 명확하게 정의하여, 사용자 인터페이스와 도구 간의 상호작용을 더욱 효율적으로 만듭니다.\n",
    "\n",
    "아래는 LCEL에서 자주 사용되는 주요 **입력 타입**에 대한 설명입니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Plain Text**\n",
    "- **설명**: 단순한 텍스트 입력입니다. 이 형식은 가장 기본적인 입력으로, LLM이 자유로운 자연어 처리를 수행할 수 있도록 합니다.\n",
    "- **예시**:\n",
    "  ```plaintext\n",
    "  What is the capital of France?\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - 텍스트 분석, 생성 및 대화형 작업에 적합.\n",
    "  - 추가적인 구조나 메타데이터 없이 단순 텍스트로 전달.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Structured Input**\n",
    "- **설명**: JSON, 딕셔너리, 또는 구조화된 형식의 입력입니다. 데이터 필드가 명시적으로 정의되어 있으며, 모델이 이 구조에 따라 데이터를 처리합니다.\n",
    "- **예시**:\n",
    "  ```json\n",
    "  {\n",
    "      \"question\": \"What is the capital of France?\",\n",
    "      \"context\": \"France is a country in Europe.\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - 명시적인 데이터 필드를 통해 LLM이 필요한 정보를 더 정확히 추출 및 활용 가능.\n",
    "  - 복잡한 데이터 분석이나 멀티 필드 처리가 필요한 작업에 유용.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Prompt Templates**\n",
    "- **설명**: 사용자가 정의한 프롬프트 템플릿을 입력으로 사용합니다. 템플릿에 변수 값을 채워 넣어 모델에 전달합니다.\n",
    "- **예시**:\n",
    "  ```python\n",
    "  template = \"Translate the following text to French: {text}\"\n",
    "  input = template.format(text=\"Hello, how are you?\")\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - 변수 기반 입력을 통해 재사용 가능성이 높음.\n",
    "  - 사용자 정의 입력 생성 및 제어에 적합.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Key-Value Pairs**\n",
    "- **설명**: 키-값 쌍의 입력 형식으로, 명시적인 쿼리 형태로 정보를 제공합니다.\n",
    "- **예시**:\n",
    "  ```python\n",
    "  {\n",
    "      \"name\": \"John\",\n",
    "      \"age\": 30,\n",
    "      \"location\": \"New York\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - 정형화된 데이터를 제공하여 LLM이 더 효율적으로 데이터를 분석 및 처리할 수 있음.\n",
    "  - 특정 정보 필드가 명확히 필요할 때 유용.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Multi-modal Inputs**\n",
    "- **설명**: 텍스트, 이미지, 오디오 등 다양한 데이터 유형을 조합한 입력 형식입니다.\n",
    "- **예시**:\n",
    "  ```python\n",
    "  {\n",
    "      \"text\": \"Describe the image.\",\n",
    "      \"image\": \"<image_data>\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - 멀티모달 모델과 통합하여 다양한 입력 형식을 처리 가능.\n",
    "  - 이미지 캡셔닝, 오디오-텍스트 변환 등의 작업에서 활용.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Serialized Inputs**\n",
    "- **설명**: 입력 데이터를 시리얼화(Serialize)하여 특정 형식으로 변환한 입력입니다. 예를 들어, JSON 문자열로 데이터를 전달합니다.\n",
    "- **예시**:\n",
    "  ```python\n",
    "  input = '{\"question\": \"What is the capital of France?\", \"context\": \"France is in Europe.\"}'\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - 데이터가 외부 시스템이나 API와 통신할 때 유용.\n",
    "  - 데이터 포맷에 대한 유연성이 높음.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Chat Messages**\n",
    "- **설명**: 채팅 메시지 형식의 입력으로, 사용자가 역할(role)과 내용(content)을 정의하여 LLM에게 대화 형식으로 정보를 전달합니다.\n",
    "- **예시**:\n",
    "  ```python\n",
    "  [\n",
    "      {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": \"What is the weather today?\"}\n",
    "  ]\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - ChatGPT 같은 대화형 모델에 적합.\n",
    "  - 대화의 맥락을 유지하고 다중 발화 입력을 처리할 수 있음.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Custom Input Types**\n",
    "- **설명**: 사용자가 애플리케이션 요구 사항에 따라 정의하는 커스텀 입력 형식입니다.\n",
    "- **예시**:\n",
    "  ```python\n",
    "  class CustomInput:\n",
    "      def __init__(self, field1, field2):\n",
    "          self.field1 = field1\n",
    "          self.field2 = field2\n",
    "  ```\n",
    "\n",
    "- **특징**:\n",
    "  - 특정 애플리케이션 로직과 완벽히 맞는 형식으로 데이터 처리.\n",
    "  - 표준 입력 타입으로 표현하기 어려운 복잡한 구조를 다룰 때 유용.\n",
    "\n",
    "---\n",
    "\n",
    "### 요약\n",
    "LCEL의 입력 타입은 단순 텍스트부터 구조화된 데이터, 멀티모달 입력까지 다양하게 제공되며, 각 타입은 특정 용도에 맞게 설계되었습니다. 입력 데이터를 정교하게 설계하고 적절한 형식을 선택함으로써 모델의 성능을 최적화할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a825bc5-17c2-465e-b9b3-80bca74f6f37",
   "metadata": {},
   "source": [
    "## Chain 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9d28149-05a5-4ec0-ace3-20fcad2fc784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 chain 출력값을 다른 chain 의 입력값으로 전달 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b2536de-7dc7-42c6-8ff3-1f884f92e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 chain\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "      You are a world-class international chef.\n",
    "      You create easy to follow recipes for any type of cuisines\n",
    "      with easy to find ingredients.        \n",
    "    \"\"\"\n",
    "    ),\n",
    "    (\"human\", \"\"\"\n",
    "        I want to cook {cuisine} food.\n",
    "    \"\"\" ),\n",
    "])\n",
    "\n",
    "chef_chain = chef_prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd845bd-cd06-4921-9b39-89db714a1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 Chef 에게서 레시피를 받게 될텐데\n",
    "# 두번째 chain 에선 채식 재료만 사용하도록 변형 할겁니다.\n",
    "\n",
    "# 작업은 두개\n",
    "#   1. 레시피를 전달해주는 셰프\n",
    "#   2. 채식주의자를 위한 셰프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8624874c-4b16-4611-b115-390356b4e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두번째 chain\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "        You are a vegetarian chef specialized on\n",
    "      making traditional recipies vegetarian.\n",
    "      You find alternative ingredients and explain their preparation.\n",
    "      You don't radically modify the recipe.\n",
    "      If there is no alternative for a foot just say\n",
    "      you don't know how to replace it.\n",
    "    \"\"\"),\n",
    "    (\"human\", \"\"\"\n",
    "        {recipe}\n",
    "    \"\"\"),\n",
    "])\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cce79ef5-0eea-4bd9-81f2-1a716ac1c79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative such as tofu or paneer. Here's how you can adapt the recipe:\\n\\nIngredients:\\n- 1 lb firm tofu or paneer, cut into bite-sized pieces\\n- 1 cup plain yogurt (you can use dairy-free yogurt for a vegan version)\\n- 2 tablespoons lemon juice\\n- 2 tablespoons vegetable oil\\n- 2 tablespoons garam masala\\n- 1 tablespoon ground cumin\\n- 1 tablespoon ground coriander\\n- 1 teaspoon turmeric\\n- 1 teaspoon paprika\\n- 1 teaspoon chili powder (adjust to taste)\\n- Salt and pepper to taste\\n- 1 onion, finely chopped\\n- 3 cloves garlic, minced\\n- 1-inch piece of ginger, grated\\n- 1 can (14 oz) tomato sauce\\n- 1 cup coconut cream (or dairy-free heavy cream alternative)\\n- Fresh cilantro, chopped (for garnish)\\n- Cooked rice or naan bread (to serve)\\n\\nInstructions:\\n1. Follow the same marinating process as the original recipe, but use tofu or paneer instead of chicken. Marinate the tofu or paneer in the yogurt and spice mixture for at least 1 hour.\\n2. Instead of baking, you can pan-fry the marinated tofu or paneer until golden brown and cooked through.\\n3. Proceed with the recipe as instructed, replacing the chicken with the cooked tofu or paneer when adding it to the sauce.\\n4. Simmer the tofu or paneer in the sauce for 10-15 minutes to allow the flavors to meld together.\\n5. Taste and adjust the seasoning as needed.\\n6. Serve the Vegetarian Tikka Masala over rice or with naan bread, garnished with fresh cilantro.\\n\\nEnjoy your vegetarian twist on this classic Indian dish!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 386, 'prompt_tokens': 808, 'total_tokens': 1194, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZYIvb04IwMKB1LBZv18lPVXih4r', 'finish_reason': 'stop', 'logprobs': None}, id='run--57a97b5d-dede-4708-a2f4-5e1d5999c0b5-0', usage_metadata={'input_tokens': 808, 'output_tokens': 386, 'total_tokens': 1194, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final chain\n",
    "\n",
    "# final_chain = chef_chain | veg_chain  \n",
    "\n",
    "final_chain = {\"recipe\" : chef_chain} | veg_chain\n",
    "\n",
    "result = final_chain.invoke({\n",
    "    \"cuisine\": \"indian\",  # chef_chain 의 {cuisine} 에 전달\n",
    "})\n",
    "\n",
    "result  # chain 두번 호출...   답변 완성까지 제법 시간이 걸린다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "708b5636-7dc8-478e-94cc-0d619f122950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative such as tofu or paneer. Here's how you can adapt the recipe:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb firm tofu or paneer, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt (you can use dairy-free yogurt for a vegan version)\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 tablespoons vegetable oil\n",
      "- 2 tablespoons garam masala\n",
      "- 1 tablespoon ground cumin\n",
      "- 1 tablespoon ground coriander\n",
      "- 1 teaspoon turmeric\n",
      "- 1 teaspoon paprika\n",
      "- 1 teaspoon chili powder (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 1 onion, finely chopped\n",
      "- 3 cloves garlic, minced\n",
      "- 1-inch piece of ginger, grated\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup coconut cream (or dairy-free heavy cream alternative)\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (to serve)\n",
      "\n",
      "Instructions:\n",
      "1. Follow the same marinating process as the original recipe, but use tofu or paneer instead of chicken. Marinate the tofu or paneer in the yogurt and spice mixture for at least 1 hour.\n",
      "2. Instead of baking, you can pan-fry the marinated tofu or paneer until golden brown and cooked through.\n",
      "3. Proceed with the recipe as instructed, replacing the chicken with the cooked tofu or paneer when adding it to the sauce.\n",
      "4. Simmer the tofu or paneer in the sauce for 10-15 minutes to allow the flavors to meld together.\n",
      "5. Taste and adjust the seasoning as needed.\n",
      "6. Serve the Vegetarian Tikka Masala over rice or with naan bread, garnished with fresh cilantro.\n",
      "\n",
      "Enjoy your vegetarian twist on this classic Indian dish!\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ded65d-7a0f-4210-bca1-36b6628889b7",
   "metadata": {},
   "source": [
    "## streaming="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df719514-52b9-46c5-9a4d-232d52f27db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ 전부 실행 완료 될때까지 기다리는게 지루하다.\n",
    "#   어떻게 진행되는지도 궁금하다.\n",
    "#   진행되는 과정을 실시간으로 출력 할수 있다!\n",
    "\n",
    "# Chat model 의 streaming=\n",
    "#  streaming 은 LLM model 의 응답(resposne) 이 생성되는 것을\n",
    "#    실시간으로(?) 보게 해줌.\n",
    "\n",
    "# callbacks=[StreamingStdOutCallbackHandler()]\n",
    "#    볼수 있는 문자(토큰)가 생길 때마다 print 해준다.\n",
    "\n",
    "# callbacks 는 다양한 'event' 감지도 가능\n",
    "#    LLM 이 작업을 시작했다거나, 끝냈다거나.\n",
    "#    문자를 생성했다거나, 에러가 발생하거나.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25d3ad6a-4a3b-424f-a3c0-0f7fbad2eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3    https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e11710c0-5105-4e43-ae01-83516489863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "877423a4-032a-4e85-829d-cf5f9e26a030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great choice! Indian cuisine is full of delicious flavors and spices. Let's make a classic dish - Chicken Tikka Masala. Here's a simple recipe for you to try at home:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb boneless, skinless chicken breasts, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 tablespoons vegetable oil\n",
      "- 2 cloves garlic, minced\n",
      "- 1 tablespoon grated ginger\n",
      "- 1 tablespoon garam masala\n",
      "- 1 teaspoon ground cumin\n",
      "- 1 teaspoon ground coriander\n",
      "- 1 teaspoon paprika\n",
      "- 1 teaspoon turmeric\n",
      "- 1 teaspoon chili powder (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup heavy cream\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (for serving)\n",
      "\n",
      "Instructions:\n",
      "1. In a bowl, mix together yogurt, lemon juice, 1 tablespoon vegetable oil, garlic, ginger, garam masala, cumin, coriander, paprika, turmeric, chili powder, salt, and pepper. Add the chicken pieces and coat them well with the marinade. Cover and refrigerate for at least 1 hour, or overnight for best results.\n",
      "\n",
      "2. Preheat the oven to 400°F (200°C). Thread the marinated chicken onto skewers and place them on a baking sheet. Bake for 20-25 minutes or until the chicken is cooked through.\n",
      "\n",
      "3. In a large skillet, heat the remaining tablespoon of vegetable oil over medium heat. Add the tomato sauce and simmer for 5 minutes.\n",
      "\n",
      "4. Stir in the heavy cream and cooked chicken tikka. Simmer for another 10 minutes, stirring occasionally.\n",
      "\n",
      "5. Taste and adjust seasoning if needed. Serve the Chicken Tikka Masala over rice or with naan bread. Garnish with chopped cilantro.\n",
      "\n",
      "Enjoy your homemade Chicken Tikka Masala! Feel free to adjust the spice levels to suit your taste preferences. Let me know if you have any questions or need more recipes.For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative like tofu or paneer. Here's how you can adapt the recipe:\n",
      "\n",
      "**Ingredients:**\n",
      "- 1 lb firm tofu or paneer, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt (you can use dairy-free yogurt for a vegan version)\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 tablespoons vegetable oil\n",
      "- 2 cloves garlic, minced\n",
      "- 1 tablespoon grated ginger\n",
      "- 1 tablespoon garam masala\n",
      "- 1 teaspoon ground cumin\n",
      "- 1 teaspoon ground coriander\n",
      "- 1 teaspoon paprika\n",
      "- 1 teaspoon turmeric\n",
      "- 1 teaspoon chili powder (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup coconut cream (or any plant-based heavy cream)\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (for serving)\n",
      "\n",
      "**Instructions:**\n",
      "1. Follow the same marinating process as the original recipe, but use tofu or paneer instead of chicken. Marinate the tofu or paneer for at least 1 hour.\n",
      "   \n",
      "2. Instead of baking, you can pan-fry the marinated tofu or paneer until they are golden brown and cooked through.\n",
      "\n",
      "3. In the skillet, proceed with the recipe by heating the vegetable oil, adding the tomato sauce, and simmering for 5 minutes.\n",
      "\n",
      "4. Stir in the coconut cream (or plant-based heavy cream) and the cooked tofu or paneer. Simmer for another 10 minutes to allow the flavors to meld together.\n",
      "\n",
      "5. Adjust the seasoning if needed and serve the Vegetarian Tikka Masala over rice or with naan bread. Garnish with chopped cilantro.\n",
      "\n",
      "By making these simple swaps, you can enjoy a flavorful and satisfying Vegetarian Tikka Masala that stays true to the essence of the traditional dish. Enjoy your meal!For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative like tofu or paneer. Here's how you can adapt the recipe:\n",
      "\n",
      "**Ingredients:**\n",
      "- 1 lb firm tofu or paneer, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt (you can use dairy-free yogurt for a vegan version)\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 tablespoons vegetable oil\n",
      "- 2 cloves garlic, minced\n",
      "- 1 tablespoon grated ginger\n",
      "- 1 tablespoon garam masala\n",
      "- 1 teaspoon ground cumin\n",
      "- 1 teaspoon ground coriander\n",
      "- 1 teaspoon paprika\n",
      "- 1 teaspoon turmeric\n",
      "- 1 teaspoon chili powder (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup coconut cream (or any plant-based heavy cream)\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (for serving)\n",
      "\n",
      "**Instructions:**\n",
      "1. Follow the same marinating process as the original recipe, but use tofu or paneer instead of chicken. Marinate the tofu or paneer for at least 1 hour.\n",
      "   \n",
      "2. Instead of baking, you can pan-fry the marinated tofu or paneer until they are golden brown and cooked through.\n",
      "\n",
      "3. In the skillet, proceed with the recipe by heating the vegetable oil, adding the tomato sauce, and simmering for 5 minutes.\n",
      "\n",
      "4. Stir in the coconut cream (or plant-based heavy cream) and the cooked tofu or paneer. Simmer for another 10 minutes to allow the flavors to meld together.\n",
      "\n",
      "5. Adjust the seasoning if needed and serve the Vegetarian Tikka Masala over rice or with naan bread. Garnish with chopped cilantro.\n",
      "\n",
      "By making these simple swaps, you can enjoy a flavorful and satisfying Vegetarian Tikka Masala that stays true to the essence of the traditional dish. Enjoy your meal!\n"
     ]
    }
   ],
   "source": [
    "# 위 Chat model 로 다시 실행해보기\n",
    "chef_chain = chef_prompt | chat\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "result = final_chain.invoke({\n",
    "    \"cuisine\": \"indian\",  # chef_chain 의 {cuisine} 에 전달\n",
    "})\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d7474-e2b8-41f2-886a-c01ae7fe9252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9aec4-177b-4f6a-907b-f9dcb4390977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13547f91-a75b-42da-b03a-66c1e387c2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b2a06-e66b-4b95-84a1-7936e689487a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b347c7-b38f-4c0d-a600-11cafeec6234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7471d548-21d7-4521-9d1b-181a53b27fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7a077-1fd9-4a32-a380-d9c56dba1dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455013df-906a-45db-a46b-10357295d07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7056f67-cce0-4cc9-9bf8-b3f413639f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac041dc-5f3b-42d3-b431-632bbae18599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
