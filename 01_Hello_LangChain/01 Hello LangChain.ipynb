{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f451fb8-048c-455b-a5a6-df7d5b426123",
   "metadata": {},
   "source": [
    "# Hello LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712498c-b9a3-4917-84f1-d000869ffb91",
   "metadata": {},
   "source": [
    "# LangChain  ê´€ë ¨ ì£¼ìš” ë§í¬\n",
    "\n",
    "-  Python Langchain ê³µì‹ í™ˆ:  https://python.langchain.com/\n",
    "-  API ë ˆí¼ëŸ°ìŠ¤ í™ˆ: https://python.langchain.com/api_reference/reference.html\n",
    "\n",
    "\n",
    "## Langchain ì˜ íŒ¨í‚¤ì§€ êµ¬ì„±\n",
    "\n",
    "\n",
    "### Base Packages\n",
    "- [Core: langchain-core](https://python.langchain.com/api_reference/core)\n",
    "- [Langchain: langchain](https://python.langchain.com/api_reference/langchain)\n",
    "- [Test Splitters: langchain-text-splitters](https://python.langchain.com/api_reference/text_splitters)\n",
    "- [Community: langchain-community](https://python.langchain.com/api_reference/community)\n",
    "- [Experimental: langchain-experimental](https://python.langchain.com/api_reference/experimental)\n",
    "\n",
    "### Integrations\n",
    "- ë­ì²´ì¸ì€ ìˆ˜ë§ì€ LLM ëª¨ë¸ë“¤ê³¼ ì»¤ë®¤ë‹ˆí‹°, ë²¡í„°ìŠ¤í† ì–´, ë°ì´í„°ë² ì´ìŠ¤, íˆ´ ë“¤ê³¼ í•¨ê»˜ ì‚¬ìš©í• ìˆ˜ ìˆë„ë¡ ì œê³µë˜ëŠ” íŒ¨í‚¤ì§€ë“¤ì´ ë§ë‹¤ (ì•ìœ¼ë¡œ ë” ë§ì•„ ì§ˆê±°ë‹¤)\n",
    "- [OpanAI: langchain-openai](https://python.langchain.com/api_reference/openai)\n",
    "- [Huggingface: langchain-huggingface](https://python.langchain.com/api_reference/huggingface)\n",
    "- [MistalAI: langchain-mistralai](https://python.langchain.com/api_reference/mistralai)\n",
    "- ê·¸ë°–ì—ë„ ë§ì´ ìˆë‹¤ ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756d471-8be8-46f0-8e42-d4ebbd9dd6d7",
   "metadata": {},
   "source": [
    "# í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7052afee-5248-40bf-a37e-db43ec58a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3345ce0-c96c-49a1-a8a6-704f2dd371cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcdda514-0b65-4836-a31a-d88b1696a2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-proj-iKU13YeoxNgF...'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'][:20] + '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dbc823e-6c98-4913-82bb-66f150880242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜ë™ìœ¼ë¡œ ì½ì–´ì˜¤ê¸°\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94b5270c-06a1-41c3-992d-fd9461e96cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89c72428-ef63-4745-ad04-ed8bf45a5fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-iKU13YeoxNgF...\n",
      "sk-proj-iKU13YeoxNgF...\n"
     ]
    }
   ],
   "source": [
    "print(f'{os.environ['OPENAI_API_KEY'][:20]}...')\n",
    "print(f'{os.getenv('OPENAI_API_KEY')[:20]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a58299a-3800-4831-b66d-0bc354e3f2b5",
   "metadata": {},
   "source": [
    "# â–  LLM vs. Chat model\n",
    "\n",
    "LangChain ì€ LLM ê³¼ Chat model ë‘ê°€ì§€ë¥¼ ì§€ì›í•©ë‹ˆë‹¤\n",
    "\n",
    "`LLM`(Large Language Model)ê³¼ `Chat Model`ì€ ë¹„ìŠ·í•œ ì—­í• ì„ í•˜ì§€ë§Œ, ì•½ê°„ì˜ ì°¨ì´ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŠ” ì£¼ë¡œ **ëª¨ë¸ì˜ ì…ë ¥ ë° ìƒí˜¸ì‘ìš© ë°©ì‹**ì—ì„œ ë‚˜íƒ€ë‚œë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. LLM (Large Language Model)\n",
    "- **íŠ¹ì§•**:\n",
    "  - ì¼ë°˜ì ìœ¼ë¡œ **í…ìŠ¤íŠ¸ ì…ë ¥**ì„ ë°›ê³ , ì´ì— ëŒ€í•œ í…ìŠ¤íŠ¸ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "  - ë‹¨ìˆœí•œ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì…ë ¥/ì¶œë ¥ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "  - ì‚¬ìš©ìê°€ ì œê³µí•œ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³ , ê·¸ì— ëŒ€í•œ ê²°ê³¼ë¥¼ í•œ ë²ˆì— ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "- **ì…ë ¥ í˜•ì‹**:\n",
    "  ```plaintext\n",
    "  \"Tell me a summary of the benefits of LangChain.\"\n",
    "  ```\n",
    "- **ì¶œë ¥ í˜•ì‹**:\n",
    "  ```plaintext\n",
    "  \"LangChain is a framework designed to simplify the development of applications powered by large language models, making it easier to manage prompts, chains, and integrations.\"\n",
    "  ```\n",
    "- **ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€**:\n",
    "  - ë‹¨ì¼ ì§ˆë¬¸-ë‹µë³€\n",
    "  - í…ìŠ¤íŠ¸ ìƒì„±\n",
    "  - ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ì—…\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Chat Model\n",
    "- **íŠ¹ì§•**:\n",
    "  - **ëŒ€í™” í˜•ì‹**ìœ¼ë¡œ ì„¤ê³„ëœ ëª¨ë¸ë¡œ, ë‹¤ì¤‘ í„´ ëŒ€í™”ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.\n",
    "  - ì…ë ¥ í˜•ì‹ì´ **ë©”ì‹œì§€ Message**ë¡œ êµ¬ì„±ë˜ë©°, ê° ë©”ì‹œì§€ëŠ” ì‚¬ìš©ìì˜ ë©”ì‹œì§€ (User Message)ì™€ ì‹œìŠ¤í…œì˜ ë©”ì‹œì§€(System Message)ë¡œ ë‚˜ë‰œë‹¤.\n",
    "  - 'ë¬¸ë§¥'ì„ ì´í•´í•˜ê³  'ëŒ€í™”ì˜ íë¦„'ì„ ìœ ì§€í•˜ëŠ” ë° ìµœì í™”ë˜ì–´ ìˆë‹¤.\n",
    "- **ì…ë ¥ í˜•ì‹**:\n",
    "  ë©”ì‹œì§€ ê°ì²´ë¥¼ ì „ë‹¬í•´ì•¼ í•˜ë©°, ë³´í†µ ì•„ë˜ì™€ ê°™ì€ êµ¬ì¡°ì…ë‹ˆë‹¤.\n",
    "  ```python\n",
    "  [\n",
    "      {\"role\": \"system\", \"content\": \"You are an assistant who helps with Python programming.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Can you explain the difference between LLM and chat models in LangChain?\"}\n",
    "  ]\n",
    "  ```\n",
    "- **ì¶œë ¥ í˜•ì‹**:\n",
    "  ```python\n",
    "  {\"role\": \"assistant\", \"content\": \"Sure! LLM and Chat Models differ in their input and interaction styles...\"}\n",
    "  ```\n",
    "- **ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€**:\n",
    "  - ë‹¤ì¤‘ í„´ ëŒ€í™”\n",
    "  - ë¬¸ë§¥ ì¶”ì  ë° ìœ ì§€ (ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜ì˜)\n",
    "  - ëŒ€í™” ê¸°ë°˜ ì±—ë´‡, FAQ ì‹œìŠ¤í…œ ë“±\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ì£¼ìš” ì°¨ì´ì  ìš”ì•½\n",
    "| **íŠ¹ì§•**        | **LLM**                                                | **Chat Model**                                        |\n",
    "|-----------------|------------------------------------------------------|----------------------------------------------------|\n",
    "| **ì…ë ¥ í˜•ì‹**   | ë‹¨ì¼ í…ìŠ¤íŠ¸ ì…ë ¥                                         | ì—­í•  ê¸°ë°˜ì˜ ëŒ€í™” ë©”ì‹œì§€ ê°ì²´ (role: system, user, assistant) |\n",
    "| **ëŒ€í™” íˆìŠ¤í† ë¦¬**| ë¬¸ë§¥ ì¶”ì  ë¶ˆê°€ëŠ¥ (ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬)                          | ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í†µí•´ ë¬¸ë§¥ì„ ìœ ì§€í•˜ê³  ë°˜ì˜                 |\n",
    "| **ì‚¬ìš© ëª©ì **   | í…ìŠ¤íŠ¸ ìƒì„±, ìš”ì•½, ë‹¨ìˆœ ì§ˆì˜ì‘ë‹µ                             | ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤, ì±—ë´‡, ë‹¤ì¤‘ í„´ ì§ˆì˜ì‘ë‹µ               |\n",
    "| **ì‘ìš© ì‚¬ë¡€**   | ë‹¨ì¼ ì§ˆë¬¸-ë‹µë³€, í…ìŠ¤íŠ¸ ìƒì„±                                | ê³ ê° ì§€ì› ì±—ë´‡, ì¸í„°ë™í‹°ë¸Œ Q&A, ë©€í‹°í„´ ëŒ€í™” ì‹œìŠ¤í…œ          |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 5. ì–¸ì œ ì–´ë–¤ ê²ƒì„ ì„ íƒí•´ì•¼ í• ê¹Œìš”?\n",
    "- **ë‹¨ì¼ ì‘ì—…ì´ë‚˜ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„±**:\n",
    "  - `LLM`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì í•©í•©ë‹ˆë‹¤.\n",
    "- **ëŒ€í™” ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ë‚˜ ë¬¸ë§¥ì„ ìœ ì§€í•´ì•¼ í•˜ëŠ” ì‘ì—…**:\n",
    "  - `Chat Model`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì í•©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ccee37-7d81-4b7a-acc2-1aae8ed333ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.23'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3551daf4-a602-4ddd-9a8c-5fc8f2a7c0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms.base import OpenAI  # LLM   (gpt-3.5-turbo ì‚¬ìš©  2024.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28c409e5-10c0-4c0d-b84a-99b642736de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models.base import ChatOpenAI  # Chat model  (gpt-3.5-turbo ì‚¬ìš©  2024.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09a2b142-c3dc-4fb9-bb81-6c416e353245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë­ì²´ì¸ì„ ì‚¬ìš©í•˜ë©´!\n",
    "# ê° model ë“¤ì˜ API ì„ ë”°ë¡œë”°ë¡œ ê³µë¶€í•˜ê³  ì•Œì•„ì•¼ í•  í•„ìš”ì—†ë‹¤.\n",
    "# ë˜í•œ ê° model api ì œê³µìê°€ ì œê³µí•˜ëŠ” Python package ë¥¼ ë‹¤ìš´ë¡œë“œ í•  í•„ìš”ë„ ì—†ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bc4964e-f686-4625-b10b-26e38366af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ebc1b90-affc-4258-a500-491890c12b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo-instruct'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46396760-901c-49e2-b2da-7c8ba354780f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI()\n",
    "\n",
    "chat.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f37149-925a-48ba-b182-7ff7fa6b668f",
   "metadata": {},
   "source": [
    "# LLM í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ffbb999-4e0e-4f25-9cd9-7b90406f8de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "ë‹µë³€ \n",
      "\n",
      "There are eight planets in our solar system:\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"How many planets are there?\")\n",
    "print(type(result))\n",
    "print('ë‹µë³€', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ce83bc0-9a66-411c-bb48-6e31290cf5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "ë‹µë³€ \n",
      "\n",
      "íƒœì–‘ê³„ì—ëŠ” 8ê°œì˜ í–‰ì„±ì´ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ì€ ìˆ˜ì„±, ê¸ˆì„±, ì§€êµ¬, í™”ì„±, ëª©ì„±, í† ì„±, ì²œì™•ì„±, í•´ì™•ì„±ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"íƒœì–‘ê³„ì—ëŠ” ì–¼ë§ˆë‚˜ ë§ì€ í–‰ì„±ë“¤ì´ ìˆë‚˜ìš”? ê·¸ í–‰ì„±ë“¤ì˜ ì´ë¦„ë„ ì•Œë ¤ì¤˜.\")\n",
    "print(type(result))\n",
    "print('ë‹µë³€', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13cf0a-997e-4feb-abde-e476273087b7",
   "metadata": {},
   "source": [
    "# ChatModel í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08679f59-0a61-47d8-b573-ce8bca5b5424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "ğŸ’š content='In our solar system, there are 8 planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 13, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ025LB5WoIyavTWn9M3bVpxgPS', 'finish_reason': 'stop', 'logprobs': None} id='run--fd10c442-8bde-478c-8301-c4642f620ff4-0' usage_metadata={'input_tokens': 13, 'output_tokens': 29, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "ğŸ§¡ë‹µë³€ In our solar system, there are 8 planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\n"
     ]
    }
   ],
   "source": [
    "result = chat.invoke(\"How many planets are there?\")\n",
    "print(type(result))  # Messageê°ì²´\n",
    "print('ğŸ’š', result)\n",
    "print('ğŸ§¡ë‹µë³€', result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f000845-966e-447e-9760-2756fceeca5f",
   "metadata": {},
   "source": [
    "## í•œê¸€ or ì˜ì–´ ?\n",
    "\n",
    "ì±— GPTì˜ ì–¸ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ì€ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ í›Œë¥­í•œ ë°œì „ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‚¬ìš©ìê°€ ë°›ëŠ” ë‹µë³€ì˜ í’ˆì§ˆì€ ì œì¶œí•˜ëŠ” ì–¸ì–´ì— ë”°ë¼ ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ì°¨ì´ëŠ” ì±— GPTê°€ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ ë‹¤ì–‘í•œ ì–¸ì–´ì˜ ë°ì´í„° ì–‘ê³¼ í’ˆì§ˆ, ê·¸ë¦¬ê³  ì–¸ì–´ë³„ íŠ¹ì„±ì„ ì–¼ë§ˆë‚˜ ì˜ ì²˜ë¦¬í•˜ëŠ”ì§€ì— ë”°ë¼ ê²°ì •ë©ë‹ˆë‹¤.\n",
    "\n",
    "OpenAIì˜ ì–¸ì–´ ëª¨ë¸, íŠ¹íˆ GPT ì‹œë¦¬ì¦ˆëŠ” ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì–»ì€ ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¡œ í•™ìŠµë©ë‹ˆë‹¤. ì´ ë°ì´í„°ëŠ” ì£¼ë¡œ ì˜ì–´ë¥¼ ë¹„ë¡¯í•œ ì—¬ëŸ¬ ì–¸ì–´ì—ì„œ ìˆ˜ì§‘ë˜ë©°, í•™ìŠµ ë°ì´í„°ì˜ êµ¬ì„±ì€ ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ì–´ëŠ” ì „ì„¸ê³„ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ë©°*, ì¸í„°ë„· ìƒì˜ ë°ì´í„°ë„ ì˜ì–´ê°€ ë§ì•„ì„œ ì±— GPTëŠ” ì˜ì–´ ì§ˆë¬¸ì— ëŒ€í•´ ë” ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ì œê³µí•  í™•ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í•œêµ­ì–´ì™€ ê°™ì€ ë‹¤ë¥¸ ì–¸ì–´ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ê±°ë‚˜, ì–¸ì–´ì˜ ë³µì¡ì„± ë•Œë¬¸ì— ì²˜ë¦¬ê°€ ë” ì–´ë ¤ìš¸ ìˆ˜ ìˆì–´, ì´ë¡œ ì¸í•´ ë‹µë³€ì˜ í’ˆì§ˆì— ì°¨ì´ê°€ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì°¸ê³ \n",
    "- https://fastcampus.co.kr/gov_review_insightGPTlang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99e53fb-c03e-4632-a76c-f34f02a39c39",
   "metadata": {},
   "source": [
    "## ì‚¬ìš©ëŸ‰ í™•ì¸ í•´ë³´ê¸°\n",
    "ì´ì¯¤ì—ì„œ openai í˜ì´ì§€\n",
    "DASHBOARD > Usage > Activity ë¥¼ í™•ì¸í•´ë³´ì.\n",
    "ì‚¬ìš©í•œ ì–‘ì´ í‘œì‹œë ê±°ë‹¤.\n",
    "\n",
    "https://platform.openai.com/usage/activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ea5cba1-7480-47db-af08-c5e2eaff696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic.chat_models import ChatAnthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb945fe9-6037-41dc-92ed-8a1a65eb1812",
   "metadata": {},
   "source": [
    "# Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05876845-72ab-4858-b8d3-0d0a842e2db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Model ì€ 'ì§ˆë¬¸'ë§Œ ë°›ëŠ”ê²Œ ì•„ë‹ˆë¼ 'ëŒ€í™”' ë„ í• ìˆ˜ ìˆë‹¤ (Message ë¥¼ ë³´ë‚¼ìˆ˜ë„ ìˆë‹¤)\n",
    "# 'ëŒ€í™”(conversation)' ì€\n",
    "#    : ì—¬ëŸ¬ ë©”ì„¸ì§€ ë¬¶ìŒ\n",
    "#    : ìƒëŒ€ì˜ ëŒ€í™”ì˜ ë§¥ë½ì— ë§ê²Œ ëŒ€ë‹µí• ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49555589-c391-4737-9f35-46ed47a3f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/chat/openai/#instantiation\n",
    "# ë ˆí¼ëŸ°ìŠ¤ : https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html\n",
    "\"\"\"\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "        # ëª¨ë¸ì˜ ì‘ë‹µ ë‹¤ì–‘ì„±ì„ ì œì–´í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "        # ì´ëŠ” OpenAIì˜ GPT ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ë¡œ,\n",
    "        #  ìƒì„±ë˜ëŠ” í…ìŠ¤íŠ¸ì˜ ì°½ì˜ì„±ê³¼ í™•ë¥ ì  ë‹¤ì–‘ì„±(ëœë¤ì„±ì„ ì¡°ì •í•©ë‹ˆë‹¤)ã„´\n",
    "        #\n",
    "    max_tokens=None,  # model ë¦¬ ë¦¬í„´í•˜ëŠ” ê²°ê³¼ì˜ ìµœëŒ€ token ê°œìˆ˜ì§€ì •.\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    # base_url=\"...\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f59b93e-5aee-47ff-bf3a-1bd38ab55505",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dc11095-bcc7-4688-a87e-c69f2c951e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "# https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html\n",
    "\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "# https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html\n",
    "\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "# https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html\n",
    "\n",
    "# HumanMessage : ì‚¬ëŒì´ AI ì— ë³´ë‚´ëŠ” Message\n",
    "# SystemMessage : LLM ì— ì„¤ì •ë“¤ì„ ì œê³µí•˜ê¸° ìœ„í•œ Message\n",
    "# AIMessage: AI ì— ì˜í•´ ë¦¬í„´ë˜ëŠ” Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "946ebe90-51db-4c82-a036-5d1039a54c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a geography expert. And your only reply in Korean\",\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content=\"ì•ˆë…•, ë‚´ ì´ë¦„ì€ ë‘˜ë¦¬ ì•¼\",\n",
    "    ),\n",
    "    HumanMessage(\n",
    "      content=\"\"\"\n",
    "          What is the distance between Mexico and Thailand.\n",
    "          Also, what is yoru name?\n",
    "      \"\"\",  \n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d02ad89-27fc-4ffc-b69a-1cbb49637e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ë©•ì‹œì½”ì™€ íƒœêµ­ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ëŒ€ëµ 16,000kmì…ë‹ˆë‹¤. ì œ ì´ë¦„ì€ ë‘˜ë¦¬ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 62, 'total_tokens': 98, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ17vzhd1CfuizhOEtTCVXV3pFf', 'finish_reason': 'stop', 'logprobs': None}, id='run--b42370ba-56da-4c8e-aee4-8449d1624ea3-0', usage_metadata={'input_tokens': 62, 'output_tokens': 36, 'total_tokens': 98, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chat.invoke(messages)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737a10e-0168-44dc-815c-33527824867f",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453ab8c-ff24-4652-b328-5ba020937977",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "â†‘ messages ë¥¼ prompt ë¼ê³ ë„ í•¨ (?)\n",
    "- ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì œê³µë˜ëŠ” í…ìŠ¤íŠ¸ë‚˜ ë°ì´í„°\n",
    "- ëª¨ë¸ì—ê²Œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ ì§€ì‹œí•˜ê±°ë‚˜, ëª¨ë¸ì´ ìƒì„±í•  í…ìŠ¤íŠ¸ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µ\n",
    "- LLM ê³¼ ì˜ì‚¬ì†Œí†µí•˜ê¸° ìœ„í•œ ë°©ë²•\n",
    "\n",
    "---\n",
    "\n",
    "LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì—ì„œ **í”„ë¡¬í”„íŠ¸ prompt**ë€ ëª¨ë¸ì— **ì…ë ¥**ìœ¼ë¡œ ì œê³µë˜ëŠ” í…ìŠ¤íŠ¸ë‚˜ ë°ì´í„°ì…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì—ê²Œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ ì§€ì‹œí•˜ê±°ë‚˜, ëª¨ë¸ì´ ìƒì„±í•  í…ìŠ¤íŠ¸ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ëŠ” ëª¨ë¸ì˜ ì¶œë ¥ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ì˜ ì—­í• :\n",
    "1. **ëª¨ë¸ì— ëŒ€í•œ ì§€ì‹œ**: í”„ë¡¬í”„íŠ¸ëŠ” ëª¨ë¸ì—ê²Œ ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ì•Œë ¤ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ìš©ìê°€ ëª¨ë¸ì—ê²Œ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜, íŠ¹ì • ìŠ¤íƒ€ì¼ì˜ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ë„ë¡ ìš”ì²­í•  ë•Œ í”„ë¡¬í”„íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "2. **ì»¨í…ìŠ¤íŠ¸ ì œê³µ**: ëª¨ë¸ì´ ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•„ìš”í•œ ë°°ê²½ ì •ë³´ë‚˜ ë¬¸ë§¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–´ë–¤ ì£¼ì œì— ëŒ€í•œ ì§ˆë¬¸ì„ í•  ë•Œ, ê´€ë ¨ ë°°ê²½ ì •ë³´ë¥¼ ì œê³µí•˜ì—¬ ëª¨ë¸ì´ ë” ì •í™•í•œ ë‹µì„ í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "3. **ëª¨ë¸ì˜ ì¶œë ¥ ìœ ë„**: í”„ë¡¬í”„íŠ¸ê°€ ëª¨ë¸ì˜ ì¶œë ¥ì„ ìœ ë„í•˜ê³ , ìƒì„±ë˜ëŠ” í…ìŠ¤íŠ¸ì˜ ìŠ¤íƒ€ì¼, ë‚´ìš©, í˜•ì‹ ë“±ì„ ê²°ì •í•˜ëŠ” ë° ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
    "\n",
    "### ì˜ˆì‹œ:\n",
    "1. **ì§ˆë¬¸ ì‘ë‹µ**:\n",
    "   - **í”„ë¡¬í”„íŠ¸**: \"What is the capital of France?\"\n",
    "   - **ì¶œë ¥**: \"The capital of France is Paris.\"\n",
    "\n",
    "2. **ì°½ì˜ì  ê¸€ì“°ê¸°**:\n",
    "   - **í”„ë¡¬í”„íŠ¸**: \"Write a short story about a dragon and a knight.\"\n",
    "   - **ì¶œë ¥**: ëª¨ë¸ì´ ì°½ì˜ì ìœ¼ë¡œ ë“œë˜ê³¤ê³¼ ê¸°ì‚¬ì— ê´€í•œ ì´ì•¼ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "3. **ë²ˆì—­**:\n",
    "   - **í”„ë¡¬í”„íŠ¸**: \"Translate the following sentence to Spanish: 'Hello, how are you?'\"\n",
    "   - **ì¶œë ¥**: \"Hola, Â¿cÃ³mo estÃ¡s?\"\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ì˜ ì¢…ë¥˜:\n",
    "- **ë‹¨ìˆœí•œ ì§ˆë¬¸**: ì‚¬ìš©ìê°€ ë‹¨ìˆœíˆ ê¶ê¸ˆí•œ ì ì„ ë¬»ëŠ” í˜•íƒœ.\n",
    "- **ì§€ì‹œë¬¸**: íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ ì§€ì‹œí•˜ëŠ” í˜•íƒœ.\n",
    "- **í˜•ì‹í™”ëœ ì…ë ¥**: íŠ¹ì • í˜•ì‹ì´ë‚˜ êµ¬ì¡°ë¥¼ ê°–ì¶˜ ì…ë ¥(ì˜ˆ: í…ìŠ¤íŠ¸ ìš”ì•½, ë²ˆì—­, ì½”ë“œ ì‘ì„± ë“±).\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ ì„¤ê³„ì˜ ì¤‘ìš”ì„±:\n",
    "- **ì •í™•í•œ ê²°ê³¼**ë¥¼ ì–»ê¸° ìœ„í•´ì„œëŠ” **í”„ë¡¬í”„íŠ¸ì˜ ì„¤ê³„**ê°€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ê°€ ëª¨í˜¸í•˜ê±°ë‚˜ ë¶ˆì™„ì „í•˜ë©´ ëª¨ë¸ì´ ì›í•˜ëŠ” ì¶œë ¥ì„ ìƒì„±í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.\n",
    "- ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‹¤í—˜í•˜ë©´ì„œ ëª¨ë¸ì˜ ë°˜ì‘ì„ ê´€ì°°í•˜ê³ , ê°€ì¥ ì í•©í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ ì„¤ê³„ íŒ:\n",
    "1. **ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì§€ì‹œ**: ë¬´ì—‡ì„ ì›í•˜ëŠ”ì§€ ì •í™•í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, \"Explain quantum mechanics\"ë³´ë‹¤ëŠ” \"Explain quantum mechanics in simple terms for a high school student\"ì™€ ê°™ì´ êµ¬ì²´ì ì¸ ìš”êµ¬ë¥¼ í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "   \n",
    "2. **ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ**: í•„ìš”í•œ ë°°ê²½ ì •ë³´ë‚˜ ë¬¸ë§¥ì„ ì œê³µí•˜ë©´ ëª¨ë¸ì´ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "3. **ë‹¤ì–‘í•œ ì‹¤í—˜**: í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ê¸ˆì”© ë°”ê¿”ê°€ë©° í…ŒìŠ¤íŠ¸í•´ ë³´ë©´ì„œ ìµœì ì˜ ì‘ë‹µì„ ìœ ë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ê²°ë¡ :\n",
    "í”„ë¡¬í”„íŠ¸ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ê²Œ ì‘ì—…ì„ ì§€ì‹œí•˜ëŠ” ì¤‘ìš”í•œ ì…ë ¥ìœ¼ë¡œ, ëª¨ë¸ì´ ìˆ˜í–‰í•  ì‘ì—…ì˜ ë°©í–¥ì„ ê²°ì •ì§“ëŠ” ìš”ì†Œì…ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ ì„¤ê³„í•˜ëŠ” ê²ƒì´ LLMì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë° í° ë„ì›€ì´ ë©ë‹ˆë‹¤.## Prompt\n",
    "â†‘ messages ë¥¼ prompt ë¼ê³ ë„ í•¨ (?)\n",
    "- ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì œê³µë˜ëŠ” í…ìŠ¤íŠ¸ë‚˜ ë°ì´í„°\n",
    "- ëª¨ë¸ì—ê²Œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ ì§€ì‹œí•˜ê±°ë‚˜, ëª¨ë¸ì´ ìƒì„±í•  í…ìŠ¤íŠ¸ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µ\n",
    "- LLM ê³¼ ì˜ì‚¬ì†Œí†µí•˜ê¸° ìœ„í•œ ë°©ë²•\n",
    "\n",
    "---\n",
    "\n",
    "LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì—ì„œ **í”„ë¡¬í”„íŠ¸ prompt**ë€ ëª¨ë¸ì— **ì…ë ¥**ìœ¼ë¡œ ì œê³µë˜ëŠ” í…ìŠ¤íŠ¸ë‚˜ ë°ì´í„°ì…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì—ê²Œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ ì§€ì‹œí•˜ê±°ë‚˜, ëª¨ë¸ì´ ìƒì„±í•  í…ìŠ¤íŠ¸ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ëŠ” ëª¨ë¸ì˜ ì¶œë ¥ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ì˜ ì—­í• :\n",
    "1. **ëª¨ë¸ì— ëŒ€í•œ ì§€ì‹œ**: í”„ë¡¬í”„íŠ¸ëŠ” ëª¨ë¸ì—ê²Œ ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ì•Œë ¤ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ìš©ìê°€ ëª¨ë¸ì—ê²Œ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜, íŠ¹ì • ìŠ¤íƒ€ì¼ì˜ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ë„ë¡ ìš”ì²­í•  ë•Œ í”„ë¡¬í”„íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "2. **ì»¨í…ìŠ¤íŠ¸ ì œê³µ**: ëª¨ë¸ì´ ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•„ìš”í•œ ë°°ê²½ ì •ë³´ë‚˜ ë¬¸ë§¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–´ë–¤ ì£¼ì œì— ëŒ€í•œ ì§ˆë¬¸ì„ í•  ë•Œ, ê´€ë ¨ ë°°ê²½ ì •ë³´ë¥¼ ì œê³µí•˜ì—¬ ëª¨ë¸ì´ ë” ì •í™•í•œ ë‹µì„ í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "3. **ëª¨ë¸ì˜ ì¶œë ¥ ìœ ë„**: í”„ë¡¬í”„íŠ¸ê°€ ëª¨ë¸ì˜ ì¶œë ¥ì„ ìœ ë„í•˜ê³ , ìƒì„±ë˜ëŠ” í…ìŠ¤íŠ¸ì˜ ìŠ¤íƒ€ì¼, ë‚´ìš©, í˜•ì‹ ë“±ì„ ê²°ì •í•˜ëŠ” ë° ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
    "\n",
    "### ì˜ˆì‹œ:\n",
    "1. **ì§ˆë¬¸ ì‘ë‹µ**:\n",
    "   - **í”„ë¡¬í”„íŠ¸**: \"What is the capital of France?\"\n",
    "   - **ì¶œë ¥**: \"The capital of France is Paris.\"\n",
    "\n",
    "2. **ì°½ì˜ì  ê¸€ì“°ê¸°**:\n",
    "   - **í”„ë¡¬í”„íŠ¸**: \"Write a short story about a dragon and a knight.\"\n",
    "   - **ì¶œë ¥**: ëª¨ë¸ì´ ì°½ì˜ì ìœ¼ë¡œ ë“œë˜ê³¤ê³¼ ê¸°ì‚¬ì— ê´€í•œ ì´ì•¼ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "3. **ë²ˆì—­**:\n",
    "   - **í”„ë¡¬í”„íŠ¸**: \"Translate the following sentence to Spanish: 'Hello, how are you?'\"\n",
    "   - **ì¶œë ¥**: \"Hola, Â¿cÃ³mo estÃ¡s?\"\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ì˜ ì¢…ë¥˜:\n",
    "- **ë‹¨ìˆœí•œ ì§ˆë¬¸**: ì‚¬ìš©ìê°€ ë‹¨ìˆœíˆ ê¶ê¸ˆí•œ ì ì„ ë¬»ëŠ” í˜•íƒœ.\n",
    "- **ì§€ì‹œë¬¸**: íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ ì§€ì‹œí•˜ëŠ” í˜•íƒœ.\n",
    "- **í˜•ì‹í™”ëœ ì…ë ¥**: íŠ¹ì • í˜•ì‹ì´ë‚˜ êµ¬ì¡°ë¥¼ ê°–ì¶˜ ì…ë ¥(ì˜ˆ: í…ìŠ¤íŠ¸ ìš”ì•½, ë²ˆì—­, ì½”ë“œ ì‘ì„± ë“±).\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ ì„¤ê³„ì˜ ì¤‘ìš”ì„±:\n",
    "- **ì •í™•í•œ ê²°ê³¼**ë¥¼ ì–»ê¸° ìœ„í•´ì„œëŠ” **í”„ë¡¬í”„íŠ¸ì˜ ì„¤ê³„**ê°€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ê°€ ëª¨í˜¸í•˜ê±°ë‚˜ ë¶ˆì™„ì „í•˜ë©´ ëª¨ë¸ì´ ì›í•˜ëŠ” ì¶œë ¥ì„ ìƒì„±í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.\n",
    "- ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‹¤í—˜í•˜ë©´ì„œ ëª¨ë¸ì˜ ë°˜ì‘ì„ ê´€ì°°í•˜ê³ , ê°€ì¥ ì í•©í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ ì„¤ê³„ íŒ:\n",
    "1. **ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì§€ì‹œ**: ë¬´ì—‡ì„ ì›í•˜ëŠ”ì§€ ì •í™•í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, \"Explain quantum mechanics\"ë³´ë‹¤ëŠ” \"Explain quantum mechanics in simple terms for a high school student\"ì™€ ê°™ì´ êµ¬ì²´ì ì¸ ìš”êµ¬ë¥¼ í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "   \n",
    "2. **ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ**: í•„ìš”í•œ ë°°ê²½ ì •ë³´ë‚˜ ë¬¸ë§¥ì„ ì œê³µí•˜ë©´ ëª¨ë¸ì´ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "3. **ë‹¤ì–‘í•œ ì‹¤í—˜**: í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ê¸ˆì”© ë°”ê¿”ê°€ë©° í…ŒìŠ¤íŠ¸í•´ ë³´ë©´ì„œ ìµœì ì˜ ì‘ë‹µì„ ìœ ë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ê²°ë¡ :\n",
    "í”„ë¡¬í”„íŠ¸ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ê²Œ ì‘ì—…ì„ ì§€ì‹œí•˜ëŠ” ì¤‘ìš”í•œ ì…ë ¥ìœ¼ë¡œ, ëª¨ë¸ì´ ìˆ˜í–‰í•  ì‘ì—…ì˜ ë°©í–¥ì„ ê²°ì •ì§“ëŠ” ìš”ì†Œì…ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ ì„¤ê³„í•˜ëŠ” ê²ƒì´ LLMì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë° í° ë„ì›€ì´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3b928-05b2-4f05-9334-f19f51e0fe35",
   "metadata": {},
   "source": [
    "Prompt ì„±ëŠ¥ì´ ì¢‹ë‹¤ë©´ LLM ë‹µë³€ì˜ ì„±ëŠ¥ë„ ì¢‹ì„ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë“  ì›¹ ì‚¬ì´íŠ¸ë“¤ì€ ìƒí™©ì— ë§ëŠ” ë›°ì–´ëŠ” ì„±ëŠ¥ì˜ prompt ë¥¼ ì œì‘í•˜ëŠ”ë° ì „ë…í•¨.\n",
    "\n",
    "LangChain ì€ prompt ë¥¼ ê³µìœ í•˜ê¸° ìœ„í•œ ì»¤ë®¤ë‹ˆí‹°ë„ í˜•ì„±ë˜ê³  ìˆë‹¤.\n",
    "ì‚°ì—… ì „ì²´ ì „ë°˜ì ìœ¼ë¡œ ê° ë¶„ì•¼ë³„ prompt ë¥¼ ë§Œë“¤ì–´ ë‚´ê³  ìˆë‹¤.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ë©´\n",
    "| í”Œë«í¼               | ê¸°ëŠ¥              | URL                                                             |\n",
    "| ----------------- | --------------- | --------------------------------------------------------------- |\n",
    "| **LangChain Hub** | í”„ë¡¬í”„íŠ¸ ë° ì²´ì¸ ê³µìœ     | [smith.langchain.com/hub](https://smith.langchain.com/hub)      |\n",
    "| **Discord**       | ì»¤ë®¤ë‹ˆí‹°, í”„ë¡¬í”„íŠ¸ ë…¼ì˜   | [discord.gg/langchain](https://discord.gg/langchain)            |\n",
    "| **GitHub**        | ì½”ë“œ ì˜ˆì œ, í”„ë¡¬í”„íŠ¸ í™œìš©ë²• | [LangChain Examples](https://github.com/langchain-ai/langchain) |\n",
    "\n",
    "\n",
    "ê·¸ë˜ì„œ, LangChain í”„ë ˆì„ì›Œí¬ì˜ ë§ì€ ë¶€ë¶„ì´ prompt ì— ì§‘ì¤‘ë˜ì–´ ìˆë‹¤.\n",
    "\n",
    "prompt ë¼ë¦¬ ê²°í•¨ë„ í• ìˆ˜ ìˆê³ , ì €ì¥í•˜ê±°ë‚˜ ë¶ˆëŸ¬ì˜¬ìˆ˜ë„ ìˆë‹¤.\n",
    "\n",
    "ë³€ìˆ˜ ì„¤ì • ë„ì¤‘ì— ê²€ì¦ë„ í• ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee61e4-c1fa-4cd9-80f2-b4da917e30ba",
   "metadata": {},
   "source": [
    "## PromptTemplate\n",
    "ë©”ì„¸ì§€ ì»¤ìŠ¤í„°ë§ˆì´ì§•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "944e6615-8527-47fa-bebf-c86acce9c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html\n",
    "\n",
    "from langchain_core.prompts.chat import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatMessagePromptTemplate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05bb28c2-140f-4088-977e-85536b969802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country_a', 'country_b'], input_types={}, partial_variables={}, template='What is the distance between {country_a} and {country_b}')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = PromptTemplate.from_template(\n",
    "     # placeholder {...} ì‚¬ìš©\n",
    "    \"What is the distance between {country_a} and {country_b}\"\n",
    ")\n",
    "\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c7ff551-685a-4f85-8eaf-066daa82bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template.format() # ì—ëŸ¬ KeyError: 'country_a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07e59b79-d26e-4821-a9a0-9ca9eb942a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the distance between Mexico and Brazil'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = template.format(country_a = 'Mexico', country_b = 'Brazil')\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05bf36f8-4687-4790-812a-74fb960236b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The distance between Mexico and Brazil is approximately 6,000 kilometers (3,728 miles) when measured in a straight line. However, the actual distance may vary depending on the specific locations in each country.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 15, 'total_tokens': 57, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ3y1TuD3vf2wFzgx13ilf9sxus', 'finish_reason': 'stop', 'logprobs': None}, id='run--4369e298-be00-4e39-9c72-d92f023cf107-0', usage_metadata={'input_tokens': 15, 'output_tokens': 42, 'total_tokens': 57, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777d5a1-4a29-4981-ab2a-03f1ee2354c1",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8f1bce1-dd5c-4033-af42-30f5bffdc2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['country_a', 'country_b', 'language', 'name'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a geography expert. And your only reply in {language}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='ì•ˆë…•, ë‚´ ì´ë¦„ì€ {name} ì•¼'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country_a', 'country_b'], input_types={}, partial_variables={}, template='\\n        What is the distance between {country_a} and {country_b}.\\n        Also, what is your name?\\n        '), additional_kwargs={})])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    # SystemMessage íŠœí”Œ\n",
    "    (\"system\", \"You are a geography expert. And your only reply in {language}\"),\n",
    "    # AIMessage íŠœí”Œ\n",
    "    (\"ai\", \"ì•ˆë…•, ë‚´ ì´ë¦„ì€ {name} ì•¼\"),\n",
    "    # HumanMessage íŠœí”Œ\n",
    "    (\"human\", \"\"\"\n",
    "        What is the distance between {country_a} and {country_b}.\n",
    "        Also, what is your name?\n",
    "        \"\"\"),    \n",
    "])\n",
    "\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e095f217-d479-44f5-b4e3-419b731e3470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a geography expert. And your only reply in Korean', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ì•ˆë…•, ë‚´ ì´ë¦„ì€ ë½€ë¡œë¡œ ì•¼', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='\\n        What is the distance between Canada and Japan.\\n        Also, what is your name?\\n        ', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = template.format_messages(\n",
    "    language=\"Korean\",\n",
    "    name=\"ë½€ë¡œë¡œ\",\n",
    "    country_a=\"Canada\",\n",
    "    country_b=\"Japan\",\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1e111b5-b02d-448b-87a8-103c34ebb4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ìºë‚˜ë‹¤ì™€ ì¼ë³¸ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ì•½ 7,000kmì…ë‹ˆë‹¤. ì œ ì´ë¦„ì€ ë½€ë¡œë¡œì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 62, 'total_tokens': 97, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ417m3mD6cC5R7cyUBmu7AFz47', 'finish_reason': 'stop', 'logprobs': None}, id='run--359698c5-013a-4469-a682-6af18ea14a8e-0', usage_metadata={'input_tokens': 62, 'output_tokens': 35, 'total_tokens': 97, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714540e8-89ee-4c3d-b540-2636dac098e7",
   "metadata": {},
   "source": [
    "# OutputParser and LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c82a91-f930-42dd-a4c8-1d8f9aa2c918",
   "metadata": {},
   "source": [
    "## Output Parser ë€\n",
    "\n",
    "LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì—ì„œ ìƒì„±ëœ ì¶œë ¥ì„ ì²˜ë¦¬í•˜ê³  'ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜'í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ìœ í‹¸ë¦¬í‹°ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ìƒì„±í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ 'êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜'í•˜ê±°ë‚˜, 'íŠ¹ì • ê·œì¹™ì— ë”°ë¼ ë°ì´í„°ë¥¼ ì¶”ì¶œ'í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "\n",
    "1. ì¶œë ¥ êµ¬ì¡°í™”\n",
    "    - ëª¨ë¸ì˜ í…ìŠ¤íŠ¸ ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ JSON, ë”•ì…”ë„ˆë¦¬, ëª©ë¡ ë“±ê³¼ ê°™ì€ í”„ë¡œê·¸ë˜ë°ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
    "    \n",
    "1. ì¶œë ¥ ê²€ì¦\n",
    "    - ëª¨ë¸ì´ ì˜ˆìƒì¹˜ ëª»í•œ ì¶œë ¥ì„ ë°˜í™˜í•  ê²½ìš° ì ì ˆí•œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì œê³µí•˜ê±°ë‚˜ ê¸°ë³¸ê°’ì„ ë°˜í™˜í•˜ë„ë¡ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    \n",
    "1. ì¶œë ¥ í‘œì¤€í™”\n",
    "    - ì–¸ì–´ ëª¨ë¸ì˜ ì¶œë ¥ì´ í•­ìƒ ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ì œê³µë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da79679-449d-49c6-8c5e-0e0426ff7ba1",
   "metadata": {},
   "source": [
    "## BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25978921-f55d-4fb4-abf1-a89e6435edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì œ: LLM ì˜ ì¶œë ¥ì„ -> list ë¡œ ë³€í™˜ì‹œì¼œ ë³´ì.  (ë³€í™˜ì‹œí‚¤ëŠ” OutputParser ë¥¼ ë§Œë“¤ì–´ ë³´ì)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a32250da-d4c3-4658-95cf-2d13dbb1260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.output_parsers.base import BaseOutputParser\n",
    "# https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html\n",
    "\n",
    "# â†“ ì´ë¥¼ ìƒì† ë°›ì•„ OutputParser ë¥¼ ë§Œë“ ë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a00c83f-918c-48a3-99b7-a45b0bda2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommaOutputParser(BaseOutputParser):\n",
    "\n",
    "    # parse() ë©”ì†Œë“œë¥¼ ë°˜.ë“œ.ì‹œ êµ¬í˜„.\n",
    "    #   text = ì…ë ¥í…ìŠ¤íŠ¸\n",
    "    def parse(self, text): \n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21b22dd8-4c0d-4bc8-89ad-9d4e72a47acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = CommaOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74cf95e9-153f-4a0a-8d4d-4ba1f4c7ee61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë™ì‘ í™•ì¸\n",
    "p.parse(\"   Hello,    how,   are,   you   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ca578a1-3843-4776-842c-8999f74ff63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='1. Mercury\\n2. Venus\\n3. Earth\\n4. Mars\\n5. Jupiter\\n6. Saturn\\n7. Uranus\\n8. Neptune\\n9. Pluto', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 47, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ5dciXT79Cr7nUoVWRq1QyhjVq', 'finish_reason': 'stop', 'logprobs': None}, id='run--f42ddc99-6493-4da6-9610-6bbecbc0511d-0', usage_metadata={'input_tokens': 47, 'output_tokens': 36, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "******************************\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune\n",
      "9. Pluto\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the planets?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.__repr__())\n",
    "print('*' * 30)\n",
    "print(result.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fbda688-4ae7-49a4-b9f3-32321185e27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 49, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ5la81j0Mq77up8Cs2bTByQ6ZI', 'finish_reason': 'stop', 'logprobs': None}, id='run--e49897c5-f5d3-48cc-a702-af7c30608117-0', usage_metadata={'input_tokens': 49, 'output_tokens': 17, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "******************************\n",
      "Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a comma separated list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the planets?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.__repr__())\n",
    "print('*' * 30)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ac815cb-21a2-43bf-a453-192801f7cb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='red, blue, green, yellow, orange, purple, pink, black, white, brown', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 49, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ7jy3Ph2e8gxndDseFLzMS3Yp3', 'finish_reason': 'stop', 'logprobs': None}, id='run--56ebfa92-b848-408e-b492-39dd1c7efc1e-0', usage_metadata={'input_tokens': 49, 'output_tokens': 19, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "******************************\n",
      "red, blue, green, yellow, orange, purple, pink, black, white, brown\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a comma separated list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the colors?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.__repr__())\n",
    "print('*' * 30)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7575fd7b-57df-4d04-ad37-f20b5367f3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='red, blue, green, yellow, orange, purple, pink, black, white, brown', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 53, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZQ7sNgVF0nF8mDK88c0R2O0vDRO', 'finish_reason': 'stop', 'logprobs': None}, id='run--6dc09180-1f61-4a15-8f82-e52febc9316c-0', usage_metadata={'input_tokens': 53, 'output_tokens': 19, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "******************************\n",
      "red, blue, green, yellow, orange, purple, pink, black, white, brown\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered \n",
    "        with a comma separated list of max {max_items} in lowercase.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the colors?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.__repr__())\n",
    "print('*' * 30)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa00ea5f-9722-4c48-a3b3-f91cd89750d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'orange',\n",
       " 'purple',\n",
       " 'pink',\n",
       " 'black',\n",
       " 'white',\n",
       " 'brown']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered \n",
    "        with a comma separated list of max {max_items} in lowercase.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the colors?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c356e142-1d4b-4de2-8438-eb86103c39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "template -> format -> invoke -> parser ...\n",
    "\n",
    "ë„ˆë¬´ ë‹¨ê³„ê°€ ë§ì•„ ë³´ì¸ë‹¤..   ì´ë¥¼ ì£„ë‹¤ ì§ì ‘ í•˜ë“œ ì½”ë”©í•˜ë‹¤ë‹ˆ?\n",
    "\n",
    "â†“ LCEL ì„ ì‚¬ìš©í•˜ë©´ ìœ„ ê³¼ì •ë“¤ì´ ë§~ì´ ìƒëµëœë‹¤!\n",
    "   => ë°”ë¡œ chain ì˜ ë“±ì¥!\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34575f-623b-4375-b49d-aec0a90fa2d7",
   "metadata": {},
   "source": [
    "## Chain, LCEL\n",
    "\n",
    "- LCEL (LangChain Expression Language: ë­ì²´ì¸ í‘œí˜„ ì–¸ì–´)\n",
    "  - LCELì€ LangChain ë‚´ì—ì„œ ë³µì¡í•œ í‘œí˜„ì‹ì„ ì²˜ë¦¬í•˜ê³ ,\n",
    "  - ëª¨ë¸ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ ë” ê°•ë ¥í•˜ê³  ìœ ì—°í•˜ê²Œ ë§Œë“œëŠ” ê¸°ëŠ¥ì„ ì œê³µ\n",
    "    - ì½”ë“œì–‘ì„ ë§ì´ ì¤„ì—¬ì¤Œ.\n",
    "    - ë‹¤ì–‘í•œ template ê³¼ LLM í˜¸ì¶œ\n",
    "    - ì„œë¡œ ë‹¤ë¥¸ ì‘ë‹µ(response) ë¥¼ í•¨ê»˜ ì‚¬ìš©ì¼€ í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6289d66-720a-4abd-be51-bbc2c01acde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['max_items', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['max_items'], input_types={}, partial_variables={}, template='You are a list generating machine.\\n        Everything you are asked will be answered \\n        with a comma separated list of max {max_items} in lowercase.\\n        Do NOT reply with anything else.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000226DABD66F0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000226DABD4500>, root_client=<openai.OpenAI object at 0x00000226DA597BC0>, root_async_client=<openai.AsyncOpenAI object at 0x00000226DABD6720>, temperature=0.1, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| CommaOutputParser()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain ìƒì„±!\n",
    "#  '|' ì—°ì‚°ì ì‚¬ìš©\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "print(type(chain))\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46b9ccb6-1f90-4b04-8266-4a43eaa20d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pikachu', 'charmander', 'bulbasaur', 'squirtle', 'jigglypuff']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain í˜¸ì¶œ!  invoke({...})\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\": \"What are the pokemons?\",  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68c1a689-4157-4a6c-892d-d9b841aac879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "â†‘ Chain ì„ ì‚¬ìš©í•´ ê½¤ë‚˜ ê°„ê²°í•œ ì½”ë“œë¡œ ì‘ë™ì‹œì¼°ëŠ”ì§€ ë³´ë¼!\n",
    "\n",
    "ì‚¬ì‹¤ ë­ì²´ì¸ì€ ë‚´ë¶€ì—ì„œ\n",
    "  .format_message() í˜¸ì¶œ\n",
    "  chat.invoke() í˜¸ì¶œ\n",
    "  parse() í˜¸ì¶œí•œë‹¤\n",
    "\n",
    "ì´ëŸ¬í•œ ì¼ë ¨ì˜ ì‘ì—…ì„ chain.invoke() í˜¸ì¶œ ë‹¨í•œë²ˆìœ¼ë¡œ ëë‚¸ë‹¤.\n",
    "\n",
    "ì´ëŸ¬í•œ chain êµ¬ë¬¸ìœ¼ë¡œ ì •ë§ ë‹¤ì–‘í•œ ì‘ì—…ì˜ íë¦„ë“¤ì„ ìˆ˜í–‰í• ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1b6610e-8669-404b-86dd-acfa5d2bee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "chain ë¼ë¦¬ë„ ê²°í•©í• ìˆ˜ ë„ ìˆë‹¤.\n",
    "\n",
    "[ì˜ˆì‹œ]\n",
    "chain_one = template | chat | CommaOutputParser()\n",
    "chain_two = template_2 | chat | OutputParser2()\n",
    "\n",
    "all = chain_one | chain_two | OutputParser3()\n",
    "  â†‘ chain_one ì˜ ì¶œë ¥ì„ chain_two ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800327f4-5036-4497-acdc-975097b62a82",
   "metadata": {},
   "source": [
    "# Chaining Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a35ce4e-c94a-4636-80fa-3fb515df1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ê³µì‹]\n",
    "#  https://python.langchain.com/docs/concepts/lcel/\n",
    "#  https://python.langchain.com/docs/how_to/#langchain-expression-language-lcel\n",
    "#  https://python.langchain.com/docs/how_to/lcel_cheatsheet/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afecef-ccaa-4808-9b3e-ce9b29171afc",
   "metadata": {},
   "source": [
    "## LCEL ì˜ input / output\n",
    "\n",
    "LangChain Expression Language (LCEL)ì€ LangChainì—ì„œ ë‹¤ì–‘í•œ ì…ë ¥ ìœ í˜•ì„ í™œìš©í•˜ì—¬ LLMê³¼ ë„êµ¬ë¥¼ ê²°í•©í•˜ê³  ë°ì´í„° íë¦„ì„ ì œì–´í•˜ëŠ” ì–¸ì–´ì…ë‹ˆë‹¤. LCELì€ LLMì˜ ì…ë ¥ê³¼ ì²˜ë¦¬ì— ì‚¬ìš©ë˜ëŠ” **ì…ë ¥ íƒ€ì…**(Input Types)ì„ ëª…í™•í•˜ê²Œ ì •ì˜í•˜ì—¬, ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ì™€ ë„êµ¬ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ë”ìš± íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ëŠ” LCELì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” **ì…ë ¥ íƒ€ì…**ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Plain Text**\n",
    "- **ì„¤ëª…**: ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ì…ë ¥ì…ë‹ˆë‹¤. ì´ í˜•ì‹ì€ ê°€ì¥ ê¸°ë³¸ì ì¸ ì…ë ¥ìœ¼ë¡œ, LLMì´ ììœ ë¡œìš´ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```plaintext\n",
    "  What is the capital of France?\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - í…ìŠ¤íŠ¸ ë¶„ì„, ìƒì„± ë° ëŒ€í™”í˜• ì‘ì—…ì— ì í•©.\n",
    "  - ì¶”ê°€ì ì¸ êµ¬ì¡°ë‚˜ ë©”íƒ€ë°ì´í„° ì—†ì´ ë‹¨ìˆœ í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Structured Input**\n",
    "- **ì„¤ëª…**: JSON, ë”•ì…”ë„ˆë¦¬, ë˜ëŠ” êµ¬ì¡°í™”ëœ í˜•ì‹ì˜ ì…ë ¥ì…ë‹ˆë‹¤. ë°ì´í„° í•„ë“œê°€ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜ë˜ì–´ ìˆìœ¼ë©°, ëª¨ë¸ì´ ì´ êµ¬ì¡°ì— ë”°ë¼ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```json\n",
    "  {\n",
    "      \"question\": \"What is the capital of France?\",\n",
    "      \"context\": \"France is a country in Europe.\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - ëª…ì‹œì ì¸ ë°ì´í„° í•„ë“œë¥¼ í†µí•´ LLMì´ í•„ìš”í•œ ì •ë³´ë¥¼ ë” ì •í™•íˆ ì¶”ì¶œ ë° í™œìš© ê°€ëŠ¥.\n",
    "  - ë³µì¡í•œ ë°ì´í„° ë¶„ì„ì´ë‚˜ ë©€í‹° í•„ë“œ ì²˜ë¦¬ê°€ í•„ìš”í•œ ì‘ì—…ì— ìœ ìš©.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Prompt Templates**\n",
    "- **ì„¤ëª…**: ì‚¬ìš©ìê°€ ì •ì˜í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. í…œí”Œë¦¿ì— ë³€ìˆ˜ ê°’ì„ ì±„ì›Œ ë„£ì–´ ëª¨ë¸ì— ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```python\n",
    "  template = \"Translate the following text to French: {text}\"\n",
    "  input = template.format(text=\"Hello, how are you?\")\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - ë³€ìˆ˜ ê¸°ë°˜ ì…ë ¥ì„ í†µí•´ ì¬ì‚¬ìš© ê°€ëŠ¥ì„±ì´ ë†’ìŒ.\n",
    "  - ì‚¬ìš©ì ì •ì˜ ì…ë ¥ ìƒì„± ë° ì œì–´ì— ì í•©.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Key-Value Pairs**\n",
    "- **ì„¤ëª…**: í‚¤-ê°’ ìŒì˜ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ, ëª…ì‹œì ì¸ ì¿¼ë¦¬ í˜•íƒœë¡œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```python\n",
    "  {\n",
    "      \"name\": \"John\",\n",
    "      \"age\": 30,\n",
    "      \"location\": \"New York\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - ì •í˜•í™”ëœ ë°ì´í„°ë¥¼ ì œê³µí•˜ì—¬ LLMì´ ë” íš¨ìœ¨ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„ì„ ë° ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ.\n",
    "  - íŠ¹ì • ì •ë³´ í•„ë“œê°€ ëª…í™•íˆ í•„ìš”í•  ë•Œ ìœ ìš©.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Multi-modal Inputs**\n",
    "- **ì„¤ëª…**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜•ì„ ì¡°í•©í•œ ì…ë ¥ í˜•ì‹ì…ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```python\n",
    "  {\n",
    "      \"text\": \"Describe the image.\",\n",
    "      \"image\": \"<image_data>\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ê³¼ í†µí•©í•˜ì—¬ ë‹¤ì–‘í•œ ì…ë ¥ í˜•ì‹ì„ ì²˜ë¦¬ ê°€ëŠ¥.\n",
    "  - ì´ë¯¸ì§€ ìº¡ì…”ë‹, ì˜¤ë””ì˜¤-í…ìŠ¤íŠ¸ ë³€í™˜ ë“±ì˜ ì‘ì—…ì—ì„œ í™œìš©.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Serialized Inputs**\n",
    "- **ì„¤ëª…**: ì…ë ¥ ë°ì´í„°ë¥¼ ì‹œë¦¬ì–¼í™”(Serialize)í•˜ì—¬ íŠ¹ì • í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•œ ì…ë ¥ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, JSON ë¬¸ìì—´ë¡œ ë°ì´í„°ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```python\n",
    "  input = '{\"question\": \"What is the capital of France?\", \"context\": \"France is in Europe.\"}'\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - ë°ì´í„°ê°€ ì™¸ë¶€ ì‹œìŠ¤í…œì´ë‚˜ APIì™€ í†µì‹ í•  ë•Œ ìœ ìš©.\n",
    "  - ë°ì´í„° í¬ë§·ì— ëŒ€í•œ ìœ ì—°ì„±ì´ ë†’ìŒ.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Chat Messages**\n",
    "- **ì„¤ëª…**: ì±„íŒ… ë©”ì‹œì§€ í˜•ì‹ì˜ ì…ë ¥ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ì—­í• (role)ê³¼ ë‚´ìš©(content)ì„ ì •ì˜í•˜ì—¬ LLMì—ê²Œ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```python\n",
    "  [\n",
    "      {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": \"What is the weather today?\"}\n",
    "  ]\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - ChatGPT ê°™ì€ ëŒ€í™”í˜• ëª¨ë¸ì— ì í•©.\n",
    "  - ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ê³  ë‹¤ì¤‘ ë°œí™” ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Custom Input Types**\n",
    "- **ì„¤ëª…**: ì‚¬ìš©ìê°€ ì• í”Œë¦¬ì¼€ì´ì…˜ ìš”êµ¬ ì‚¬í•­ì— ë”°ë¼ ì •ì˜í•˜ëŠ” ì»¤ìŠ¤í…€ ì…ë ¥ í˜•ì‹ì…ë‹ˆë‹¤.\n",
    "- **ì˜ˆì‹œ**:\n",
    "  ```python\n",
    "  class CustomInput:\n",
    "      def __init__(self, field1, field2):\n",
    "          self.field1 = field1\n",
    "          self.field2 = field2\n",
    "  ```\n",
    "\n",
    "- **íŠ¹ì§•**:\n",
    "  - íŠ¹ì • ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œì§ê³¼ ì™„ë²½íˆ ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬.\n",
    "  - í‘œì¤€ ì…ë ¥ íƒ€ì…ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ì–´ë ¤ìš´ ë³µì¡í•œ êµ¬ì¡°ë¥¼ ë‹¤ë£° ë•Œ ìœ ìš©.\n",
    "\n",
    "---\n",
    "\n",
    "### ìš”ì•½\n",
    "LCELì˜ ì…ë ¥ íƒ€ì…ì€ ë‹¨ìˆœ í…ìŠ¤íŠ¸ë¶€í„° êµ¬ì¡°í™”ëœ ë°ì´í„°, ë©€í‹°ëª¨ë‹¬ ì…ë ¥ê¹Œì§€ ë‹¤ì–‘í•˜ê²Œ ì œê³µë˜ë©°, ê° íƒ€ì…ì€ íŠ¹ì • ìš©ë„ì— ë§ê²Œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì…ë ¥ ë°ì´í„°ë¥¼ ì •êµí•˜ê²Œ ì„¤ê³„í•˜ê³  ì ì ˆí•œ í˜•ì‹ì„ ì„ íƒí•¨ìœ¼ë¡œì¨ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a825bc5-17c2-465e-b9b3-80bca74f6f37",
   "metadata": {},
   "source": [
    "## Chain ì—°ê²°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9d28149-05a5-4ec0-ace3-20fcad2fc784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ë‚˜ì˜ chain ì¶œë ¥ê°’ì„ ë‹¤ë¥¸ chain ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì „ë‹¬ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b2536de-7dc7-42c6-8ff3-1f884f92e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²«ë²ˆì§¸ chain\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "      You are a world-class international chef.\n",
    "      You create easy to follow recipes for any type of cuisines\n",
    "      with easy to find ingredients.        \n",
    "    \"\"\"\n",
    "    ),\n",
    "    (\"human\", \"\"\"\n",
    "        I want to cook {cuisine} food.\n",
    "    \"\"\" ),\n",
    "])\n",
    "\n",
    "chef_chain = chef_prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd845bd-cd06-4921-9b39-89db714a1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ„ Chef ì—ê²Œì„œ ë ˆì‹œí”¼ë¥¼ ë°›ê²Œ ë í…ë°\n",
    "# ë‘ë²ˆì§¸ chain ì—ì„  ì±„ì‹ ì¬ë£Œë§Œ ì‚¬ìš©í•˜ë„ë¡ ë³€í˜• í• ê²ë‹ˆë‹¤.\n",
    "\n",
    "# ì‘ì—…ì€ ë‘ê°œ\n",
    "#   1. ë ˆì‹œí”¼ë¥¼ ì „ë‹¬í•´ì£¼ëŠ” ì…°í”„\n",
    "#   2. ì±„ì‹ì£¼ì˜ìë¥¼ ìœ„í•œ ì…°í”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8624874c-4b16-4611-b115-390356b4e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ë²ˆì§¸ chain\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "        You are a vegetarian chef specialized on\n",
    "      making traditional recipies vegetarian.\n",
    "      You find alternative ingredients and explain their preparation.\n",
    "      You don't radically modify the recipe.\n",
    "      If there is no alternative for a foot just say\n",
    "      you don't know how to replace it.\n",
    "    \"\"\"),\n",
    "    (\"human\", \"\"\"\n",
    "        {recipe}\n",
    "    \"\"\"),\n",
    "])\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cce79ef5-0eea-4bd9-81f2-1a716ac1c79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative such as tofu or paneer. Here's how you can adapt the recipe:\\n\\nIngredients:\\n- 1 lb firm tofu or paneer, cut into bite-sized pieces\\n- 1 cup plain yogurt (you can use dairy-free yogurt for a vegan version)\\n- 2 tablespoons lemon juice\\n- 2 tablespoons vegetable oil\\n- 2 tablespoons garam masala\\n- 1 tablespoon ground cumin\\n- 1 tablespoon ground coriander\\n- 1 teaspoon turmeric\\n- 1 teaspoon paprika\\n- 1 teaspoon chili powder (adjust to taste)\\n- Salt and pepper to taste\\n- 1 onion, finely chopped\\n- 3 cloves garlic, minced\\n- 1-inch piece of ginger, grated\\n- 1 can (14 oz) tomato sauce\\n- 1 cup coconut cream (or dairy-free heavy cream alternative)\\n- Fresh cilantro, chopped (for garnish)\\n- Cooked rice or naan bread (to serve)\\n\\nInstructions:\\n1. Follow the same marinating process as the original recipe, but use tofu or paneer instead of chicken. Marinate the tofu or paneer in the yogurt and spice mixture for at least 1 hour.\\n2. Instead of baking, you can pan-fry the marinated tofu or paneer until golden brown and cooked through.\\n3. Proceed with the recipe as instructed, replacing the chicken with the cooked tofu or paneer when adding it to the sauce.\\n4. Simmer the tofu or paneer in the sauce for 10-15 minutes to allow the flavors to meld together.\\n5. Taste and adjust the seasoning as needed.\\n6. Serve the Vegetarian Tikka Masala over rice or with naan bread, garnished with fresh cilantro.\\n\\nEnjoy your vegetarian twist on this classic Indian dish!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 386, 'prompt_tokens': 808, 'total_tokens': 1194, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BhZYIvb04IwMKB1LBZv18lPVXih4r', 'finish_reason': 'stop', 'logprobs': None}, id='run--57a97b5d-dede-4708-a2f4-5e1d5999c0b5-0', usage_metadata={'input_tokens': 808, 'output_tokens': 386, 'total_tokens': 1194, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final chain\n",
    "\n",
    "# final_chain = chef_chain | veg_chain  \n",
    "\n",
    "final_chain = {\"recipe\" : chef_chain} | veg_chain\n",
    "\n",
    "result = final_chain.invoke({\n",
    "    \"cuisine\": \"indian\",  # chef_chain ì˜ {cuisine} ì— ì „ë‹¬\n",
    "})\n",
    "\n",
    "result  # chain ë‘ë²ˆ í˜¸ì¶œ...   ë‹µë³€ ì™„ì„±ê¹Œì§€ ì œë²• ì‹œê°„ì´ ê±¸ë¦°ë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "708b5636-7dc8-478e-94cc-0d619f122950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative such as tofu or paneer. Here's how you can adapt the recipe:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb firm tofu or paneer, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt (you can use dairy-free yogurt for a vegan version)\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 tablespoons vegetable oil\n",
      "- 2 tablespoons garam masala\n",
      "- 1 tablespoon ground cumin\n",
      "- 1 tablespoon ground coriander\n",
      "- 1 teaspoon turmeric\n",
      "- 1 teaspoon paprika\n",
      "- 1 teaspoon chili powder (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 1 onion, finely chopped\n",
      "- 3 cloves garlic, minced\n",
      "- 1-inch piece of ginger, grated\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup coconut cream (or dairy-free heavy cream alternative)\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (to serve)\n",
      "\n",
      "Instructions:\n",
      "1. Follow the same marinating process as the original recipe, but use tofu or paneer instead of chicken. Marinate the tofu or paneer in the yogurt and spice mixture for at least 1 hour.\n",
      "2. Instead of baking, you can pan-fry the marinated tofu or paneer until golden brown and cooked through.\n",
      "3. Proceed with the recipe as instructed, replacing the chicken with the cooked tofu or paneer when adding it to the sauce.\n",
      "4. Simmer the tofu or paneer in the sauce for 10-15 minutes to allow the flavors to meld together.\n",
      "5. Taste and adjust the seasoning as needed.\n",
      "6. Serve the Vegetarian Tikka Masala over rice or with naan bread, garnished with fresh cilantro.\n",
      "\n",
      "Enjoy your vegetarian twist on this classic Indian dish!\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ded65d-7a0f-4210-bca1-36b6628889b7",
   "metadata": {},
   "source": [
    "## streaming="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df719514-52b9-46c5-9a4d-232d52f27db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â†‘ ì „ë¶€ ì‹¤í–‰ ì™„ë£Œ ë ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ”ê²Œ ì§€ë£¨í•˜ë‹¤.\n",
    "#   ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ì§€ë„ ê¶ê¸ˆí•˜ë‹¤.\n",
    "#   ì§„í–‰ë˜ëŠ” ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥ í• ìˆ˜ ìˆë‹¤!\n",
    "\n",
    "# Chat model ì˜ streaming=\n",
    "#  streaming ì€ LLM model ì˜ ì‘ë‹µ(resposne) ì´ ìƒì„±ë˜ëŠ” ê²ƒì„\n",
    "#    ì‹¤ì‹œê°„ìœ¼ë¡œ(?) ë³´ê²Œ í•´ì¤Œ.\n",
    "\n",
    "# callbacks=[StreamingStdOutCallbackHandler()]\n",
    "#    ë³¼ìˆ˜ ìˆëŠ” ë¬¸ì(í† í°)ê°€ ìƒê¸¸ ë•Œë§ˆë‹¤ print í•´ì¤€ë‹¤.\n",
    "\n",
    "# callbacks ëŠ” ë‹¤ì–‘í•œ 'event' ê°ì§€ë„ ê°€ëŠ¥\n",
    "#    LLM ì´ ì‘ì—…ì„ ì‹œì‘í–ˆë‹¤ê±°ë‚˜, ëëƒˆë‹¤ê±°ë‚˜.\n",
    "#    ë¬¸ìë¥¼ ìƒì„±í–ˆë‹¤ê±°ë‚˜, ì—ëŸ¬ê°€ ë°œìƒí•˜ê±°ë‚˜.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25d3ad6a-4a3b-424f-a3c0-0f7fbad2eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3    https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e11710c0-5105-4e43-ae01-83516489863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "877423a4-032a-4e85-829d-cf5f9e26a030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great choice! Indian cuisine is full of delicious flavors and spices. Let's make a classic dish - Chicken Tikka Masala. Here's a simple recipe for you to try at home:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb boneless, skinless chicken breasts, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 tablespoons vegetable oil\n",
      "- 2 cloves garlic, minced\n",
      "- 1 tablespoon grated ginger\n",
      "- 1 tablespoon garam masala\n",
      "- 1 teaspoon ground cumin\n",
      "- 1 teaspoon ground coriander\n",
      "- 1 teaspoon paprika\n",
      "- 1 teaspoon turmeric\n",
      "- 1 teaspoon chili powder (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup heavy cream\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (for serving)\n",
      "\n",
      "Instructions:\n",
      "1. In a bowl, mix together yogurt, lemon juice, 1 tablespoon vegetable oil, garlic, ginger, garam masala, cumin, coriander, paprika, turmeric, chili powder, salt, and pepper. Add the chicken pieces and coat them well with the marinade. Cover and refrigerate for at least 1 hour, or overnight for best results.\n",
      "\n",
      "2. Preheat the oven to 400Â°F (200Â°C). Thread the marinated chicken onto skewers and place them on a baking sheet. Bake for 20-25 minutes or until the chicken is cooked through.\n",
      "\n",
      "3. In a large skillet, heat the remaining tablespoon of vegetable oil over medium heat. Add the tomato sauce and simmer for 5 minutes.\n",
      "\n",
      "4. Stir in the heavy cream and cooked chicken tikka. Simmer for another 10 minutes, stirring occasionally.\n",
      "\n",
      "5. Taste and adjust seasoning if needed. Serve the Chicken Tikka Masala over rice or with naan bread. Garnish with chopped cilantro.\n",
      "\n",
      "Enjoy your homemade Chicken Tikka Masala! Feel free to adjust the spice levels to suit your taste preferences. Let me know if you have any questions or need more recipes.For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative like tofu or paneer. Here's how you can adapt the recipe:\n",
      "\n",
      "**Ingredients:**\n",
      "- 1 lb firm tofu or paneer, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt (you can use dairy-free yogurt for a vegan version)\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 tablespoons vegetable oil\n",
      "- 2 cloves garlic, minced\n",
      "- 1 tablespoon grated ginger\n",
      "- 1 tablespoon garam masala\n",
      "- 1 teaspoon ground cumin\n",
      "- 1 teaspoon ground coriander\n",
      "- 1 teaspoon paprika\n",
      "- 1 teaspoon turmeric\n",
      "- 1 teaspoon chili powder (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup coconut cream (or any plant-based heavy cream)\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (for serving)\n",
      "\n",
      "**Instructions:**\n",
      "1. Follow the same marinating process as the original recipe, but use tofu or paneer instead of chicken. Marinate the tofu or paneer for at least 1 hour.\n",
      "   \n",
      "2. Instead of baking, you can pan-fry the marinated tofu or paneer until they are golden brown and cooked through.\n",
      "\n",
      "3. In the skillet, proceed with the recipe by heating the vegetable oil, adding the tomato sauce, and simmering for 5 minutes.\n",
      "\n",
      "4. Stir in the coconut cream (or plant-based heavy cream) and the cooked tofu or paneer. Simmer for another 10 minutes to allow the flavors to meld together.\n",
      "\n",
      "5. Adjust the seasoning if needed and serve the Vegetarian Tikka Masala over rice or with naan bread. Garnish with chopped cilantro.\n",
      "\n",
      "By making these simple swaps, you can enjoy a flavorful and satisfying Vegetarian Tikka Masala that stays true to the essence of the traditional dish. Enjoy your meal!For a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a plant-based alternative like tofu or paneer. Here's how you can adapt the recipe:\n",
      "\n",
      "**Ingredients:**\n",
      "- 1 lb firm tofu or paneer, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt (you can use dairy-free yogurt for a vegan version)\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 tablespoons vegetable oil\n",
      "- 2 cloves garlic, minced\n",
      "- 1 tablespoon grated ginger\n",
      "- 1 tablespoon garam masala\n",
      "- 1 teaspoon ground cumin\n",
      "- 1 teaspoon ground coriander\n",
      "- 1 teaspoon paprika\n",
      "- 1 teaspoon turmeric\n",
      "- 1 teaspoon chili powder (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup coconut cream (or any plant-based heavy cream)\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Cooked rice or naan bread (for serving)\n",
      "\n",
      "**Instructions:**\n",
      "1. Follow the same marinating process as the original recipe, but use tofu or paneer instead of chicken. Marinate the tofu or paneer for at least 1 hour.\n",
      "   \n",
      "2. Instead of baking, you can pan-fry the marinated tofu or paneer until they are golden brown and cooked through.\n",
      "\n",
      "3. In the skillet, proceed with the recipe by heating the vegetable oil, adding the tomato sauce, and simmering for 5 minutes.\n",
      "\n",
      "4. Stir in the coconut cream (or plant-based heavy cream) and the cooked tofu or paneer. Simmer for another 10 minutes to allow the flavors to meld together.\n",
      "\n",
      "5. Adjust the seasoning if needed and serve the Vegetarian Tikka Masala over rice or with naan bread. Garnish with chopped cilantro.\n",
      "\n",
      "By making these simple swaps, you can enjoy a flavorful and satisfying Vegetarian Tikka Masala that stays true to the essence of the traditional dish. Enjoy your meal!\n"
     ]
    }
   ],
   "source": [
    "# ìœ„ Chat model ë¡œ ë‹¤ì‹œ ì‹¤í–‰í•´ë³´ê¸°\n",
    "chef_chain = chef_prompt | chat\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "result = final_chain.invoke({\n",
    "    \"cuisine\": \"indian\",  # chef_chain ì˜ {cuisine} ì— ì „ë‹¬\n",
    "})\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d7474-e2b8-41f2-886a-c01ae7fe9252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9aec4-177b-4f6a-907b-f9dcb4390977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13547f91-a75b-42da-b03a-66c1e387c2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b2a06-e66b-4b95-84a1-7936e689487a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b347c7-b38f-4c0d-a600-11cafeec6234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7471d548-21d7-4521-9d1b-181a53b27fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7a077-1fd9-4a32-a380-d9c56dba1dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455013df-906a-45db-a46b-10357295d07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7056f67-cce0-4cc9-9bf8-b3f413639f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac041dc-5f3b-42d3-b431-632bbae18599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
