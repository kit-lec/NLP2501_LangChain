{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f451fb8-048c-455b-a5a6-df7d5b426123",
   "metadata": {},
   "source": [
    "# Hello LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712498c-b9a3-4917-84f1-d000869ffb91",
   "metadata": {},
   "source": [
    "# LangChain  관련 주요 링크\n",
    "\n",
    "-  Python Langchain 공식 홈:  https://python.langchain.com/\n",
    "-  API 레퍼런스 홈: https://python.langchain.com/api_reference/reference.html\n",
    "\n",
    "\n",
    "## Langchain 의 패키지 구성\n",
    "\n",
    "\n",
    "### Base Packages\n",
    "- [Core: langchain-core](https://python.langchain.com/api_reference/core)\n",
    "- [Langchain: langchain](https://python.langchain.com/api_reference/langchain)\n",
    "- [Test Splitters: langchain-text-splitters](https://python.langchain.com/api_reference/text_splitters)\n",
    "- [Community: langchain-community](https://python.langchain.com/api_reference/community)\n",
    "- [Experimental: langchain-experimental](https://python.langchain.com/api_reference/experimental)\n",
    "\n",
    "### Integrations\n",
    "- 랭체인은 수많은 LLM 모델들과 커뮤니티, 벡터스토어, 데이터베이스, 툴 들과 함께 사용할수 있도록 제공되는 패키지들이 많다 (앞으로 더 많아 질거다)\n",
    "- [OpanAI: langchain-openai](https://python.langchain.com/api_reference/openai)\n",
    "- [Huggingface: langchain-huggingface](https://python.langchain.com/api_reference/huggingface)\n",
    "- [MistalAI: langchain-mistralai](https://python.langchain.com/api_reference/mistralai)\n",
    "- 그밖에도 많이 있다 ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756d471-8be8-46f0-8e42-d4ebbd9dd6d7",
   "metadata": {},
   "source": [
    "# 환경변수 설정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052afee-5248-40bf-a37e-db43ec58a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3345ce0-c96c-49a1-a8a6-704f2dd371cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcdda514-0b65-4836-a31a-d88b1696a2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-proj-iKU13YeoxNgF...'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'][:20] + '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dbc823e-6c98-4913-82bb-66f150880242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수동으로 읽어오기\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b5270c-06a1-41c3-992d-fd9461e96cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c72428-ef63-4745-ad04-ed8bf45a5fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-iKU13YeoxNgF...\n",
      "sk-proj-iKU13YeoxNgF...\n"
     ]
    }
   ],
   "source": [
    "print(f'{os.environ['OPENAI_API_KEY'][:20]}...')\n",
    "print(f'{os.getenv('OPENAI_API_KEY')[:20]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a58299a-3800-4831-b66d-0bc354e3f2b5",
   "metadata": {},
   "source": [
    "# ■ LLM vs. Chat model\n",
    "\n",
    "LangChain 은 LLM 과 Chat model 두가지를 지원합니다\n",
    "\n",
    "`LLM`(Large Language Model)과 `Chat Model`은 비슷한 역할을 하지만, 약간의 차이점이 있습니다.\n",
    "\n",
    "이는 주로 **모델의 입력 및 상호작용 방식**에서 나타난다.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. LLM (Large Language Model)\n",
    "- **특징**:\n",
    "  - 일반적으로 **텍스트 입력**을 받고, 이에 대한 텍스트 출력을 생성합니다.\n",
    "  - 단순한 프롬프트 기반 입력/출력을 처리하기 위한 모델입니다.\n",
    "  - 사용자가 제공한 입력 텍스트를 분석하고, 그에 대한 결과를 한 번에 출력합니다.\n",
    "- **입력 형식**:\n",
    "  ```plaintext\n",
    "  \"Tell me a summary of the benefits of LangChain.\"\n",
    "  ```\n",
    "- **출력 형식**:\n",
    "  ```plaintext\n",
    "  \"LangChain is a framework designed to simplify the development of applications powered by large language models, making it easier to manage prompts, chains, and integrations.\"\n",
    "  ```\n",
    "- **주요 사용 사례**:\n",
    "  - 단일 질문-답변\n",
    "  - 텍스트 생성\n",
    "  - 간단한 프롬프트 처리를 위한 작업\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Chat Model\n",
    "- **특징**:\n",
    "  - **대화 형식**으로 설계된 모델로, 다중 턴 대화를 처리할 수 있다.\n",
    "  - 입력 형식이 **메시지 Message**로 구성되며, 각 메시지는 사용자의 메시지 (User Message)와 시스템의 메시지(System Message)로 나뉜다.\n",
    "  - '문맥'을 이해하고 '대화의 흐름'을 유지하는 데 최적화되어 있다.\n",
    "- **입력 형식**:\n",
    "  메시지 객체를 전달해야 하며, 보통 아래와 같은 구조입니다.\n",
    "  ```python\n",
    "  [\n",
    "      {\"role\": \"system\", \"content\": \"You are an assistant who helps with Python programming.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Can you explain the difference between LLM and chat models in LangChain?\"}\n",
    "  ]\n",
    "  ```\n",
    "- **출력 형식**:\n",
    "  ```python\n",
    "  {\"role\": \"assistant\", \"content\": \"Sure! LLM and Chat Models differ in their input and interaction styles...\"}\n",
    "  ```\n",
    "- **주요 사용 사례**:\n",
    "  - 다중 턴 대화\n",
    "  - 문맥 추적 및 유지 (대화 히스토리 반영)\n",
    "  - 대화 기반 챗봇, FAQ 시스템 등\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 주요 차이점 요약\n",
    "| **특징**        | **LLM**                                                | **Chat Model**                                        |\n",
    "|-----------------|------------------------------------------------------|----------------------------------------------------|\n",
    "| **입력 형식**   | 단일 텍스트 입력                                         | 역할 기반의 대화 메시지 객체 (role: system, user, assistant) |\n",
    "| **대화 히스토리**| 문맥 추적 불가능 (단일 요청 처리)                          | 대화 히스토리를 통해 문맥을 유지하고 반영                 |\n",
    "| **사용 목적**   | 텍스트 생성, 요약, 단순 질의응답                             | 대화형 인터페이스, 챗봇, 다중 턴 질의응답               |\n",
    "| **응용 사례**   | 단일 질문-답변, 텍스트 생성                                | 고객 지원 챗봇, 인터랙티브 Q&A, 멀티턴 대화 시스템          |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 5. 언제 어떤 것을 선택해야 할까요?\n",
    "- **단일 작업이나 간단한 텍스트 생성**:\n",
    "  - `LLM`을 사용하는 것이 적합합니다.\n",
    "- **대화 기반 애플리케이션이나 문맥을 유지해야 하는 작업**:\n",
    "  - `Chat Model`을 사용하는 것이 더 적합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ccee37-7d81-4b7a-acc2-1aae8ed333ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.23'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3551daf4-a602-4ddd-9a8c-5fc8f2a7c0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms.base import OpenAI  # LLM   (gpt-3.5-turbo 사용  2024.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c409e5-10c0-4c0d-b84a-99b642736de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models.base import ChatOpenAI  # Chat model  (gpt-3.5-turbo 사용  2024.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a2b142-c3dc-4fb9-bb81-6c416e353245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랭체인을 사용하면!\n",
    "# 각 model 들의 API 을 따로따로 공부하고 알아야 할 필요없다.\n",
    "# 또한 각 model api 제공자가 제공하는 Python package 를 다운로드 할 필요도 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bc4964e-f686-4625-b10b-26e38366af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ebc1b90-affc-4258-a500-491890c12b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo-instruct'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46396760-901c-49e2-b2da-7c8ba354780f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI()\n",
    "\n",
    "chat.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f37149-925a-48ba-b182-7ff7fa6b668f",
   "metadata": {},
   "source": [
    "# LLM 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ffbb999-4e0e-4f25-9cd9-7b90406f8de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "답변 \n",
      "\n",
      "There are currently eight planets in our solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"How many planets are there?\")\n",
    "print(type(result))\n",
    "print('답변', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ce83bc0-9a66-411c-bb48-6e31290cf5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "답변 \n",
      "\n",
      "태양계에는 8개의 행성이 있으며, 수성, 금성, 지구, 화성, 목성, 토성, 천왕성, 해왕성이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"태양계에는 얼마나 많은 행성들이 있나요? 그 행성들의 이름도 알려줘.\")\n",
    "print(type(result))\n",
    "print('답변', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13cf0a-997e-4feb-abde-e476273087b7",
   "metadata": {},
   "source": [
    "# ChatModel 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08679f59-0a61-47d8-b573-ce8bca5b5424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "💚 content='There are eight planets in our solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 13, 'total_tokens': 40, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BgrgFb7OXnXBUQaqKCJKFQ69itQfn', 'finish_reason': 'stop', 'logprobs': None} id='run--8d1479cf-098b-4389-a408-4d7b5e2e2555-0' usage_metadata={'input_tokens': 13, 'output_tokens': 27, 'total_tokens': 40, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "🧡답변 There are eight planets in our solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\n"
     ]
    }
   ],
   "source": [
    "result = chat.invoke(\"How many planets are there?\")\n",
    "print(type(result))  # Message객체\n",
    "print('💚', result)\n",
    "print('🧡답변', result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f000845-966e-447e-9760-2756fceeca5f",
   "metadata": {},
   "source": [
    "## 한글 or 영어 ?\n",
    "\n",
    "챗 GPT의 언어 처리 능력은 인공지능 기술의 훌륭한 발전을 보여줍니다. 하지만 사용자가 받는 답변의 품질은 제출하는 언어에 따라 약간의 차이가 있을 수 있습니다. 이런 차이는 챗 GPT가 학습하는 과정에서 다양한 언어의 데이터 양과 품질, 그리고 언어별 특성을 얼마나 잘 처리하는지에 따라 결정됩니다.\n",
    "\n",
    "OpenAI의 언어 모델, 특히 GPT 시리즈는 다양한 데이터 소스에서 얻은 대량의 데이터로 학습됩니다. 이 데이터는 주로 영어를 비롯한 여러 언어에서 수집되며, 학습 데이터의 구성은 모델의 성능과 일반화 능력에 큰 영향을 미칩니다.\n",
    "\n",
    "영어는 전세계적으로 많이 사용되며*, 인터넷 상의 데이터도 영어가 많아서 챗 GPT는 영어 질문에 대해 더 정확하고 자연스러운 답변을 제공할 확률이 높습니다. 그러나 한국어와 같은 다른 언어는 상대적으로 데이터가 부족하거나, 언어의 복잡성 때문에 처리가 더 어려울 수 있어, 이로 인해 답변의 품질에 차이가 나타날 수 있습니다.\n",
    "\n",
    "참고\n",
    "- https://fastcampus.co.kr/gov_review_insightGPTlang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99e53fb-c03e-4632-a76c-f34f02a39c39",
   "metadata": {},
   "source": [
    "## 사용량 확인 해보기\n",
    "이쯤에서 openai 페이지\n",
    "DASHBOARD > Usage > Activity 를 확인해보자.\n",
    "사용한 양이 표시될거다.\n",
    "\n",
    "https://platform.openai.com/usage/activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ea5cba1-7480-47db-af08-c5e2eaff696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic.chat_models import ChatAnthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb945fe9-6037-41dc-92ed-8a1a65eb1812",
   "metadata": {},
   "source": [
    "# Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05876845-72ab-4858-b8d3-0d0a842e2db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Model 은 '질문'만 받는게 아니라 '대화' 도 할수 있다 (Message 를 보낼수도 있다)\n",
    "# '대화(conversation)' 은\n",
    "#    : 여러 메세지 묶음\n",
    "#    : 상대의 대화의 맥락에 맞게 대답할수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49555589-c391-4737-9f35-46ed47a3f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/chat/openai/#instantiation\n",
    "# 레퍼런스 : https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html\n",
    "\"\"\"\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "        # 모델의 응답 다양성을 제어하는 역할을 합니다.\n",
    "        # 이는 OpenAI의 GPT 모델에서 사용하는 매개변수로,\n",
    "        #  생성되는 텍스트의 창의성과 확률적 다양성(랜덤성을 조정합니다)ㄴ\n",
    "        #\n",
    "    max_tokens=None,  # model 리 리턴하는 결과의 최대 token 개수지정.\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    # base_url=\"...\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f59b93e-5aee-47ff-bf3a-1bd38ab55505",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dc11095-bcc7-4688-a87e-c69f2c951e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "# https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html\n",
    "\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "# https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html\n",
    "\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "# https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html\n",
    "\n",
    "# HumanMessage : 사람이 AI 에 보내는 Message\n",
    "# SystemMessage : LLM 에 설정들을 제공하기 위한 Message\n",
    "# AIMessage: AI 에 의해 리턴되는 Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "946ebe90-51db-4c82-a036-5d1039a54c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a geography expert. And your only reply in Korean\",\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content=\"안녕, 내 이름은 둘리 야\",\n",
    "    ),\n",
    "    HumanMessage(\n",
    "      content=\"\"\"\n",
    "          What is the distance between Mexico and Thailand.\n",
    "          Also, what is yoru name?\n",
    "      \"\"\",  \n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d02ad89-27fc-4ffc-b69a-1cbb49637e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='멕시코와 태국 사이의 거리는 대략 16,000km입니다. 제 이름은 둘리입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 62, 'total_tokens': 98, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bgs6ftzIvJv3sew7SG7PFFlpPNaEL', 'finish_reason': 'stop', 'logprobs': None}, id='run--31ebb19a-f645-4599-8612-26477f08f7b1-0', usage_metadata={'input_tokens': 62, 'output_tokens': 36, 'total_tokens': 98, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chat.invoke(messages)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737a10e-0168-44dc-815c-33527824867f",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453ab8c-ff24-4652-b328-5ba020937977",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "↑ messages 를 prompt 라고도 함 (?)\n",
    "- 모델에 입력으로 제공되는 텍스트나 데이터\n",
    "- 모델에게 작업을 수행하도록 지시하거나, 모델이 생성할 텍스트의 컨텍스트를 제공\n",
    "- LLM 과 의사소통하기 위한 방법\n",
    "\n",
    "---\n",
    "\n",
    "LLM(대형 언어 모델)에서 **프롬프트 prompt**란 모델에 **입력**으로 제공되는 텍스트나 데이터입니다. 이는 모델에게 작업을 수행하도록 지시하거나, 모델이 생성할 텍스트의 컨텍스트를 제공합니다. 프롬프트는 모델의 출력을 결정하는 중요한 역할을 합니다.\n",
    "\n",
    "### 프롬프트의 역할:\n",
    "1. **모델에 대한 지시**: 프롬프트는 모델에게 무엇을 해야 할지 알려주는 역할을 합니다. 예를 들어, 사용자가 모델에게 질문을 하거나, 특정 스타일의 텍스트를 생성하도록 요청할 때 프롬프트가 필요합니다.\n",
    "\n",
    "2. **컨텍스트 제공**: 모델이 적절한 응답을 생성할 수 있도록 필요한 배경 정보나 문맥을 제공합니다. 예를 들어, 어떤 주제에 대한 질문을 할 때, 관련 배경 정보를 제공하여 모델이 더 정확한 답을 할 수 있게 합니다.\n",
    "\n",
    "3. **모델의 출력 유도**: 프롬프트가 모델의 출력을 유도하고, 생성되는 텍스트의 스타일, 내용, 형식 등을 결정하는 데 중요한 영향을 미칩니다.\n",
    "\n",
    "### 예시:\n",
    "1. **질문 응답**:\n",
    "   - **프롬프트**: \"What is the capital of France?\"\n",
    "   - **출력**: \"The capital of France is Paris.\"\n",
    "\n",
    "2. **창의적 글쓰기**:\n",
    "   - **프롬프트**: \"Write a short story about a dragon and a knight.\"\n",
    "   - **출력**: 모델이 창의적으로 드래곤과 기사에 관한 이야기를 생성합니다.\n",
    "\n",
    "3. **번역**:\n",
    "   - **프롬프트**: \"Translate the following sentence to Spanish: 'Hello, how are you?'\"\n",
    "   - **출력**: \"Hola, ¿cómo estás?\"\n",
    "\n",
    "### 프롬프트의 종류:\n",
    "- **단순한 질문**: 사용자가 단순히 궁금한 점을 묻는 형태.\n",
    "- **지시문**: 특정 작업을 수행하도록 지시하는 형태.\n",
    "- **형식화된 입력**: 특정 형식이나 구조를 갖춘 입력(예: 텍스트 요약, 번역, 코드 작성 등).\n",
    "\n",
    "### 프롬프트 설계의 중요성:\n",
    "- **정확한 결과**를 얻기 위해서는 **프롬프트의 설계**가 매우 중요합니다. 프롬프트가 모호하거나 불완전하면 모델이 원하는 출력을 생성하기 어렵습니다.\n",
    "- 다양한 프롬프트를 실험하면서 모델의 반응을 관찰하고, 가장 적합한 프롬프트를 찾는 것이 중요합니다.\n",
    "\n",
    "### 프롬프트 설계 팁:\n",
    "1. **명확하고 구체적인 지시**: 무엇을 원하는지 정확하게 전달하세요. 예를 들어, \"Explain quantum mechanics\"보다는 \"Explain quantum mechanics in simple terms for a high school student\"와 같이 구체적인 요구를 하는 것이 좋습니다.\n",
    "   \n",
    "2. **적절한 컨텍스트 제공**: 필요한 배경 정보나 문맥을 제공하면 모델이 더 정확한 답변을 생성할 수 있습니다.\n",
    "\n",
    "3. **다양한 실험**: 프롬프트를 조금씩 바꿔가며 테스트해 보면서 최적의 응답을 유도할 수 있습니다.\n",
    "\n",
    "### 결론:\n",
    "프롬프트는 대형 언어 모델에게 작업을 지시하는 중요한 입력으로, 모델이 수행할 작업의 방향을 결정짓는 요소입니다. 프롬프트를 잘 설계하는 것이 LLM을 효과적으로 활용하는 데 큰 도움이 됩니다.## Prompt\n",
    "↑ messages 를 prompt 라고도 함 (?)\n",
    "- 모델에 입력으로 제공되는 텍스트나 데이터\n",
    "- 모델에게 작업을 수행하도록 지시하거나, 모델이 생성할 텍스트의 컨텍스트를 제공\n",
    "- LLM 과 의사소통하기 위한 방법\n",
    "\n",
    "---\n",
    "\n",
    "LLM(대형 언어 모델)에서 **프롬프트 prompt**란 모델에 **입력**으로 제공되는 텍스트나 데이터입니다. 이는 모델에게 작업을 수행하도록 지시하거나, 모델이 생성할 텍스트의 컨텍스트를 제공합니다. 프롬프트는 모델의 출력을 결정하는 중요한 역할을 합니다.\n",
    "\n",
    "### 프롬프트의 역할:\n",
    "1. **모델에 대한 지시**: 프롬프트는 모델에게 무엇을 해야 할지 알려주는 역할을 합니다. 예를 들어, 사용자가 모델에게 질문을 하거나, 특정 스타일의 텍스트를 생성하도록 요청할 때 프롬프트가 필요합니다.\n",
    "\n",
    "2. **컨텍스트 제공**: 모델이 적절한 응답을 생성할 수 있도록 필요한 배경 정보나 문맥을 제공합니다. 예를 들어, 어떤 주제에 대한 질문을 할 때, 관련 배경 정보를 제공하여 모델이 더 정확한 답을 할 수 있게 합니다.\n",
    "\n",
    "3. **모델의 출력 유도**: 프롬프트가 모델의 출력을 유도하고, 생성되는 텍스트의 스타일, 내용, 형식 등을 결정하는 데 중요한 영향을 미칩니다.\n",
    "\n",
    "### 예시:\n",
    "1. **질문 응답**:\n",
    "   - **프롬프트**: \"What is the capital of France?\"\n",
    "   - **출력**: \"The capital of France is Paris.\"\n",
    "\n",
    "2. **창의적 글쓰기**:\n",
    "   - **프롬프트**: \"Write a short story about a dragon and a knight.\"\n",
    "   - **출력**: 모델이 창의적으로 드래곤과 기사에 관한 이야기를 생성합니다.\n",
    "\n",
    "3. **번역**:\n",
    "   - **프롬프트**: \"Translate the following sentence to Spanish: 'Hello, how are you?'\"\n",
    "   - **출력**: \"Hola, ¿cómo estás?\"\n",
    "\n",
    "### 프롬프트의 종류:\n",
    "- **단순한 질문**: 사용자가 단순히 궁금한 점을 묻는 형태.\n",
    "- **지시문**: 특정 작업을 수행하도록 지시하는 형태.\n",
    "- **형식화된 입력**: 특정 형식이나 구조를 갖춘 입력(예: 텍스트 요약, 번역, 코드 작성 등).\n",
    "\n",
    "### 프롬프트 설계의 중요성:\n",
    "- **정확한 결과**를 얻기 위해서는 **프롬프트의 설계**가 매우 중요합니다. 프롬프트가 모호하거나 불완전하면 모델이 원하는 출력을 생성하기 어렵습니다.\n",
    "- 다양한 프롬프트를 실험하면서 모델의 반응을 관찰하고, 가장 적합한 프롬프트를 찾는 것이 중요합니다.\n",
    "\n",
    "### 프롬프트 설계 팁:\n",
    "1. **명확하고 구체적인 지시**: 무엇을 원하는지 정확하게 전달하세요. 예를 들어, \"Explain quantum mechanics\"보다는 \"Explain quantum mechanics in simple terms for a high school student\"와 같이 구체적인 요구를 하는 것이 좋습니다.\n",
    "   \n",
    "2. **적절한 컨텍스트 제공**: 필요한 배경 정보나 문맥을 제공하면 모델이 더 정확한 답변을 생성할 수 있습니다.\n",
    "\n",
    "3. **다양한 실험**: 프롬프트를 조금씩 바꿔가며 테스트해 보면서 최적의 응답을 유도할 수 있습니다.\n",
    "\n",
    "### 결론:\n",
    "프롬프트는 대형 언어 모델에게 작업을 지시하는 중요한 입력으로, 모델이 수행할 작업의 방향을 결정짓는 요소입니다. 프롬프트를 잘 설계하는 것이 LLM을 효과적으로 활용하는 데 큰 도움이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3b928-05b2-4f05-9334-f19f51e0fe35",
   "metadata": {},
   "source": [
    "Prompt 성능이 좋다면 LLM 답변의 성능도 좋을 것입니다.\n",
    "\n",
    "모든 웹 사이트들은 상황에 맞는 뛰어는 성능의 prompt 를 제작하는데 전념함.\n",
    "\n",
    "LangChain 은 prompt 를 공유하기 위한 커뮤니티도 형성되고 있다.\n",
    "산업 전체 전반적으로 각 분야별 prompt 를 만들어 내고 있다.\n",
    "\n",
    "예를 들면\n",
    "| 플랫폼               | 기능              | URL                                                             |\n",
    "| ----------------- | --------------- | --------------------------------------------------------------- |\n",
    "| **LangChain Hub** | 프롬프트 및 체인 공유    | [smith.langchain.com/hub](https://smith.langchain.com/hub)      |\n",
    "| **Discord**       | 커뮤니티, 프롬프트 논의   | [discord.gg/langchain](https://discord.gg/langchain)            |\n",
    "| **GitHub**        | 코드 예제, 프롬프트 활용법 | [LangChain Examples](https://github.com/langchain-ai/langchain) |\n",
    "\n",
    "\n",
    "그래서, LangChain 프레임워크의 많은 부분이 prompt 에 집중되어 있다.\n",
    "\n",
    "prompt 끼리 결함도 할수 있고, 저장하거나 불러올수도 있다.\n",
    "\n",
    "변수 설정 도중에 검증도 할수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee61e4-c1fa-4cd9-80f2-b4da917e30ba",
   "metadata": {},
   "source": [
    "## PromptTemplate\n",
    "메세지 커스터마이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "944e6615-8527-47fa-bebf-c86acce9c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html\n",
    "\n",
    "from langchain_core.prompts.chat import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatMessagePromptTemplate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05bb28c2-140f-4088-977e-85536b969802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country_a', 'country_b'], input_types={}, partial_variables={}, template='What is the distance between {country_a} and {country_b}')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = PromptTemplate.from_template(\n",
    "     # placeholder {...} 사용\n",
    "    \"What is the distance between {country_a} and {country_b}\"\n",
    ")\n",
    "\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c7ff551-685a-4f85-8eaf-066daa82bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template.format() # 에러 KeyError: 'country_a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07e59b79-d26e-4821-a9a0-9ca9eb942a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the distance between Mexico and Brazil'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = template.format(country_a = 'Mexico', country_b = 'Brazil')\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05bf36f8-4687-4790-812a-74fb960236b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The distance between Mexico City, Mexico and Brasília, Brazil is approximately 4,500 kilometers (2,800 miles) when measured in a straight line. However, the actual distance may vary depending on the specific locations within each country.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 15, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BgsH63C1K7IGZVhWbFSK41mpeEkch', 'finish_reason': 'stop', 'logprobs': None}, id='run--8292957c-9157-411c-8ceb-b58408be578c-0', usage_metadata={'input_tokens': 15, 'output_tokens': 48, 'total_tokens': 63, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777d5a1-4a29-4981-ab2a-03f1ee2354c1",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8f1bce1-dd5c-4033-af42-30f5bffdc2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['country_a', 'country_b', 'language', 'name'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a geography expert. And your only reply in {language}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='안녕, 내 이름은 {name} 야'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country_a', 'country_b'], input_types={}, partial_variables={}, template='\\n        What is the distance between {country_a} and {country_b}.\\n        Also, what is your name?\\n        '), additional_kwargs={})])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    # SystemMessage 튜플\n",
    "    (\"system\", \"You are a geography expert. And your only reply in {language}\"),\n",
    "    # AIMessage 튜플\n",
    "    (\"ai\", \"안녕, 내 이름은 {name} 야\"),\n",
    "    # HumanMessage 튜플\n",
    "    (\"human\", \"\"\"\n",
    "        What is the distance between {country_a} and {country_b}.\n",
    "        Also, what is your name?\n",
    "        \"\"\"),    \n",
    "])\n",
    "\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e095f217-d479-44f5-b4e3-419b731e3470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a geography expert. And your only reply in Korean', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='안녕, 내 이름은 뽀로로 야', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='\\n        What is the distance between Canada and Japan.\\n        Also, what is your name?\\n        ', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = template.format_messages(\n",
    "    language=\"Korean\",\n",
    "    name=\"뽀로로\",\n",
    "    country_a=\"Canada\",\n",
    "    country_b=\"Japan\",\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1e111b5-b02d-448b-87a8-103c34ebb4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='캐나다와 일본 사이의 거리는 약 7,000km입니다. 제 이름은 뽀로로입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 62, 'total_tokens': 97, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BgsMdUNP0LFyAZN2jMBuXh0Zt4nOQ', 'finish_reason': 'stop', 'logprobs': None}, id='run--7290cfee-22b6-41c6-919e-f8bd333ae1dd-0', usage_metadata={'input_tokens': 62, 'output_tokens': 35, 'total_tokens': 97, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714540e8-89ee-4c3d-b540-2636dac098e7",
   "metadata": {},
   "source": [
    "# OutputParser and LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c82a91-f930-42dd-a4c8-1d8f9aa2c918",
   "metadata": {},
   "source": [
    "## Output Parser 란\n",
    "\n",
    "LLM(대형 언어 모델)에서 생성된 출력을 처리하고 '원하는 형식으로 변환'하는 데 사용되는 유틸리티입니다. 이를 통해 모델이 생성하는 텍스트를 '구조화된 데이터로 변환'하거나, '특정 규칙에 따라 데이터를 추출'할 수 있습니다\n",
    "\n",
    "1. 출력 구조화\n",
    "    - 모델의 텍스트 응답을 파싱하여 JSON, 딕셔너리, 목록 등과 같은 프로그래밍에서 사용 가능한 구조화된 데이터로 변환합니다\n",
    "    \n",
    "1. 출력 검증\n",
    "    - 모델이 예상치 못한 출력을 반환할 경우 적절한 에러 메시지를 제공하거나 기본값을 반환하도록 처리할 수 있습니다.\n",
    "    \n",
    "1. 출력 표준화\n",
    "    - 언어 모델의 출력이 항상 일관된 형식으로 제공되도록 보장합니다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da79679-449d-49c6-8c5e-0e0426ff7ba1",
   "metadata": {},
   "source": [
    "## BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25978921-f55d-4fb4-abf1-a89e6435edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제: LLM 의 출력을 -> list 로 변환시켜 보자.  (변환시키는 OutputParser 를 만들어 보자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a32250da-d4c3-4658-95cf-2d13dbb1260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.output_parsers.base import BaseOutputParser\n",
    "# https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html\n",
    "\n",
    "# ↓ 이를 상속 받아 OutputParser 를 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a00c83f-918c-48a3-99b7-a45b0bda2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommaOutputParser(BaseOutputParser):\n",
    "\n",
    "    # parse() 메소드를 반.드.시 구현.\n",
    "    #   text = 입력텍스트\n",
    "    def parse(self, text): \n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21b22dd8-4c0d-4bc8-89ad-9d4e72a47acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = CommaOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74cf95e9-153f-4a0a-8d4d-4ba1f4c7ee61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 동작 확인\n",
    "p.parse(\"   Hello,    how,   are,   you   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ca578a1-3843-4776-842c-8999f74ff63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='1. Mercury\\n2. Venus\\n3. Earth\\n4. Mars\\n5. Jupiter\\n6. Saturn\\n7. Uranus\\n8. Neptune\\n9. Pluto', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 47, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BgsXhgMoZSiwQPlFPBu9wC5iInfsV', 'finish_reason': 'stop', 'logprobs': None}, id='run--9d2b06d7-6654-45a2-8806-16048ad150e9-0', usage_metadata={'input_tokens': 47, 'output_tokens': 36, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "******************************\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune\n",
      "9. Pluto\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the planets?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.__repr__())\n",
    "print('*' * 30)\n",
    "print(result.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fbda688-4ae7-49a4-b9f3-32321185e27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 49, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BgsZbQ7Jp4mPHuGCNP3SlkbWtATjA', 'finish_reason': 'stop', 'logprobs': None}, id='run--c9aba4bf-4142-4842-b870-2078527e84a8-0', usage_metadata={'input_tokens': 49, 'output_tokens': 17, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "******************************\n",
      "Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a comma separated list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the planets?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.__repr__())\n",
    "print('*' * 30)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ac815cb-21a2-43bf-a453-192801f7cb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='red, blue, green, yellow, orange, purple, pink, black, white, brown', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 49, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BgsbE2FgauuFQ3K0GzKXzH3RqK1fd', 'finish_reason': 'stop', 'logprobs': None}, id='run--d247344a-16ce-4471-a5a1-ee616177d022-0', usage_metadata={'input_tokens': 49, 'output_tokens': 19, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "******************************\n",
      "red, blue, green, yellow, orange, purple, pink, black, white, brown\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered with a comma separated list of max {max_items}.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the colors?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.__repr__())\n",
    "print('*' * 30)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7575fd7b-57df-4d04-ad37-f20b5367f3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='red, blue, green, yellow, orange, purple, pink, black, white, brown', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 53, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BgscWRry5HrkkxXoSwEUtDh9vxXoo', 'finish_reason': 'stop', 'logprobs': None}, id='run--7741c850-fa48-4f1f-bf8e-7702f4a0da4c-0', usage_metadata={'input_tokens': 53, 'output_tokens': 19, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "******************************\n",
      "red, blue, green, yellow, orange, purple, pink, black, white, brown\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered \n",
    "        with a comma separated list of max {max_items} in lowercase.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the colors?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "print(result.__repr__())\n",
    "print('*' * 30)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa00ea5f-9722-4c48-a3b3-f91cd89750d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'orange',\n",
       " 'purple',\n",
       " 'pink',\n",
       " 'black',\n",
       " 'white',\n",
       " 'brown']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"\"\"You are a list generating machine.\n",
    "        Everything you are asked will be answered \n",
    "        with a comma separated list of max {max_items} in lowercase.\n",
    "        Do NOT reply with anything else.\"\"\"),\n",
    "\n",
    "      (\"human\", \"{question}\")    \n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10,\n",
    "    question='What are the colors?'\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c356e142-1d4b-4de2-8438-eb86103c39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "template -> format -> invoke -> parser ...\n",
    "\n",
    "너무 단계가 많아 보인다..   이를 죄다 직접 하드 코딩하다니?\n",
    "\n",
    "↓ LCEL 을 사용하면 위 과정들이 많~이 생략된다!\n",
    "   => 바로 chain 의 등장!\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34575f-623b-4375-b49d-aec0a90fa2d7",
   "metadata": {},
   "source": [
    "## Chain, LCEL\n",
    "\n",
    "- LCEL (LangChain Expression Language: 랭체인 표현 언어)\n",
    "  - LCEL은 LangChain 내에서 복잡한 표현식을 처리하고,\n",
    "  - 모델과의 상호작용을 더 강력하고 유연하게 만드는 기능을 제공\n",
    "    - 코드양을 많이 줄여줌.\n",
    "    - 다양한 template 과 LLM 호출\n",
    "    - 서로 다른 응답(response) 를 함께 사용케 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b6289d66-720a-4abd-be51-bbc2c01acde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['max_items', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['max_items'], input_types={}, partial_variables={}, template='You are a list generating machine.\\n        Everything you are asked will be answered \\n        with a comma separated list of max {max_items} in lowercase.\\n        Do NOT reply with anything else.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002D14A3CEC30>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002D14A3E07D0>, root_client=<openai.OpenAI object at 0x000002D149E87BC0>, root_async_client=<openai.AsyncOpenAI object at 0x000002D1498C1610>, temperature=0.1, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| CommaOutputParser()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain 생성!\n",
    "#  '|' 연산자 사용\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "print(type(chain))\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46b9ccb6-1f90-4b04-8266-4a43eaa20d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pikachu', 'charmander', 'bulbasaur', 'squirtle', 'jigglypuff']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain 호출!  invoke({...})\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\": \"What are the pokemons?\",  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1a689-4157-4a6c-892d-d9b841aac879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6610e-8669-404b-86dd-acfa5d2bee2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7471d548-21d7-4521-9d1b-181a53b27fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7a077-1fd9-4a32-a380-d9c56dba1dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455013df-906a-45db-a46b-10357295d07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7056f67-cce0-4cc9-9bf8-b3f413639f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac041dc-5f3b-42d3-b431-632bbae18599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
