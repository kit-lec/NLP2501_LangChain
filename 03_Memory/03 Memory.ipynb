{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7e0bc7-da43-4c2a-80be-e64394cc8ed4",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "https://python.langchain.com/api_reference/langchain/memory.html\n",
    "\n",
    "챗봇으로 하여금 대화(상태)를 '기억'하게끔 한다\n",
    "\n",
    "Memory maintains Chain state, incorporating context from past runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986f60e-1848-4a26-b5f1-d89907f400a1",
   "metadata": {},
   "source": [
    "# 기존 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9496d7-df3d-412c-9e09-5c872bdf6332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea68021-c212-4a14-92ab-3d35b288f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 사에서 제공하는 '기본 API' 도 랭체인 없이 사용 가능.\n",
    "# 메모리 지원하지 않는다.  이전 대화 기억 못함.  stateless 하다!\n",
    "\n",
    "# ChatGPT 는 '메모리' 기능이 탑재되어 있다.\n",
    "# 챗봇이 이전의 대화 내용과 질문을 기억하고 답할수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bacd90e5-8b8f-4d24-9ed2-4b3491c4449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain 의 memory 계층\n",
    "#  BaseMemory --> BaseChatMemory --> <name>Memory  # Examples: ZepMemory, MotorheadMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b6cef3-ccf0-405d-9215-b3726d1894a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5487dc7-f5fa-446e-8dca-d66058dcbf32",
   "metadata": {},
   "source": [
    "# ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9a8bed-4050-4f17-8b71-6ad36b865cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationBufferMemory\n",
    "# 대화 내용 '전체'를 저장하는 메모리\n",
    "\n",
    "# 장점: 단순하다\n",
    "\n",
    "# 단점:\n",
    "# => 매번 요청할때마다 '이전 대화 기록 전체' 를 같이 보내야 함.\n",
    "#  그래야 모델이 전에 일어났던 대화를 보고 이해 할수 있다.\n",
    "#  대화내용이 길어질수록 메모리도 계속 커지니까 성능적으로도 & 비용적으로도 비효율적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b877cc9c-8feb-4bb5-be3b-1154afb31977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "# https://python.langchain.com/api_reference/langchain/memory/langchain.memory.buffer.ConversationBufferMemory.html#conversationbuffermemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80295ce2-c9c4-42f2-8840-31394bcc2599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12440\\3832128472.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi!\\nAI: How are you?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# 직접 save 해보기\n",
    "memory.save_context(\n",
    "    {'input': 'Hi!'}, # 사용자 입력값\n",
    "    {'output': 'How are you?'}, # AI가 사용자에게 뭐라고 답할지 \n",
    ")\n",
    "\n",
    "# history buffer 를 리턴\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6141a491-6cde-43da-a400-6f137b413c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ★참고★ memory 종류와 관계없이 API 는 다 똑같다\n",
    "# 즉, 모든 memory 는 save_context(), load_memory_variables() 함수를 갖고 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4b0eaf-32c0-41cd-b58d-b6d3f87d9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatModel 과 작업을 하게 되면\n",
    "# AIMessage 와 HumanMessage 가 다 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c2418-e2db-40ec-877b-3b470a5d2804",
   "metadata": {},
   "source": [
    "## return_messages=True\n",
    "history 에  AIMessage 와 HumanMessage 로 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "688d5be0-f10d-43ca-b151-0abf13372a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[]), return_messages=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "752d06f9-033e-4498-9960-8f13e5d5cba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context(\n",
    "    {'input': 'Hi!'},\n",
    "    {'output': 'How are you?'},\n",
    ")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5823d7e5-bd9b-421d-836d-0b5fd53e28d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context(\n",
    "    {'input': 'Hi!'},\n",
    "    {'output': 'How are you?'},\n",
    ")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9c93951-940d-476d-bb3f-f60431f960df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- 우선 메모리를 만들고\n",
    "- ChatModel을 위한건지 아닌지 선택하고\n",
    "- ChatModel을 위한게 아니라면 return_messages=False (디폴트)\n",
    "    => 그러면 history 는 문자열로 표시됨.\n",
    "- ChatModel을 위한거라면 return_messages=True\n",
    "    => 그러면 history  는 챗모델이 사용할수 있는 형태로 출력됨\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18da50f-d407-4c33-b167-3b7b67a572dd",
   "metadata": {},
   "source": [
    "# ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e42c2d95-3240-4828-a566-ae24bab562bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationBufferWindowMemory 는 대화의 '특정 부분만' 을 저장하는 메모리.\n",
    "\n",
    "# 장점:\n",
    "#   메모리를 특정 크기로 유지할 수 있다!\n",
    "#   따라서 모든 대화 내용을 저장하지 않아도 된다!\n",
    "\n",
    "# 단점:\n",
    "#   챗봇이 전체 대화가 아닌 '최근 대화' 에만 집중하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d75b3da0-efaa-497f-9eb1-95820c06d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain.memory.buffer_window import ConversationBufferWindowMemory\n",
    "# https://python.langchain.com/api_reference/langchain/memory/langchain.memory.buffer_window.ConversationBufferWindowMemory.html#conversationbufferwindowmemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72a67e9c-7a19-4915-86f5-69c8964081bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12440\\2498295360.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,  # 버퍼 윈도우 사이즈.  몇개의 메세지를 저장할지 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c392ca54-1161-4304-8a47-06a823d9d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도우미 함수 준비.\n",
    "def add_message(input, output):\n",
    "    memory.save_context({'input': input}, {'output': output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f73e826f-6fd4-4dcd-a592-49deb05498d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"1\", \"1\")\n",
    "add_message(\"2\", \"2\")\n",
    "add_message(\"3\", \"3\")\n",
    "add_message(\"4\", \"4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8080c03f-7795-4f85-8ea7-927f6cb2354b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k=4.\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8775082-8cef-4544-8976-2a88bcdaec1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='5', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='5', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"5\", \"5\")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c3164d-3a74-4260-8528-48729aebe74f",
   "metadata": {},
   "source": [
    "# ConversationSummaryMemory\n",
    "- 대화를 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5bbec37-3542-4054-a391-488a05fc1507",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24019565-852a-4750-b0f8-faf7200d044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# v0.3\n",
    "from langchain.memory.summary import ConversationSummaryMemory\n",
    "# https://python.langchain.com/api_reference/langchain/memory/langchain.memory.summary.ConversationSummaryMemory.html#langchain.memory.summary.ConversationSummaryMemory\n",
    "\n",
    "# Continually summarizes the conversation history.\n",
    "# The summary is updated after each conversation turn.\n",
    "# The implementations returns a summary of the conversation history\n",
    "# which can be used to provide context to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "044d5f62-a94e-41e9-a24e-16ad9d200b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12440\\3108008542.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "# ConversationSummaryMemory\n",
    "# 메세지를 그대로 저장하는 것이 아니라 Convertaion 의 '요약'을 해준다. LLM 필요\n",
    "#  장점: 대화의 메세지가 많아질수록 요약을 해주어 입력 토큰의 양도 줄여줌.\n",
    "memory = ConversationSummaryMemory(llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acfcf8c0-3bba-4a51-9e27-f8d5cd5deac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_history():\n",
    "    return memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb6c4486-dfa2-4b6b-9b28-f2dc669083ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message 추가\n",
    "add_message(\n",
    "    \"Hi I'm John, I live in South Korea\",    # input\n",
    "    \"Wow that is so cool!\"  # output : AI 답변\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "956404f7-acca-4b93-a686-4382854d497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 또 message 추가\n",
    "add_message(\n",
    "    \"South Korea is so pretty\",\n",
    "    \"I wish I could go!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef2d9e69-06a4-4117-b40f-e7172942625d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'John introduces himself as living in South Korea. The AI responds by expressing admiration for his location, saying it wishes it could go there because South Korea is so pretty.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5149663-3ab6-4148-996b-39ead6e9fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ 대화를 '요약' 한 내용으로 기억하고 있다\n",
    "# 대화의 turn 이 길어질수로 summary 가 각 메세지를 효율적으료 '요약(압축)' 해준다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a917c383-5926-4506-92dd-31554e5c9ab5",
   "metadata": {},
   "source": [
    "# ConversationSummaryBufferMemory\n",
    "- summary + buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a2fbcba-8f83-4669-9853-aa83d7a1b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain.memory.summary_buffer import ConversationSummaryBufferMemory\n",
    "# https://python.langchain.com/api_reference/langchain/memory/langchain.memory.summary_buffer.ConversationSummaryBufferMemory.html#langchain.memory.summary_buffer.ConversationSummaryBufferMemory\n",
    "\n",
    "# Buffer with summarizer for storing conversation memory.\n",
    "# Provides a running summary of the conversation together with\n",
    "#  the most recent messages in the conversation under the constraint\n",
    "#   that the total number of tokens in the conversation does not exceed a certain limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf26c062-4410-4f1f-80df-2df11fbed11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationSummaryBufferMemory 는\n",
    "#   ConversationBufferMemory 와 ConversationSummaryMemory 의 결합형\n",
    "\n",
    "# 메모리에 보내온 메세지의 수를 지정하여 저장한다.\n",
    "# 오래된 메세지들 또한 요약 하여 저장함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cb2143a-fd98-4ac0-a942-978b9f243619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12440\\3522467102.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150,   # 최대 가용한 메세지 토큰수  (메세지 요약되기 전)\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68dc6915-d5cf-4d7f-b677-97017e2b156f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메세지 추가하고 history 확인\n",
    "\n",
    "add_message(\n",
    "    \"Hi I'm John, I live in South Korea\",    # input\n",
    "    \"Wow that is so cool!\"  # output : AI 답변\n",
    ")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70c3650c-e5d8-403b-95bc-3c292dd254ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='South Korea is so pretty', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I wish I could go!!!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다시 메세지 추가하고 확인!\n",
    "\n",
    "add_message(\n",
    "    \"South Korea is so pretty\",\n",
    "    \"I wish I could go!!!\")\n",
    "\n",
    "get_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b6607df-049f-47c2-9056-8d1037e466cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ 아직까진 max_token_limit=150 이하이다  (요약은 발생하지 않았다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0af94251-0c9f-42b2-953b-84ff102552f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='South Korea is so pretty', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I wish I could go!!!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Korea from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\n",
    "    \"How far is Korea from Argentina?\",\n",
    "    \"I don't know! Super far!\"\n",
    ")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc9ff2a9-c62e-4c86-94ca-b1a1e55f81b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ 아직까진 max_token_limit=150 이하이다  (요약은 발생하지 않았다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4006a690-59da-4b12-a5ac-a5c7b291ca6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm John, I live in South Korea\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Wow that is so cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='South Korea is so pretty', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I wish I could go!!!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Korea from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know! Super far!\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\n",
    "    \"How far is Brazil from Argentina?\",\n",
    "    \"I don't know! Super far!\"\n",
    ")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fabebbc1-7507-41d2-bceb-b440a65174d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit 에 도달하면, 오래된 메세지들이 요약되고 있을 것을 확인할수 있다. (SystemMessage 확인)\n",
    "\n",
    "# ★그러나 '요약' 이라는 과정은 API 를 사용한다는 사실을 명심하세요.\n",
    "# ★'요약' 동작은 비용 지출이 발생되는 부분입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763abb33-4f4f-4f2d-b4cf-c03e83323806",
   "metadata": {},
   "source": [
    "# ConversationKGMemory\n",
    "Conversation Knowledge Graph Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34fc4237-b892-4b1b-af39-742a9725234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화중에 '엔티티'의 knowledge graph 를 형성한다 => 가장 중요한 것들만 추출한 요약본.\n",
    "# knowledge graph 는 history 를 가지고 오지 않는다.  대신 '엔티티' 를 가지고 옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f782990f-b57e-4dd3-8b63-6a767bce825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_community.memory.kg import ConversationKGMemory\n",
    "# https://python.langchain.com/api_reference/community/memory/langchain_community.memory.kg.ConversationKGMemory.html\n",
    "\n",
    "# Knowledge graph conversation memory.\n",
    "# Integrates with external knowledge graph to store and retrieve information about knowledge triples in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96e8f810-7c51-400e-88c7-8903a1af4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0070172c-f58f-4739-9a6d-d7b0dd612bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\n",
    "    \"Hi I'm John, I live in South Korea\",    # input\n",
    "    \"Wow that is so cool!\"  # output : AI 답변\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abf27960-7a3c-4694-8abf-9e6c702ccaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On John: John lives in South Korea.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이후에,  대화의 특정 엔티티(entity) 에 대해 질문해보자\n",
    "memory.load_memory_variables({\"input\": \"who is John\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63a35a37-fac1-4190-9d45-bc7c18f10bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SystemMessage 에서 '요약' 은 되었지만,  대화에서 entity 를 뽑아내는 거다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf6ac0ab-9fd1-43a3-8785-5dd8a96e5a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 메세지를 더해보자\n",
    "add_message(\"John likes kimchi\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd10aa42-e1c7-4803-8c16-1ab9b0ac9f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On John: John lives in South Korea. John likes kimchi.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"What does John like\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be82415e-8cdb-4e21-bb1c-a7ffd01bc8b0",
   "metadata": {},
   "source": [
    "# 그 밖의 메모리들.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6cd36cdc-fe5e-4732-b53c-9e69e14d36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationTokenBufferMemory 도 있다\n",
    "#   interaction 의 최대값을 가지고 있는 것 대신에 token의 총 양을 계산하는게 전부다.\n",
    "#   max_token_limit= 값!  <= ConversationBufferWindowMemory 와 비슷하다 (k= 값)\n",
    "\n",
    "# https://python.langchain.com/api_reference/langchain/memory/langchain.memory.token_buffer.ConversationTokenBufferMemory.html#langchain.memory.token_buffer.ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e79c6854-ddf1-4f2d-9a56-bfe86957278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationEntityMemory\n",
    "#  Entity 를 활용한 메모리도 있다.\n",
    "#  이는 대화중에 entity 를 추출해 활용\n",
    "# https://python.langchain.com/api_reference/langchain/memory/langchain.memory.entity.ConversationEntityMemory.html#langchain.memory.entity.ConversationEntityMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244407d-254d-4037-87f6-9022dc667591",
   "metadata": {},
   "source": [
    "- **참고: Database 와 integration 된 메모리들**\n",
    "\n",
    "| Memory Class                  | 통합 대상 (Integration)             | 설명                                                           |\n",
    "| ----------------------------- | ------------------------------- | ------------------------------------------------------------ |\n",
    "| `RedisChatMessageHistory`     | **Redis**                       | Redis에 메시지 저장. 빠르고 확장 가능한 저장소.                               |\n",
    "| `SQLChatMessageHistory`       | **SQLite, PostgreSQL 등 SQL DB** | SQL 데이터베이스에 메시지 저장. SQLAlchemy 기반.                           |\n",
    "| `DynamoDBChatMessageHistory`  | **AWS DynamoDB**                | AWS의 NoSQL DB인 DynamoDB에 대화 저장. 서버리스 환경에서 유용.                |\n",
    "| `MongoDBChatMessageHistory`   | **MongoDB**                     | 문서 기반 DB인 MongoDB와 통합하여 대화 저장.                               |\n",
    "| `PostgresChatMessageHistory`  | **PostgreSQL**                  | PostgreSQL 전용 구현 (SQLAlchemy 없이).                            |\n",
    "| `FileChatMessageHistory`      | **Local 파일**                    | JSON 파일로 로컬에 저장. 간단한 로깅에 적합.                                 |\n",
    "| `FirestoreChatMessageHistory` | **Google Firestore**            | Firebase 기반 클라우드 NoSQL DB와 통합.                               |\n",
    "| `ChromaMemory`                | **Chroma (벡터 DB)**              | 벡터 DB에 embedding 형태로 memory 저장. RAG나 유사 검색 기반 memory에 활용 가능. |\n",
    "| `WeaviateMemory`              | **Weaviate**                    | Weaviate 벡터 DB와 통합된 memory 저장.                               |\n",
    "| `QdrantMemory`                | **Qdrant**                      | 벡터 기반 memory 저장소로 Qdrant 사용.                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcbf392-1baa-41ac-bce1-9c46ca255ce2",
   "metadata": {},
   "source": [
    "# Memory on LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d78db134-9d9c-4c7c-9eb1-8267b6b7cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMChain 은 off-the-shelf chain으로서 '일반적인 목적' 의 chain을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d9dcdf4-41e1-44a2-90dc-62def2448000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "150eae54-de2d-4150-814b-db07a7737497",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d591a4e2-fd65-43c4-8c16-7fe9c2d802d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12440\\1686826008.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,  # 메모리 제공!\n",
    "    prompt=PromptTemplate.from_template(\"{question}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90f48ef9-3d27-4b9a-adfb-006be0a59f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'My name is John',\n",
       " 'history': '',\n",
       " 'text': 'Nice to meet you, John! How can I assist you today?'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'My name is John'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9630cb95-dd0e-441c-aba6-df49100ccd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'history': 'Human: My name is John\\nAI: Nice to meet you, John! How can I assist you today?',\n",
       " 'text': \", the capital city of South Korea. It is a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic palaces. I love exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views of the Han River. Seoul is a dynamic and exciting place to call home.\"}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={'question':\"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0003ba7-53ea-41cb-92bc-75c3d4961bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'question': 'I live in Seoul',\n",
    " 'history': 'Human: My name is John\\nAI: Nice to meet you, John! How can I assist you today?',\n",
    " 'text': \", the capital city of South Korea. It is a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic palaces. I love exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views of the Han River. Seoul is a dynamic and exciting place to call home.\"}\n",
    "\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "521fbafe-c9ab-4194-a033-57834a2efea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'history': \"System: The human introduces himself as John. The AI greets John and asks how it can assist him. John mentions that he lives in Seoul.\\nAI: , the capital city of South Korea. It is a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic palaces. I love exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views of the Han River. Seoul is a dynamic and exciting place to call home.\",\n",
       " 'text': \"I'm sorry, I do not have access to personal information such as your name.\"}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 과연 내 이름을 기억하곤 있을까?\n",
    "chain.invoke({'question':'What is my name?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1205126c-f4eb-484f-a12a-e2d122412dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'question': 'What is my name?',\n",
    " 'history': \"System: The human introduces himself as John. The AI greets John and asks how it can assist him. John mentions that he lives in Seoul.\\nAI: , the capital city of South Korea. It is a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic palaces. I love exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views of the Han River. Seoul is a dynamic and exciting place to call home.\",\n",
    " 'text': \"I'm sorry, I do not have access to that information.\"}  <-- 어라? 모른다고?\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fe1173ee-cc97-49a4-a1af-fbfd29185003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그럼 chain 을 디버깅 해보자!\n",
    "# verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef10a152-55c4-409a-a9b6-71c615cfecc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mMy name is John\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'history': \"System: John introduces himself as living in Seoul, the capital city of South Korea. The AI describes Seoul as a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic palaces. The AI loves exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views of the Han River, making Seoul a dynamic and exciting place to call home. The AI then learns John's name and greets him, ready to assist with any questions or requests he may have.\\nAI: , the capital city of South Korea. It is a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic temples. I love exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views of the Han River. Seoul is a dynamic and exciting place to call home.\",\n",
       " 'text': \"I'm sorry, I do not have access to personal information such as your name.\"}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory, \n",
    "    prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "    verbose=True,   # chain 을 실행했을때 chain 의 프롬프트 로그들을 확인할수 있다 (디버깅용 활용)\n",
    ")\n",
    "\n",
    "chain.invoke(input={'question':\"My name is John\"})\n",
    "chain.invoke(input={'question':\"I live in Seoul\"})\n",
    "chain.invoke(input={'question':\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c857ba43-f27c-4183-9156-cc81591d951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "> Entering new LLMChain chain...\n",
    "Prompt after formatting:\n",
    "My name is John   <--- 이게 프롬프트의 전부다!?\n",
    "\n",
    "> Finished chain.\n",
    "\n",
    "\n",
    "> Entering new LLMChain chain...\n",
    "Prompt after formatting:\n",
    "I live in Seoul <--- 이게 프롬프트의 전부다!?\n",
    "\n",
    "> Finished chain.\n",
    "\n",
    "\n",
    "> Entering new LLMChain chain...\n",
    "Prompt after formatting:\n",
    "What is my name? <--- 이게 프롬프트의 전부다!?\n",
    "\n",
    "> Finished chain.\n",
    "\n",
    "\n",
    "verbose=True 를 통해 chain 의 prompt 로그를 확인할수 있다.\n",
    "prompt 에는 대화의 history 가 없었다!!!\n",
    "\n",
    "=> prompt 에 대화의 history 를 추가해주어야 한다!\n",
    "\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f11365c-2e14-4c28-a301-0379cd4c4f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: John introduces himself as living in Seoul, the capital city of South Korea. The AI describes Seoul as a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic temples. The AI loves exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views of the Han River, making Seoul a dynamic and exciting place to call home.\\nHuman: What is my name?\\nAI: I'm sorry, I do not have access to personal information such as your name.\"}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그러나! 메모리는 계속 요약 업데이트 되고 있다! \n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b090367-d512-4658-9f41-41251249fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ 위 메모리의 내용이 prompt 에 포함되어야 하는 것이다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2308788a-978a-4d90-ba74-71d4c36c3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d0ea8f16-69c7-4b86-a60d-71499a4deb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history 까지 담을 괜찮은 템플릿을 준비해보자\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "# AI 가 우리의 대화기록을 기억하면서 여기의 question 을 완성할 수 있기를 기대해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb6f4c-af7c-4fa3-92d3-3e64a80cc0dc",
   "metadata": {},
   "source": [
    "## memory_key=\"chat_history\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4e729c7-c2c6-40c9-bd3f-2876fb7633d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",   # 간단하게 이렇게만 지정해도 된다!?!  load_memory_variables({})  안해도 된단다!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f5871e2f-5f20-4d78-9262-7037d8c8e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),  # <-- template 지정\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6221a5e4-daf9-4f09-9596-ccf9ad602899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human:My name is John\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'My name is John',\n",
       " 'chat_history': '',\n",
       " 'text': 'Hello John! How can I assist you today?'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={'question':\"My name is John\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f0b3a3b7-270f-41ad-a6b6-40049ce3db3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "    Human:I live in Seoul\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'chat_history': 'Human: My name is John\\nAI: Hello John! How can I assist you today?',\n",
       " 'text': \"That's great to hear! How can I assist you with information or tasks related to Seoul?\"}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={'question':\"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d9362de9-aac0-429e-b98d-4f683e6f209d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to hear! How can I assist you with information or tasks related to Seoul?\n",
      "    Human:What is my name?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'chat_history': \"Human: My name is John\\nAI: Hello John! How can I assist you today?\\nHuman: I live in Seoul\\nAI: That's great to hear! How can I assist you with information or tasks related to Seoul?\",\n",
       " 'text': 'Your name is John.'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={'question':\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26536067-1c5f-4c18-a69e-3221edca8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑ 보다시피 prompt 에 대화의 chat history 가 남겨져 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "66f82449-9bc0-470f-b03c-f114fbbd9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'question': 'What is my name?',\n",
    " 'chat_history': \"Human: My name is John\\nAI: Nice to meet you, John! How can I assist you today?\\nHuman: I live in Seoul\\nAI: That's great to know! How can I assist you with information or tasks related to Seoul?\",\n",
    " 'text': 'Your name is John.'}  <-- 최종적으로 What is my name 에 대한 답변을 해준다.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a0ad78c-a8da-4d95-a252-8c264c950051",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- 프롬프트 템플릿 안에서 메모리 내용이 들어갈 공간을 준비한다.  (예: chat_history)\n",
    "- 메모리를 활용할 템플릿은 원하는대로 작성하면 된다.\n",
    "- Memory 클래스에선 history 를 어디에 꽂을지 지정해준다 (memory_key=)\n",
    "\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a535f5aa-9ead-44d5-be66-fdfe9bde5fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to hear! How can I assist you with information or tasks related to Seoul?\n",
      "Human: What is my name?\n",
      "AI: Your name is John.\n",
      "    Human:My name is John\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to hear! How can I assist you with information or tasks related to Seoul?\n",
      "Human: What is my name?\n",
      "AI: Your name is John.\n",
      "Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "    Human:I live in Seoul\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'chat_history': \"Human: My name is John\\nAI: Hello John! How can I assist you today?\\nHuman: I live in Seoul\\nAI: That's great to hear! How can I assist you with information or tasks related to Seoul?\\nHuman: What is my name?\\nAI: Your name is John.\\nHuman: My name is John\\nAI: Hello John! How can I assist you today?\",\n",
       " 'text': \"That's great to hear! How can I assist you with information or tasks related to Seoul?\"}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 잘 요약이 되는지도 확인해보자\n",
    "chain.invoke({'question': \"My name is John\"})\n",
    "chain.invoke({'question': \"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab20e1b-8c9c-42eb-a095-d6921064d1d9",
   "metadata": {},
   "source": [
    "# Chat based memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d00d84db-f11c-47f1-b45f-186f462d6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '메세지' 를 기반으로 한 '사람과 AI' 의 대화 history 를 추가하는 방법을 알아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf6d2975-14b8-4608-9c35-ba971eb5f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mamory 클래스 출력방식 2가지\n",
    "#  문자열 형태\n",
    "#  message 형태"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3083ff6b-9b08-4b1d-af04-4bfec19dbb5f",
   "metadata": {},
   "source": [
    "## return_message=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a41fdc83-181a-45dd-8ad4-9627a2fe1897",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\", \n",
    "    return_messages=True,   # 문자열이 아닌 message 로 리턴함.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "06b534c3-c263-40d9-abe2-ded74b8bdd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과연 prompt 에는 어떻게 대화 history 들을 넘겨줄수 있을까?\n",
    "#  단순한 하나의 텍스트가 아니라... Human Messagbe - AI Message - Human Messagbe - AI Message - .... (여러 메세지들)\n",
    "#  심지여 요약본 발생시 System message 도 있을텐데?\n",
    "\n",
    "# prompt 에 이를 위한 공간을 어케 만드나?\n",
    "#  => MessagePlaceHolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9bbf0d-f2cc-4e89-ab6b-3363facdaf21",
   "metadata": {},
   "source": [
    "## MessagePlaceHolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e088dfde-089e-49e5-af0f-a36d78f9cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.prompts.chat import MessagesPlaceholder\n",
    "\n",
    "# https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html#langchain_core.prompts.chat.MessagesPlaceholder\n",
    "\n",
    "# Prompt template that assumes variable is already list of messages.\n",
    "# A placeholder which can be used to pass in a list of messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e555416-17cc-471b-9e9f-e02fd168f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful AI talking to a human'),\n",
    "\n",
    "    # ↓ 누가 보냈는지 알수 없는,\n",
    "    #  예측하기 어려운 메세지의 양과 제한 없는 양의 메세지를 가질수 있다.\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    # ↑ variable_name=\"chat_history\"\n",
    "    #  ConversationSummaryBufferMemory 는\n",
    "    #    history 에서 message 들을 가져와서 (return_messages=True)\n",
    "    #    이곳 MessagesPlaceHolder 를 채운다.\n",
    "    #     AI message, System message, Human message ...\n",
    "    #     얼마나 많은지 알수 없어도 여기에 채워지는 거다!    \n",
    "    \n",
    "    ('human', '{question}'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "78eae12b-04ae-4e29-ba1f-0b9233e946b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt, # 위에서 작성한 프롬프트!\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9afc6dc7-444e-443d-a556-cf31bdcdd0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is John\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'My name is John',\n",
       " 'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'Hello John! How can I assist you today?'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':\"My name is John\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "59d7ed77-fbbc-43e7-94e2-1515bed29624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "Human: I live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'I live in Seoul',\n",
       " 'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I live in Seoul', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':\"I live in Seoul\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5741c7e3-0d8c-4e85-a739-a34827d5160a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is John\n",
      "AI: Hello John! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my name?',\n",
       " 'chat_history': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I live in Seoul', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is John.', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'Your name is John.'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5258056-fe69-4258-83c0-003ef1b44bae",
   "metadata": {},
   "source": [
    "# LCEL Based Memory\n",
    "- 커스텀 chain 에 memory 를 장착하기!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8006cc35-bfad-4d61-9acb-4423e88307d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "86143312-017f-4cc9-8abd-1bd42c8bbd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes, you mentioned that earlier. How can I assist you further, John?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 103, 'total_tokens': 119, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bk51AQtxVt0klQw5lZhGyHIZWcVpO', 'finish_reason': 'stop', 'logprobs': None}, id='run--d49a9587-9bd8-4598-9f2c-1995f029ed7d-0', usage_metadata={'input_tokens': 103, 'output_tokens': 16, 'total_tokens': 119, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    'chat_history': memory.load_memory_variables({})['chat_history'],\n",
    "    'question': 'My name is John',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fc257975-9363-4f7e-995f-ef5427c472a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "↑ 위 방법도 가능은 하다\n",
    "단, 위 접근 방식의 문제는\n",
    "우리가 chain 을 호출할 때마다 chat_history 도 추가해줘야 한다는 거다.\n",
    "\n",
    "↓ 이보다 더 좋은 방법도 있다. 바로 'Runnables' 라는 것을 사용하는 것이다\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd9470-e77f-4e82-85f3-2ac833846d65",
   "metadata": {},
   "source": [
    "## RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c58800a0-f216-477f-8b82-aa765cab6e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.3\n",
    "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
    "# https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html#langchain_core.runnables.passthrough.RunnablePassthrough\n",
    "\n",
    "\n",
    "# Runnable to passthrough inputs unchanged or with additional keys.\n",
    "# This Runnable behaves almost like the identity function,\n",
    "#  except that it can be configured to add additional keys to the output,\n",
    "#  if the input is a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ee7bfa4-cf67-4159-bc56-c15b30aa1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 변수 획특하는 함수 작성\n",
    "def load_memory():\n",
    "    return memory.load_memory_variables({})['chat_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ec42d34e-aa67-4c53-a953-a5fed5dc3ed8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_memory() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMy name is John\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m↑ 위를 실행하게 되면 가장 먼저 load_memory() 를 호출한다.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03mprompt가 필요로 하는 chat_history= 키 내부에 넣는다.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2501\\NLPWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2501\\NLPWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:511\u001b[39m, in \u001b[36mRunnableAssign.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    506\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    509\u001b[39m     **kwargs: Any,\n\u001b[32m    510\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2501\\NLPWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1940\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1938\u001b[39m         output = cast(\n\u001b[32m   1939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1948\u001b[39m         )\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1950\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2501\\NLPWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2501\\NLPWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:497\u001b[39m, in \u001b[36mRunnableAssign._invoke\u001b[39m\u001b[34m(self, value, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m    492\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    496\u001b[39m     **value,\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m     **\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    502\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2501\\NLPWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3774\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3769\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3770\u001b[39m         futures = [\n\u001b[32m   3771\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3772\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3773\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m3774\u001b[39m         output = {key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[32m   3775\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3776\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2501\\NLPWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3758\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3752\u001b[39m child_config = patch_config(\n\u001b[32m   3753\u001b[39m     config,\n\u001b[32m   3754\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3755\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3756\u001b[39m )\n\u001b[32m   3757\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3760\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3762\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2501\\NLPWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4771\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4757\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[32m   4758\u001b[39m \n\u001b[32m   4759\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4768\u001b[39m \u001b[33;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[32m   4769\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4770\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4772\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4773\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4774\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4775\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4776\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4777\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4778\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2501\\NLPWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1940\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1938\u001b[39m         output = cast(\n\u001b[32m   1939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1948\u001b[39m         )\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1950\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2501\\NLPWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2501\\NLPWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4629\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4627\u001b[39m                 output = chunk\n\u001b[32m   4628\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4629\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4630\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4631\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4632\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\NLP2501\\NLPWork\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: load_memory() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "chain.invoke({\n",
    "    'question': 'My name is John',\n",
    "})\n",
    "\n",
    "\"\"\"\n",
    "↑ 위를 실행하게 되면 가장 먼저 load_memory() 를 호출한다.\n",
    "prompt가 필요로 하는 chat_history= 키 내부에 넣는다.\n",
    "\n",
    "이는 마치\n",
    "chain.invoke({\n",
    "    \"chat_history\": load_memory(),  <- 요렇게 한것과 동일하다.\n",
    "    \"question\": \"My name is John\",\n",
    "})\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2bdcb412-bb9f-48e9-be35-29781de8f12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃load_memory() {'question': 'My name is John'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello John! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bk51ZNRnpOVx0WbpmYRRBXHVcfJS7', 'finish_reason': 'stop', 'logprobs': None}, id='run--6d705b61-e65e-46f4-b78b-ba860d1c8c9d-0', usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(input):  \n",
    "  print(\"🎃load_memory()\", input)  # 확인해보자!!\n",
    "  return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "chain.invoke({\n",
    "    \"question\": \"My name is John\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9beae16-e873-4054-8fbb-71dcb6b7c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load_memory() {'question': 'My name is John'}\n",
    "\n",
    "체인에 있는 모든 컴포넌트는 input 을 받을거고, 또 output 을 줄거다.\n",
    "이게 랭체인의 핵심이다.  모든 것이 input을 얻을거고, 그 후엔 output 을 줄거다.\n",
    "\n",
    "\n",
    "chain.invoke(\n",
    "  {     <--  이 dict 는 chain 의 첫번째 아이템의 input 이 되는거다.\n",
    "             바로 그게 load_memory() 의 input 이 된것이다!   규칙이다!\n",
    "    \"question\": \"My name is John\",\n",
    "  }\n",
    ")\n",
    "\n",
    "이후 load_memory() 를 실행히킨 결과로 얻은것이 chat_history 속성으로 들어가\n",
    "체인의 다음 요소 로 전달되는 것이다.\n",
    "\n",
    "\"\"\"\n",
    "None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9568da8e-711f-4435-b25d-5ef8b2f3dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(input):  \n",
    "  print(\"🎃load_memory()\", input)\n",
    "  return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "# 체인 호출 함수를 직접 만들어 보자.\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\n",
    "        \"question\": question,\n",
    "    })\n",
    "    # 체인 호출 결과를 메모리에 저장\n",
    "    memory.save_context( \n",
    "        {\"input\": question},   # 사용자 질문\n",
    "        {\"output\": result.content},  # result 는 AIMessage 가 될거아.\n",
    "    )\n",
    "    print('🟨result:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fe6b16db-01ba-4ef5-8d06-77e3bd241a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃load_memory() {'question': 'My name is John'}\n",
      "🟨result: content='Hello John! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bk6Yq0EP2z1OYUl7ux1Mt35og5jMg', 'finish_reason': 'stop', 'logprobs': None} id='run--9b9bc26e-857e-44b9-808b-1955724811fe-0' usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is John\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "125f6e81-5ea6-467a-804c-5c0355ac18f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃load_memory() {'question': 'What is my name?'}\n",
      "🟨result: content='Your name is John.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 47, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bk6ZayEoTA9AcYdo48gnZnrpKbc6W', 'finish_reason': 'stop', 'logprobs': None} id='run--b1567000-d036-4bd3-9348-35f04c5297c8-0' usage_metadata={'input_tokens': 47, 'output_tokens': 5, 'total_tokens': 52, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")  # 이름 기억하겠죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3c9ecc52-c3ee-461c-9bf8-09a23f5af39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    # memory_key=\"chat_history\",   # 이제 이 또한 필요하지 않을 거다.  기본적인 메모리 key 는 'history' \n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),  # variable_name= 값 변경!\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(input):  \n",
    "  print(\"🎃load_memory()\", input)\n",
    "  return memory.load_memory_variables({})[\"history\"]  # key 값 변경\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm # key 값 변경\n",
    "\n",
    "# 체인 호출 함수를 직접 만들어 보자.\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\n",
    "        \"question\": question,\n",
    "    })\n",
    "    # 체인 호출 결과를 메모리에 저장\n",
    "    memory.save_context( \n",
    "        {\"input\": question},   # 사용자 질문\n",
    "        {\"output\": result.content},  # result 는 AIMessage 가 될거아.\n",
    "    )\n",
    "    print('🟨result:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2f21a138-bef7-4d11-b4dd-dc5b1bc74672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃load_memory() {'question': 'My name is John'}\n",
      "🟨result: content='Hello John! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bk6d6vpFBqVcT8lcKE8JBuWMIR0mz', 'finish_reason': 'stop', 'logprobs': None} id='run--6bbbd428-0b35-4f0e-8da7-ea6a337cdf88-0' usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is John\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0c3c6ca2-eb5f-42b0-ae69-821cce3cadfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎃load_memory() {'question': 'What is my name?'}\n",
      "🟨result: content='Your name is John.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 47, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bk6dOqkGpZQe7jdP2FT3TjZoA8Ogn', 'finish_reason': 'stop', 'logprobs': None} id='run--c2f099af-1f48-4860-b134-808dd0fa10a5-0' usage_metadata={'input_tokens': 47, 'output_tokens': 5, 'total_tokens': 52, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e01e6b4-09de-43c0-b391-1c35a1c12892",
   "metadata": {},
   "source": [
    "## refactoring\n",
    "아래와 같이 코드를 작성할수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "65b70f05-7c8d-4108-93a3-0101f8bb036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory():    # 매개변수 삭제\n",
    "  return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = prompt | llm  # Runnable 삭제\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question, \"history\": load_memory()})  # 체인호출히 'history' 추가\n",
    "    memory.save_context( \n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print('🟨result:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "34441ea6-40a1-43f4-9f01-5f689544ec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟨result: content='Hello John! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 24, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bk6jlb8L1QJ6Rfp9HRVL0bj0fQ0hZ', 'finish_reason': 'stop', 'logprobs': None} id='run--a137031d-d65e-4b56-8aaa-78105828975e-0' usage_metadata={'input_tokens': 24, 'output_tokens': 10, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is John\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "735ca518-6759-4fde-b7a0-5dde342f50f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟨result: content='Your name is John.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 47, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bk6jtIy8ln5zoXFYnsVBqDKeBc5a4', 'finish_reason': 'stop', 'logprobs': None} id='run--1a6d5024-035b-44ac-b932-918fd3e1dfdc-0' usage_metadata={'input_tokens': 47, 'output_tokens': 5, 'total_tokens': 52, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf63d9-d0b8-484e-a15c-b5bfe2dda033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d8f7f-9cbb-488c-a03c-c42399fed8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6994db44-2830-4d15-a055-05d7486324be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57f6f4-5144-4cf7-8569-78ae7a48ac84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f013790-1f13-4ab8-a47a-eeed30a398e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b7fff-e755-4031-8622-a8743eb75762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da602522-6ca7-4bd2-b3ae-bf052d93878f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098448b-6c20-47e9-9441-4cf200877184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d6453-361c-42f0-845b-b4ff30ca56ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
