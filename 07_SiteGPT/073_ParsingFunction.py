import os
import time

print(f'âœ… {os.path.basename( __file__ )} ì‹¤í–‰ë¨ {time.strftime('%Y-%m-%d %H:%M:%S')}') # ì‹¤í–‰íŒŒì¼ëª…, í˜„ì¬ì‹œê°„ì¶œë ¥
print(f'\tOPENAI_API_KEY={os.getenv("OPENAI_API_KEY")[:20]}...') # OPENAI_API_KEY í•„ìš”!

import streamlit as st
from langchain_community.document_loaders.sitemap import SitemapLoader
from langchain_text_splitters.character import RecursiveCharacterTextSplitter

# document ì „ì²´ HTML ì„ ê°€ì§„ Beautiful soup object ê°’ì´ ì „ë‹¬ëœë‹¤
# ì—¬ê¸°ì„œ ê²€ìƒ‰(search) í•˜ê±°ë‚˜, HTML element ë“¤ì„ ì œê±°í• ìˆ˜ ìˆë‹¤.
def parse_page(soup):
    # <header> ì™€ <footer> ì œê±°
    header = soup.find('header')
    footer = soup.find('footer')

    # decompose()  í•´ë‹¹ ìš”ì†Œë¥¼ HTML ë¬¸ì„œì—ì € ì œê±°.
    if header:
        header.decompose()

    if footer:
        footer.decompose()
        
    # <header> ì™€ <footer> ì œê±°ëœ ë‚˜ë¨¸ì§€ HTML text ë¦¬í„´
    return (str(soup.get_text())
            # ê³µë°±ë¬¸ì, ë¶ˆí•„ìš”í•œ UI ì œê±°(ì¹˜í™˜)
            .replace(r"\n", " ")
            .replace("\xa0", " ")
            .replace("Try Claude", " ")
            )

@st.cache_resource(show_spinner="Fetching URL...")
def load_website(url):
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=1000,
        chunk_overlap=200,
    )

    loader = SitemapLoader(
        url,
        # data ë¥¼ laod í•˜ê³  ì‹¶ì€ url ë“¤ì„ ë‹´ì„ list.  url ì€ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¸ì‹ëœë‹¤.
        filter_urls=[            
            # "https://www.anthropic.com/news/anthropic-raises-series-e-at-usd61-5b-post-money-valuation",

            # ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©
            # .*  â† ì „ í›„ì— ë‹¤ë¥¸ ë¬¸ìë“¤ì´ ì˜¬ìˆ˜ëŠ” ìˆì§€ë§Œ
            #  /news/ ë¥¼ í¬í•¨í•˜ëŠ” url ë§Œ ë³¼ìˆ˜ ìˆë‹¤.            
            r"^(.*\/news\/).*",  

            # ?! <- negative lookahead    /news/ ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ” url ì˜ í˜ì´ì§€ë§Œ ê°€ì ¸ì˜¨ë‹¤.
            # r"^(?!.*\/news\/).*",  
        ],
        parsing_function=parse_page,  # BeautifulSoup ì¶”ì¶œí•  HTML ë¬¸ì„œì˜ ìš”ì†Œë¥¼ ë‹¤ë£°ìˆ˜ ìˆëŠ” í•¨ìˆ˜.
                                        #  ì´ í•¨ìˆ˜ëŠ” í¬í•¨í•˜ê³  ì‹¶ì€ text ë¥¼ ë¦¬í„´í•´ì•¼ í•¨.
    )
    loader.requests_per_second = 1
    loader.max_depth=1  # (ê¸°ë³¸ê°’ 10)  ìˆ˜ì—…ì—¬ê±´ìƒ 1ë¡œ ì„¤ì •
    loader.headers = {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Mobile Safari/537.36'}
    docs = loader.load_and_split(text_splitter=splitter)
    return docs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â­• Streamlit ë¡œì§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="SiteGPT",
    page_icon="ğŸ–¥ï¸",
)

st.title("SiteGPT")

with st.sidebar:
    url = st.text_input(
        "Write down a URL",
        placeholder="https://example.com",
    )


st.markdown(
"""
    Ask questions about the content of a website.
           
    Start by writing the URL of the website on the sidebar.
"""
)



if url:
    # ì‚¬ìš©ìê°€ URL ì„ ì…ë ¥í•˜ë©´, ê±°ê¸°ì— XML sitemap ì´ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸í• ê±°ë‹¤.
    # í¬í•¨ë˜ì§€ ì•Šë‹¤ë©´ error ë¥¼ ë³´ì—¬ì¤˜ì„œ application ì˜ ì¶œëŒì„ ë¯¸ë¦¬ ë°©ì§€í•˜ì.
    if ".xml" not in url:
        with st.sidebar:
            st.error("Please write down a Sitemap url")
    
    else:
        docs = load_website(url)
        st.write(docs)


